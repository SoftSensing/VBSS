{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9b7f11-681b-4d7b-b5c9-1233c5eb78c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:36:53.580350Z",
     "iopub.status.busy": "2024-05-26T17:36:53.579842Z",
     "iopub.status.idle": "2024-05-26T17:36:57.555644Z",
     "shell.execute_reply": "2024-05-26T17:36:57.554773Z",
     "shell.execute_reply.started": "2024-05-26T17:36:53.580328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/GerbenBeintema/deepSI@master\n",
      "  Cloning https://github.com/GerbenBeintema/deepSI (to revision master) to /tmp/pip-req-build-nllqn0wv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/GerbenBeintema/deepSI /tmp/pip-req-build-nllqn0wv\n",
      "  Resolved https://github.com/GerbenBeintema/deepSI to commit 28c96c174fa2e1c83aeb26091d67785d468a4bee\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (1.23.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (3.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (1.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (4.64.1)\n",
      "Requirement already satisfied: progressbar in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (2.5)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (1.12.1+cu116)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (1.1.2)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (0.26.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (4.6.0.66)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (2.28.2)\n",
      "Requirement already satisfied: rarfile in /usr/local/lib/python3.9/dist-packages (from deepSI==0.3.22) (4.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym->deepSI==0.3.22) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym->deepSI==0.3.22) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym->deepSI==0.3.22) (6.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (23.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->deepSI==0.3.22) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->deepSI==0.3.22) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->deepSI==0.3.22) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->deepSI==0.3.22) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->deepSI==0.3.22) (1.26.14)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->deepSI==0.3.22) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->deepSI==0.3.22) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->deepSI==0.3.22) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym->deepSI==0.3.22) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->deepSI==0.3.22) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/GerbenBeintema/deepSI@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2460d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:36:57.558700Z",
     "iopub.status.busy": "2024-05-26T17:36:57.557929Z",
     "iopub.status.idle": "2024-05-26T17:36:58.949249Z",
     "shell.execute_reply": "2024-05-26T17:36:58.948479Z",
     "shell.execute_reply.started": "2024-05-26T17:36:57.558667Z"
    }
   },
   "outputs": [],
   "source": [
    "import deepSI\n",
    "from deepSI import System_data, System_data_list\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "from torch import optim, nn\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "linewidth = 3.320\n",
    "reducesize = 0.85\n",
    "fig_fontsize = (10*19/28)*reducesize\n",
    "plt.rcParams.update({'font.size': fig_fontsize})\n",
    "dpi = 200\n",
    "pad = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c171a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:36:58.950390Z",
     "iopub.status.busy": "2024-05-26T17:36:58.950074Z",
     "iopub.status.idle": "2024-05-26T17:37:07.253146Z",
     "shell.execute_reply": "2024-05-26T17:37:07.252476Z",
     "shell.execute_reply.started": "2024-05-26T17:36:58.950368Z"
    }
   },
   "outputs": [],
   "source": [
    "archive_path = 'ffmpeg-master-latest-linux64-gpl.tar.xz'\n",
    "output_dir = 'ffmpeg/'  # You can use an existing or new directory\n",
    "\n",
    "# Extract the archive\n",
    "!tar -xf {archive_path} -C {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef1b15b-0623-459f-8782-3999e840e1c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:07.254236Z",
     "iopub.status.busy": "2024-05-26T17:37:07.254033Z",
     "iopub.status.idle": "2024-05-26T17:37:07.257847Z",
     "shell.execute_reply": "2024-05-26T17:37:07.257328Z",
     "shell.execute_reply.started": "2024-05-26T17:37:07.254216Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example directory containing the FFmpeg binaries\n",
    "ffmpeg_dir = 'ffmpeg/ffmpeg-master-latest-linux64-gpl/bin'\n",
    "\n",
    "# Add the FFmpeg directory to the PATH environment variable\n",
    "os.environ['PATH'] = ffmpeg_dir + os.pathsep + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d55f49-3bdd-4007-9549-81b82191984f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:07.259635Z",
     "iopub.status.busy": "2024-05-26T17:37:07.259466Z",
     "iopub.status.idle": "2024-05-26T17:37:37.986263Z",
     "shell.execute_reply": "2024-05-26T17:37:37.985280Z",
     "shell.execute_reply.started": "2024-05-26T17:37:07.259620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1cd03831624ca1badd05c46696b8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resizing frames:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the npz file\n",
    "adi = 12\n",
    "systemsdir = 'systems_force/'\n",
    "filen = 'combined_output_20_3.npz'\n",
    "data = np.load(filen, allow_pickle=True)\n",
    "\n",
    "# Assuming you have arrays 'frames' and 'forces' within your npz file\n",
    "frames = data['frames']\n",
    "forces = data['forces']\n",
    "\n",
    "# Determine the size of each set\n",
    "total_size = len(frames)\n",
    "train_size = int(total_size * 0.6)  # 50% of the data for training\n",
    "val_size = int(total_size * 0.20)  # 20% of the data for validation\n",
    "test_size = total_size - train_size - val_size  # Remaining 30% for testing\n",
    "\n",
    "# Function to resize and reshape frames with progress bar\n",
    "def resize_and_reshape_frames(frames, batch_size, new_height, new_width):\n",
    "    num_frames = frames.shape[0]\n",
    "    resized_and_reshaped_frames = np.zeros((num_frames, frames.shape[3], new_height, new_width), dtype=np.uint8)\n",
    "    for start in tqdm(range(0, num_frames, batch_size), desc=\"Resizing frames\"):\n",
    "        end = start + batch_size\n",
    "        batch_frames = frames[start:end]\n",
    "        for i in range(batch_frames.shape[0]):\n",
    "            resized_frame = cv2.resize(batch_frames[i], (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "            resized_and_reshaped_frames[start + i] = resized_frame.transpose(2, 0, 1)\n",
    "    return resized_and_reshaped_frames\n",
    "\n",
    "# Resize and reshape parameters\n",
    "new_height = frames.shape[1] // 2\n",
    "new_width = frames.shape[2] // 2\n",
    "batch_size = 30\n",
    "\n",
    "# Resize and reshape all frames\n",
    "frames_resized_reshaped = resize_and_reshape_frames(frames, batch_size, new_height, new_width).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa9cd84-1f34-4f30-aff7-9f3274baa1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:37.987560Z",
     "iopub.status.busy": "2024-05-26T17:37:37.987317Z",
     "iopub.status.idle": "2024-05-26T17:37:38.001755Z",
     "shell.execute_reply": "2024-05-26T17:37:38.000840Z",
     "shell.execute_reply.started": "2024-05-26T17:37:37.987532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "del frames\n",
    "frames_train = frames_resized_reshaped[:train_size]\n",
    "frames_val = frames_resized_reshaped[train_size:train_size + val_size]\n",
    "frames_test = frames_resized_reshaped[train_size + val_size:]\n",
    "n_channels, height, width = frames_train.shape[1], frames_train.shape[2], frames_train.shape[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9214f7bd-3948-4144-885c-6b6c268a1e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:38.002959Z",
     "iopub.status.busy": "2024-05-26T17:37:38.002729Z",
     "iopub.status.idle": "2024-05-26T17:37:38.006870Z",
     "shell.execute_reply": "2024-05-26T17:37:38.006275Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.002939Z"
    }
   },
   "outputs": [],
   "source": [
    "forces_train = forces[:train_size, 2]\n",
    "forces_val = forces[train_size:train_size + val_size, 2]\n",
    "forces_test = forces[train_size + val_size:, 2]\n",
    "del forces\n",
    "# Initialize the SS_encoder_CNN_video system\n",
    "#sys_vbss = deepSI.fit_systems.SS_encoder_CNN_video(na=adi, nb=adi)\n",
    "n_channels, height, width = frames_train.shape[1], frames_train.shape[2], frames_train.shape[3]\n",
    "#sys_vbss.init_nets(nu=1, ny=(n_channels, height, width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4cce54-84b4-4888-af09-736477aba682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:38.007585Z",
     "iopub.status.busy": "2024-05-26T17:37:38.007433Z",
     "iopub.status.idle": "2024-05-26T17:37:38.011482Z",
     "shell.execute_reply": "2024-05-26T17:37:38.010845Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.007571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create System_data instances with resized and reshaped frames\n",
    "system_data = System_data(y=forces_train, u=frames_train)\n",
    "system_data_val = System_data(y=forces_val, u=frames_val)\n",
    "system_data_test = System_data(y=forces_test, u=frames_test)\n",
    "del forces_train, frames_train, forces_val, frames_val, forces_test, frames_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3913141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:38.012425Z",
     "iopub.status.busy": "2024-05-26T17:37:38.012260Z",
     "iopub.status.idle": "2024-05-26T17:37:38.019536Z",
     "shell.execute_reply": "2024-05-26T17:37:38.018920Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.012411Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepSI.utils.torch_nets import CNN_encoder\n",
    "\n",
    "\n",
    "#Psi(upast, ypast) where upast is an image and ypast an vector\n",
    "#nb, nu, na, ny, nx\n",
    "class CNN_encoder_image_input(nn.Module):\n",
    "    def __init__(self, nb, nu, na, ny, nx, n_nodes_per_layer=64, n_hidden_layers=2, activation=nn.Tanh, features_ups_factor=1.33):\n",
    "        super(CNN_encoder_image_input, self).__init__()\n",
    "        self.net = CNN_encoder(na, ny, nb, nu, nx, \\\n",
    "                               n_nodes_per_layer=n_nodes_per_layer, \\\n",
    "                               n_hidden_layers=n_hidden_layers, \\\n",
    "                                activation=activation, \\\n",
    "                                features_ups_factor=features_ups_factor)\n",
    "\n",
    "    def forward(self, upast, ypast):\n",
    "        return self.net(ypast, upast)\n",
    "\n",
    "#nx, nu\n",
    "class CNN_encoder_f_image(nn.Module):\n",
    "    def __init__(self, nx, nu, n_nodes_per_layer=64, n_hidden_layers=2, activation=nn.Tanh, features_ups_factor=1.33):\n",
    "        super(CNN_encoder_f_image, self).__init__()\n",
    "        self.net = CNN_encoder(1, nx, 1, nu, nx, \\\n",
    "                               n_nodes_per_layer=n_nodes_per_layer, \\\n",
    "                               n_hidden_layers=n_hidden_layers, \\\n",
    "                                activation=activation, \\\n",
    "                                features_ups_factor=features_ups_factor)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        #x.shape = (Nbatch, nx)\n",
    "        #u.shape = (Nbatch, C, H, W)\n",
    "\n",
    "        #self.net \n",
    "        # x -> upast = (Nbatch, nb, nu)\n",
    "        # u -> ypast = (Nbatch, na, C, H, W)\n",
    "        return self.net(x[:,None], u[:,None])\n",
    "    \n",
    "#nx, ny, nu=-1\n",
    "class CNN_encoder_h_image(nn.Module):\n",
    "    def __init__(self, nx, ny, nu, n_nodes_per_layer=64, n_hidden_layers=2, activation=nn.Tanh, features_ups_factor=1.33):\n",
    "        super(CNN_encoder_h_image, self).__init__()\n",
    "        self.net = CNN_encoder(1, nx, 1, nu, ny, \\\n",
    "                               n_nodes_per_layer=n_nodes_per_layer, \\\n",
    "                               n_hidden_layers=n_hidden_layers, \\\n",
    "                                activation=activation, \\\n",
    "                                features_ups_factor=features_ups_factor)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        #x.shape = (Nbatch, nx)\n",
    "        #u.shape = (Nbatch, C, H, W)\n",
    "\n",
    "        #self.net \n",
    "        # x -> upast = (Nbatch, nb, nu)\n",
    "        # u -> ypast = (Nbatch, na, C, H, W)\n",
    "        return self.net(x[:,None], u[:,None])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c63561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:37:38.020339Z",
     "iopub.status.busy": "2024-05-26T17:37:38.020156Z",
     "iopub.status.idle": "2024-05-26T17:37:38.243105Z",
     "shell.execute_reply": "2024-05-26T17:37:38.242443Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.020324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System_data of length: 7200 nu=(3, 135, 240) ny=None normed=False dt=None\n"
     ]
    }
   ],
   "source": [
    "# from deepSI.utils import CNN_chained_upscales, CNN_encoder\n",
    "from deepSI.fit_systems import SS_encoder_general\n",
    "\n",
    "class SS_encoder_CNN_video_input(SS_encoder_general):\n",
    "    \"\"\"The subspace encoder convolutonal neural network with image inputs\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The subspace encoder\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, nx=10, na=20, nb=20, feedthrough=True, e_net=CNN_encoder_image_input, \\\n",
    "                 f_net=CNN_encoder_f_image, h_net=CNN_encoder_h_image, \\\n",
    "                                            e_net_kwargs={}, f_net_kwargs={}, h_net_kwargs={}):\n",
    "        super(SS_encoder_CNN_video_input, self).__init__(nx=nx,na=na,nb=nb, feedthrough=feedthrough, \\\n",
    "            e_net=e_net,               f_net=f_net,                h_net=h_net, \\\n",
    "            e_net_kwargs=e_net_kwargs, f_net_kwargs=f_net_kwargs,  h_net_kwargs=h_net_kwargs)\n",
    "\n",
    "model = SS_encoder_CNN_video_input(nx=20, na=12, nb=12)\n",
    "\n",
    "# nu = (3,25,30)\n",
    "# ny = 1\n",
    "# N = 200\n",
    "# u_seq = np.random.randn(N,*nu)\n",
    "# y_seq = np.random.randn(N, ny)\n",
    "#sys_data = System_data(u_seq, y_seq)\n",
    "\n",
    "print(system_data)\n",
    "model.init_model(nu=(3,135,240), ny=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80b6180a-d004-44de-abcc-ff1588cfd577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:44:27.967211Z",
     "iopub.status.busy": "2024-05-26T17:44:27.966947Z",
     "iopub.status.idle": "2024-05-26T17:44:27.993407Z",
     "shell.execute_reply": "2024-05-26T17:44:27.992589Z",
     "shell.execute_reply.started": "2024-05-26T17:44:27.967191Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(self, train_sys_data, val_sys_data, epochs=30, batch_size=256, loss_kwargs={}, \\\n",
    "            auto_fit_norm=True, validation_measure='sim-NRMS', optimizer_kwargs={}, concurrent_val=False, cuda=False, \\\n",
    "            timeout=None, verbose=1, sqrt_train=True, num_workers_data_loader=0, print_full_time_profile=False, scheduler_kwargs={}):\n",
    "        '''The batch optimization method with parallel validation, \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_sys_data : System_data or System_data_list\n",
    "            The system data to be fitted\n",
    "        val_sys_data : System_data or System_data_list\n",
    "            The validation system data after each used after each epoch for early stopping. Use the keyword argument validation_measure to specify which measure should be used. \n",
    "        epochs : int\n",
    "        batch_size : int\n",
    "        loss_kwargs : dict\n",
    "            The Keyword Arguments to be passed to the self.make_training_data and self.loss of the current fit_system.\n",
    "        auto_fit_norm : boole\n",
    "            If true will use self.norm.fit(train_sys_data) which will fit it element wise. \n",
    "        validation_measure : str\n",
    "            Specify which measure should be used for validation, e.g. 'sim-RMS', '10-step-last-RMS', 'sim-NRMS_sys_norm', ect. See self.cal_validation_error for details.\n",
    "        optimizer_kwargs : dict\n",
    "            The Keyword Arguments to be passed on to init_optimizer. notes; init_optimizer['optimizer'] is the optimization function used (default torch.Adam)\n",
    "            and optimizer_kwargs['parameters_optimizer_kwargs'] the learning rates and such for the different elements of the models. see https://pytorch.org/docs/stable/optim.html\n",
    "        concurrent_val : boole\n",
    "            If set to true a subprocess will be started which concurrently evaluates the validation method selected.\n",
    "            Warning: if concurrent_val is set than \"if __name__=='__main__'\" or import from a file if using self defined method or networks.\n",
    "        cuda : bool\n",
    "            if cuda will be used (often slower than not using it, be aware)\n",
    "        timeout : None or number\n",
    "            Alternative to epochs to run until a set amount of time has past. \n",
    "        verbose : int\n",
    "            Set to 0 for a silent run\n",
    "        sqrt_train : boole\n",
    "            will sqrt the loss while printing\n",
    "        num_workers_data_loader : int\n",
    "            see https://pytorch.org/docs/stable/data.html\n",
    "        print_full_time_profile : boole\n",
    "            will print the full time profile, useful for debugging and basic process optimization. \n",
    "        scheduler_kwargs : dict\n",
    "            learning rate scheduals are a work in progress.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        This method implements a batch optimization method in the following way; each epoch the training data is scrambled and batched where each batch\n",
    "        is passed to the self.loss method and utilized to optimize the parameters. After each epoch the systems is validated using the evaluation of a \n",
    "        simulation or a validation split and a checkpoint will be crated if a new lowest validation loss has been achieved. (or concurrently if concurrent_val=True)\n",
    "        After training (which can be stopped at any moment using a KeyboardInterrupt) the system is loaded with the lowest validation loss. \n",
    "\n",
    "        The default checkpoint location is \"C:/Users/USER/AppData/Local/deepSI/checkpoints\" for windows and ~/.deepSI/checkpoints/ for unix like.\n",
    "        These can be loaded manually using sys.load_checkpoint(\"_best\") or \"_last\". (For this to work the sys.unique_code needs to be set to the correct string)\n",
    "        '''\n",
    "        def validation(train_loss=None, time_elapsed_total=None):\n",
    "            self.eval(); self.cpu()\n",
    "            Loss_val = self.cal_validation_error(val_sys_data, validation_measure=validation_measure)\n",
    "            self.Loss_val.append(Loss_val)\n",
    "            self.Loss_train.append(train_loss)\n",
    "            self.time.append(time_elapsed_total)\n",
    "            self.batch_id.append(self.batch_counter)\n",
    "            self.epoch_id.append(self.epoch_counter)\n",
    "            if self.bestfit>=Loss_val:\n",
    "                self.bestfit = Loss_val\n",
    "                self.checkpoint_save_system()\n",
    "            if cuda: \n",
    "                self.cuda()\n",
    "            self.train()\n",
    "            return Loss_val\n",
    "        \n",
    "        ########## Initialization ##########\n",
    "        if self.init_model_done==False:\n",
    "            if verbose: print('Initilizing the model and optimizer')\n",
    "            device = 'cuda' if cuda else 'cpu'\n",
    "            optimizer_kwargs = deepcopy(optimizer_kwargs)\n",
    "            parameters_optimizer_kwargs = optimizer_kwargs.get('parameters_optimizer_kwargs',{})\n",
    "            if parameters_optimizer_kwargs:\n",
    "                del optimizer_kwargs['parameters_optimizer_kwargs']\n",
    "            self.init_model(sys_data=train_sys_data, device=device, auto_fit_norm=auto_fit_norm, optimizer_kwargs=optimizer_kwargs,\\\n",
    "                    parameters_optimizer_kwargs=parameters_optimizer_kwargs, scheduler_kwargs=scheduler_kwargs)\n",
    "        else:\n",
    "            if verbose: print('Model already initilized (init_model_done=True), skipping initilizing of the model, the norm and the creation of the optimizer')\n",
    "            self._check_and_refresh_optimizer_if_needed() \n",
    "\n",
    "\n",
    "        if self.scheduler==False and verbose:\n",
    "            print('!!!! Your might be continuing from a save which had scheduler but which was removed during saving... check this !!!!!!')\n",
    "        \n",
    "        self.dt = train_sys_data.dt\n",
    "        if cuda: \n",
    "            self.cuda()\n",
    "        self.train()\n",
    "\n",
    "        self.epoch_counter = 0 if len(self.epoch_id)==0 else self.epoch_id[-1]\n",
    "        self.batch_counter = 0 if len(self.batch_id)==0 else self.batch_id[-1]\n",
    "        extra_t            = 0 if len(self.time)    ==0 else self.time[-1] #correct timer after restart\n",
    "\n",
    "        ########## Getting the data ##########\n",
    "        data_train = self.make_training_data(self.norm.transform(train_sys_data), **loss_kwargs)\n",
    "        if not isinstance(data_train, Dataset) and verbose: print_array_byte_size(sum([d.nbytes for d in data_train]))\n",
    "\n",
    "        #### transforming it back to a list to be able to append. ########\n",
    "        self.Loss_val, self.Loss_train, self.batch_id, self.time, self.epoch_id = list(self.Loss_val), list(self.Loss_train), list(self.batch_id), list(self.time), list(self.epoch_id)\n",
    "\n",
    "        #### init monitoring values ########\n",
    "        Loss_acc_val, N_batch_acc_val, val_counter, best_epoch, batch_id_start = 0, 0, 0, 0, self.batch_counter #to print the frequency of the validation step.\n",
    "        N_training_samples = len(data_train) if isinstance(data_train, Dataset) else len(data_train[0])\n",
    "        batch_size = min(batch_size, N_training_samples)\n",
    "        N_batch_updates_per_epoch = N_training_samples//batch_size\n",
    "        if verbose>0: \n",
    "            print(f'N_training_samples = {N_training_samples}, batch_size = {batch_size}, N_batch_updates_per_epoch = {N_batch_updates_per_epoch}')\n",
    "        \n",
    "        ### convert to dataset ###\n",
    "        if isinstance(data_train, Dataset):\n",
    "            persistent_workers = False if num_workers_data_loader==0 else True\n",
    "            data_train_loader = DataLoader(data_train, batch_size=batch_size, drop_last=True, shuffle=True, \\\n",
    "                                   num_workers=num_workers_data_loader, persistent_workers=persistent_workers)\n",
    "        else: #add my basic DataLoader\n",
    "            data_train_loader = My_Simple_DataLoader(data_train, batch_size=batch_size) #is quite a bit faster for low data situations\n",
    "\n",
    "        if concurrent_val:\n",
    "            self.remote_start(val_sys_data, validation_measure)\n",
    "            self.remote_send(float('nan'), extra_t)\n",
    "        else: #start with the initial validation \n",
    "            validation(train_loss=float('nan'), time_elapsed_total=extra_t) #also sets current model to cuda\n",
    "            if verbose: \n",
    "                print(f'Initial Validation {validation_measure}=', self.Loss_val[-1])\n",
    "\n",
    "        try:\n",
    "            t = Tictoctimer()\n",
    "            start_t = time.time() #time keeping\n",
    "            epochsrange = range(epochs) if timeout is None else itertools.count(start=0)\n",
    "            if timeout is not None and verbose>0: \n",
    "                print(f'Starting indefinite training until {timeout} seconds have passed due to provided timeout')\n",
    "\n",
    "            for epoch in (tqdm(epochsrange) if verbose>0 else epochsrange):\n",
    "                bestfit_old = self.bestfit #to check if a new lowest validation loss has been achieved\n",
    "                Loss_acc_epoch = 0.\n",
    "                t.start()\n",
    "                t.tic('data get')\n",
    "                for train_batch in data_train_loader:\n",
    "                    if cuda:\n",
    "                        train_batch = [b.cuda() for b in train_batch]\n",
    "                    t.toc('data get')\n",
    "                    def closure(backward=True):\n",
    "                        t.toc('optimizer start')\n",
    "                        t.tic('loss')\n",
    "                        Loss = self.loss(*train_batch, **loss_kwargs)\n",
    "                        t.toc('loss')\n",
    "                        if backward:\n",
    "                            t.tic('zero_grad')\n",
    "                            self.optimizer.zero_grad()\n",
    "                            t.toc('zero_grad')\n",
    "                            t.tic('backward')\n",
    "                            Loss.backward()\n",
    "                            t.toc('backward')\n",
    "                        t.tic('stepping')\n",
    "                        return Loss\n",
    "\n",
    "                    t.tic('optimizer start')\n",
    "                    training_loss = self.optimizer.step(closure).item()\n",
    "                    t.toc('stepping')\n",
    "                    if self.scheduler:\n",
    "                        t.tic('scheduler')\n",
    "                        self.scheduler.step()\n",
    "                        t.tic('scheduler')\n",
    "                    Loss_acc_val += training_loss\n",
    "                    Loss_acc_epoch += training_loss\n",
    "                    N_batch_acc_val += 1\n",
    "                    self.batch_counter += 1\n",
    "                    self.epoch_counter += 1/N_batch_updates_per_epoch\n",
    "\n",
    "                    t.tic('val')\n",
    "                    if concurrent_val and self.remote_recv(): ####### validation #######\n",
    "                        self.remote_send(Loss_acc_val/N_batch_acc_val, time.time()-start_t+extra_t)\n",
    "                        Loss_acc_val, N_batch_acc_val, val_counter = 0., 0, val_counter + 1\n",
    "                    t.toc('val')\n",
    "                    t.tic('data get')\n",
    "                t.toc('data get')\n",
    "\n",
    "                ########## end of epoch clean up ##########\n",
    "                train_loss_epoch = Loss_acc_epoch/N_batch_updates_per_epoch\n",
    "                if np.isnan(train_loss_epoch):\n",
    "                    if verbose>0: print(f'&&&&&&&&&&&&& Encountered a NaN value in the training loss at epoch {epoch}, breaking from loop &&&&&&&&&&')\n",
    "                    break\n",
    "\n",
    "                t.tic('val')\n",
    "                if not concurrent_val:\n",
    "                    validation(train_loss=train_loss_epoch, \\\n",
    "                               time_elapsed_total=time.time()-start_t+extra_t) #updates bestfit and goes back to cpu and back\n",
    "                t.toc('val')\n",
    "                t.pause()\n",
    "\n",
    "                ######### Printing Routine ##########\n",
    "                if verbose>0:\n",
    "                    time_elapsed = time.time() - start_t\n",
    "                    if bestfit_old > self.bestfit:\n",
    "                        print(f'########## New lowest validation loss achieved ########### {validation_measure} = {self.bestfit}')\n",
    "                        best_epoch = epoch+1\n",
    "                    if concurrent_val: #if concurrent val than print validation freq\n",
    "                        val_feq = val_counter/(epoch+1)\n",
    "                        valfeqstr = f', {val_feq:4.3} vals/epoch' if (val_feq>1 or val_feq==0) else f', {1/val_feq:4.3} epochs/val'\n",
    "                    else: #else print validation time use\n",
    "                        valfeqstr = f''\n",
    "                    trainstr = f'sqrt loss {train_loss_epoch**0.5:7.4}' if sqrt_train and train_loss_epoch>=0 else f'loss {train_loss_epoch:7.4}'\n",
    "                    Loss_val_now = self.Loss_val[-1] if len(self.Loss_val)!=0 else float('nan')\n",
    "\n",
    "                    # Handle validation loss as float\n",
    "                    if isinstance(Loss_val_now, (list, np.ndarray)) and len(Loss_val_now) == 1:\n",
    "                        Loss_val_now = Loss_val_now[0]\n",
    "\n",
    "                    Loss_str = f'Epoch {epoch+1:4}, {trainstr}, Val {validation_measure} {Loss_val_now:6.4f}'\n",
    "                    loss_time = (t.acc_times['loss'] + t.acc_times['optimizer start'] + t.acc_times['zero_grad'] + t.acc_times['backward'] + t.acc_times['stepping'])  / t.time_elapsed\n",
    "                    time_str = f'Time Loss: {loss_time:.1%}, data: {t.acc_times[\"data get\"]/t.time_elapsed:.1%}, val: {t.acc_times[\"val\"]/t.time_elapsed:.1%}{valfeqstr}'\n",
    "\n",
    "                    self.batch_feq = (self.batch_counter - batch_id_start)/(time.time() - start_t)\n",
    "                    batch_str = (f'{self.batch_feq:4.1f} batches/sec' if (self.batch_feq>1 or self.batch_feq==0) else f'{1/self.batch_feq:4.1f} sec/batch')\n",
    "                    print(f'{Loss_str}, {time_str}, {batch_str}')\n",
    "                    if print_full_time_profile:\n",
    "                        print('Time profile:',t.percent())\n",
    "\n",
    "                ####### Timeout Breaking ##########\n",
    "                if timeout is not None:\n",
    "                    if time.time() >= start_t+timeout:\n",
    "                        break\n",
    "        except KeyboardInterrupt:\n",
    "            print('Stopping early due to a KeyboardInterrupt')\n",
    "\n",
    "        self.train(); self.cpu()\n",
    "        del data_train_loader\n",
    "\n",
    "        ####### end of training concurrent things #####\n",
    "        if concurrent_val:\n",
    "            if verbose: print(f'Waiting for started validation process to finish and one last validation... (receiving = {self.remote.receiving})',end='')\n",
    "            if self.remote_recv(wait=True):\n",
    "                if verbose: print('Recv done... ',end='')\n",
    "                if N_batch_acc_val>0:\n",
    "                    self.remote_send(Loss_acc_val/N_batch_acc_val, time.time()-start_t+extra_t)\n",
    "                    self.remote_recv(wait=True)\n",
    "            self.remote_close()\n",
    "            if verbose: print('Done!')\n",
    "\n",
    "        \n",
    "        self.Loss_val, self.Loss_train, self.batch_id, self.time, self.epoch_id = np.array(self.Loss_val), np.array(self.Loss_train), np.array(self.batch_id), np.array(self.time), np.array(self.epoch_id)\n",
    "        self.checkpoint_save_system(name='_last')\n",
    "        try:\n",
    "            self.checkpoint_load_system(name='_best')\n",
    "        except FileNotFoundError:\n",
    "            print('no best checkpoint found keeping last')\n",
    "        if verbose: \n",
    "            print(f'Loaded model with best known validation {validation_measure} of {self.bestfit:6.4} which happened on epoch {best_epoch} (epoch_id={self.epoch_id[-1] if len(self.epoch_id)>0 else 0:.2f})')\n",
    "\n",
    "deepSI.fit_systems.fit = corrected_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98eec88b-d17b-4687-8002-55d3c65333c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T17:44:33.478160Z",
     "iopub.status.busy": "2024-05-26T17:44:33.477894Z",
     "iopub.status.idle": "2024-05-26T17:46:20.107401Z",
     "shell.execute_reply": "2024-05-26T17:46:20.105612Z",
     "shell.execute_reply.started": "2024-05-26T17:44:33.478140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System_data of length: 7200 nu=(3, 135, 240) ny=None normed=False dt=None\n",
      "System_data_norm: (u0=[[[179.06047]]\n",
      "\n",
      " [[178.54716]]\n",
      "\n",
      " [[174.84698]]], ustd=[[[81.04006 ]]\n",
      "\n",
      " [[81.9512  ]]\n",
      "\n",
      " [[88.001884]]], y0=-2.81599406957767, ystd=2.198542996611473)\n",
      "Model already initilized (init_model_done=True), skipping initilizing of the model, the norm and the creation of the optimizer\n",
      "N_training_samples = 7184, batch_size = 128, N_batch_updates_per_epoch = 56\n",
      "Initial Validation sim-NRMS_sys_norm= [0.96985066]\n",
      "Starting indefinite training until 10000 seconds have passed due to provided timeout\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ee923052ec415a811081c1e5765aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to numpy.ndarray.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m         sys_vbss_s\u001b[38;5;241m.\u001b[39msave_system(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msse-cnn-base-force-last\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys_vbss_s, sys_vbss_t\n\u001b[0;32m---> 46\u001b[0m sys_vbss \u001b[38;5;241m=\u001b[39m \u001b[43mget_base_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn [15], line 31\u001b[0m, in \u001b[0;36mget_base_results\u001b[0;34m(load, timeout, n_channels, height, width)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(sys_vbss_s\u001b[38;5;241m.\u001b[39mnorm)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m## n_channels, height, width = frames_train.shape[1], frames_train.shape[2], frames_train.shape[3]\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43msys_vbss_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sys_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_data_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m               \u001b[49m\u001b[43mvalidation_measure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msim-NRMS_sys_norm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m               \u001b[49m\u001b[43mloss_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monline_construct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m               \u001b[49m\u001b[43mauto_fit_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;66;43;03m#optimizer_kwargs={'lr':5e-4}\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m sys_vbss_s\u001b[38;5;241m.\u001b[39msave_system(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msse-cnn-base-force-best\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m sys_vbss_t \u001b[38;5;241m=\u001b[39m sys_vbss_s\u001b[38;5;241m.\u001b[39mapply_experiment(test)\u001b[38;5;241m.\u001b[39mNRMS(test)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/deepSI/fit_systems/fit_system.py:446\u001b[0m, in \u001b[0;36mSystem_torch.fit\u001b[0;34m(self, train_sys_data, val_sys_data, epochs, batch_size, loss_kwargs, auto_fit_norm, validation_measure, optimizer_kwargs, concurrent_val, cuda, timeout, verbose, sqrt_train, num_workers_data_loader, print_full_time_profile, scheduler_kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m trainstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_epoch\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sqrt_train \u001b[38;5;129;01mand\u001b[39;00m train_loss_epoch\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss_epoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    445\u001b[0m Loss_val_now \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoss_val[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoss_val)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m Loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainstr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_measure\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLoss_val_now\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.4\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    447\u001b[0m loss_time \u001b[38;5;241m=\u001b[39m (t\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m t\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer start\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m t\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero_grad\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m t\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m t\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstepping\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;241m/\u001b[39mt\u001b[38;5;241m.\u001b[39mtime_elapsed\n\u001b[1;32m    448\u001b[0m time_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata get\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39mt\u001b[38;5;241m.\u001b[39mtime_elapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39macc_times[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39mt\u001b[38;5;241m.\u001b[39mtime_elapsed\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalfeqstr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to numpy.ndarray.__format__"
     ]
    }
   ],
   "source": [
    "def get_base_results(load=True, timeout=10000,n_channels=n_channels, height=height, width=width):\n",
    "    train, val, test = system_data, system_data_val, system_data_test\n",
    "    print(train)\n",
    "    if load:\n",
    "        sys_vbss_s = deepSI.load_system(systemsdir+'sse-cnn-base-last')\n",
    "        sys_vbss_s._dt = None\n",
    "        #sys_vbss_s.feedthrough=False\n",
    "        sys_vbss_s.norm.u0 = np.mean(train.u, axis=(0, 2, 3))[:, None, None]\n",
    "        sys_vbss_s.norm.ustd = np.std(train.u, axis=(0, 2, 3))[:, None, None]\n",
    "        ##Normalize forces by computing mean and standard deviation over samples\n",
    "        sys_vbss_s.norm.y0 = np.mean(train.y, axis=0)\n",
    "        sys_vbss_s.norm.ystd = np.std(train.y, axis=0)\n",
    "        print(sys_vbss_s.norm)\n",
    "        sys_vbss_best = deepSI.load_system(systemsdir+'sse-cnn-base-best')\n",
    "        sys_vbss_best._dt = None\n",
    "        sys_vbss_best.feedthrough=False\n",
    "        sys_vbss_t = sys_vbss_best.apply_experiment(test).NRMS(test)\n",
    "    else:\n",
    "        \n",
    "        sys_vbss_s = SS_encoder_CNN_video_input(nx=20, na=12, nb=12)\n",
    "        sys_vbss_s.init_model(nu=(3,135,240), ny=1)\n",
    "        sys_vbss_s.feedthrough=True\n",
    "        ##Normalize the frames by computing mean and standard deviation over samples, height, and width\n",
    "        sys_vbss_s.norm.u0 = np.mean(train.u, axis=(0, 2, 3))[:, None, None]\n",
    "        sys_vbss_s.norm.ustd = np.std(train.u, axis=(0, 2, 3))[:, None, None]\n",
    "        ##Normalize forces by computing mean and standard deviation over samples\n",
    "        sys_vbss_s.norm.y0 = np.mean(train.y, axis=0)\n",
    "        sys_vbss_s.norm.ystd = np.std(train.y, axis=0)\n",
    "        print(sys_vbss_s.norm)\n",
    "        ## n_channels, height, width = frames_train.shape[1], frames_train.shape[2], frames_train.shape[3]\n",
    "        sys_vbss_s.fit(system_data, val_sys_data=system_data_val, cuda=True, \n",
    "                       epochs=round(1500/(2*adi)), timeout=timeout, \n",
    "                       batch_size=128, \n",
    "                       validation_measure='sim-NRMS_sys_norm', \n",
    "                       loss_kwargs={'online_construct': True, 'nf':5},\n",
    "                       auto_fit_norm=False,\n",
    "                       #optimizer_kwargs={'lr':5e-4}\n",
    "                      )\n",
    "        sys_vbss_s.save_system('sse-cnn-base-force-best')\n",
    "        sys_vbss_t = sys_vbss_s.apply_experiment(test).NRMS(test)\n",
    "        sys_vbss_s.checkpoint_load_system('_last')\n",
    "        sys_vbss_s.save_system('sse-cnn-base-force-last')\n",
    "\n",
    "    return sys_vbss_s, sys_vbss_t\n",
    "\n",
    "sys_vbss = get_base_results(load=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068150b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T17:37:38.482427Z",
     "iopub.status.idle": "2024-05-26T17:37:38.482653Z",
     "shell.execute_reply": "2024-05-26T17:37:38.482554Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.482543Z"
    }
   },
   "outputs": [],
   "source": [
    "t = lambda x: torch.as_tensor(x, dtype=torch.float32)\n",
    "\n",
    "upast, ypast, ufuture, yfuture = [t(x) for x in model.make_training_data(model.norm.transform(sys_data), nf=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81166aaf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T17:37:38.483404Z",
     "iopub.status.idle": "2024-05-26T17:37:38.483644Z",
     "shell.execute_reply": "2024-05-26T17:37:38.483531Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.483520Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{upast.shape=}')\n",
    "print(f'{ypast.shape=}')\n",
    "print(f'{ufuture.shape=}')\n",
    "print(f'{yfuture.shape=}')\n",
    "\n",
    "xinit = model.encoder(upast, ypast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec36cd7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T17:37:38.487696Z",
     "iopub.status.idle": "2024-05-26T17:37:38.487906Z",
     "shell.execute_reply": "2024-05-26T17:37:38.487818Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.487807Z"
    }
   },
   "outputs": [],
   "source": [
    "xnext = model.fn(xinit,ufuture[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddeacc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T17:37:38.491734Z",
     "iopub.status.idle": "2024-05-26T17:37:38.491972Z",
     "shell.execute_reply": "2024-05-26T17:37:38.491870Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.491856Z"
    }
   },
   "outputs": [],
   "source": [
    "ynext = model.hn(xnext,ufuture[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7329e112",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-26T17:37:38.495825Z",
     "iopub.status.idle": "2024-05-26T17:37:38.496048Z",
     "shell.execute_reply": "2024-05-26T17:37:38.495956Z",
     "shell.execute_reply.started": "2024-05-26T17:37:38.495945Z"
    }
   },
   "outputs": [],
   "source": [
    "ynext.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
