{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "\n",
    "# relative path to npz files\n",
    "path = 'Measurements/Sensor-based-dome-2'\n",
    "file_name = 'output_batch_%d.npz'\n",
    "\n",
    "# Training\n",
    "# how many frames to load\n",
    "frames_num = 5\n",
    "data_frames = []\n",
    "data_forces = []\n",
    "for i in range(frames_num):\n",
    "    file_path = os.path.join(path,file_name %i)\n",
    "    data = np.load(file_path)\n",
    "    data_frames.append(data['frames'])\n",
    "    data_forces.append(data['forces'])\n",
    "\n",
    "combined_frames = np.concatenate(data_frames, axis=0)\n",
    "combined_forces = np.concatenate(data_forces, axis=0)\n",
    "\n",
    "features_train = combined_frames\n",
    "targets_train = combined_forces[:,2] # only z axis force\n",
    "\n",
    "features_train = torch.from_numpy(features_train)\n",
    "targets_train = torch.from_numpy(targets_train)\n",
    "\n",
    "# Test\n",
    "# file_path = os.path.join(path, file_name %11)\n",
    "# test_data = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split. Size of train data is 10/11 and size of test data is 1/11 \n",
    "# features_train, features_test, targets_train, targets_test = train_test_split(combined_frames,\n",
    "#                                                                              combined_forces[:,2],\n",
    "#                                                                              test_size = (1/11),\n",
    "#                                                                              random_state = 42) \n",
    "# train_test_split NOT RECOMMENDED, because it shuffles temporally dependent data\n",
    "\n",
    "# features_test = test_data['frames']\n",
    "# targets_test = test_data['forces']\n",
    "# targets_test = targets_test[:,2]\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "\n",
    "# create feature and targets tensor for test set. \n",
    "# featuresTest = torch.from_numpy(features_test)\n",
    "# targetsTest = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 9, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(9, 18, kernel_size = 5, stride = 1, padding = 2) # If needed\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        # Input size is the number of convolutional filters of the previous layer times the size\n",
    "        # of the image after the last pooling\n",
    "        self.rnn = nn.RNN(18 * 16 * 30, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ELU()(self.conv2(x))) # If needed\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (-1, 1, 18 * 16 * 30))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        \n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        #print(\"Size of h0:\", h0.size())  # Print size of h0 - Debugging - Obsolete\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1000\n",
    "num_epochs = 5\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "# test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 200  # hidden layer dimension\n",
    "layer_dim = 2     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 1  Loss: 1.6021342277526855\n",
      "Size of x: torch.Size([1000, 1, 8640])\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 50 # Consider the whole batch to be temporally correlated\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('~~~BEGINNING OF DATASET~~~')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        # print(images.shape)  # Add this line to check the shape of images - Debugging purposes\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data)\n",
    "        # Print Loss\n",
    "        if count % 1 == 0: # for now print for every iteration\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(263.5344), tensor(318.0808), tensor(175.7530), tensor(4.9789), tensor(63.0799), tensor(129.6409), tensor(88.4487), tensor(20.5503), tensor(1.6785), tensor(31.3393), tensor(59.6225), tensor(40.5326), tensor(10.9264), tensor(1.3135), tensor(14.9198), tensor(27.9537), tensor(22.2273), tensor(7.6367), tensor(1.1139), tensor(3.9478), tensor(11.4169), tensor(11.4815), tensor(7.5084), tensor(2.8617), tensor(1.1848), tensor(3.3818), tensor(5.5569), tensor(5.5407), tensor(4.1380), tensor(1.6186), tensor(1.1567), tensor(1.7834), tensor(2.5494), tensor(2.5374), tensor(1.9607), tensor(1.4184), tensor(1.1095), tensor(1.3553), tensor(1.5855), tensor(1.3069), tensor(1.0281), tensor(0.9966), tensor(1.1966), tensor(1.1213), tensor(1.3447), tensor(1.4265), tensor(0.9916), tensor(0.9639), tensor(0.9287), tensor(1.0158)]\n",
      "1.049458\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)\n",
    "print(np.sqrt(np.mean(loss_list[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4390, -1.6798, -1.3761, -1.6742, -1.4726, -1.5396, -1.4566, -1.6832,\n",
      "        -1.6207, -1.5412, -1.4651, -1.2927, -1.1839, -0.9136, -1.0314, -0.6858,\n",
      "        -0.8589, -0.7863, -0.9078, -1.0000, -1.3303, -1.1615, -1.5106, -1.2541,\n",
      "        -1.5544, -1.5094, -1.5063, -1.5673, -1.7674, -1.3458, -1.8002, -1.6865,\n",
      "        -1.6369, -1.6120, -1.5567, -1.4272, -1.6723, -1.4395, -1.1112, -0.9764,\n",
      "        -1.0229, -0.8187, -0.7604, -0.7127, -0.9836, -0.9059, -1.1708, -1.1666,\n",
      "        -1.3527, -1.4642, -1.5702, -1.4378, -1.5149, -1.5482, -1.6335, -1.5511,\n",
      "        -1.4784, -1.3611, -1.3325, -1.5687, -1.4828, -1.2066, -0.9760, -1.0698,\n",
      "        -0.9529, -0.6747, -0.6082, -0.7939, -0.7483, -0.8993, -0.8894, -1.0418,\n",
      "        -0.9757, -1.2215, -1.3234, -1.5811, -1.5214, -1.5055, -1.4919, -1.7612,\n",
      "        -1.6361, -1.5542, -1.4665, -1.4462, -1.4591, -1.4921, -1.6085, -1.2376,\n",
      "        -1.0455, -0.8925, -0.7541, -0.5016, -0.5448, -0.6175, -0.8644, -0.5999,\n",
      "        -0.8163, -0.9700, -1.1658, -1.2778, -1.2900, -1.4486, -1.6155, -1.3893,\n",
      "        -1.6186, -1.6087, -1.4413, -1.5054, -1.2092, -1.5608, -1.4007, -1.3714,\n",
      "        -1.0042, -0.8679, -1.0081, -0.9125, -0.7964, -0.6237, -0.6413, -0.6653,\n",
      "        -0.9248, -0.8185, -1.0556, -1.0828, -1.3824, -1.3565, -1.2617, -1.5267,\n",
      "        -1.6097, -1.4062, -1.4030, -1.4595, -1.6601, -1.4885, -1.5213, -1.2740,\n",
      "        -1.3843, -1.2258, -0.8503, -0.7915, -0.5682, -0.5331, -0.7649, -0.9046,\n",
      "        -0.9979, -0.9602, -0.9912, -1.1352, -1.4218, -1.1765, -1.4093, -1.3783,\n",
      "        -1.5900, -1.5525, -1.5559, -1.6474, -1.5876, -1.6284, -1.3895, -1.7472,\n",
      "        -1.5939, -1.4050, -1.0835, -1.1312, -0.7784, -0.6577, -0.8093, -0.8639,\n",
      "        -0.6833, -0.8620, -0.6304, -0.8675, -0.8602, -1.0619, -1.0291, -1.3151,\n",
      "        -1.4695, -1.2895, -1.6205, -1.2221, -1.6386, -1.6101, -1.5924, -1.5723,\n",
      "        -1.6452, -1.6352, -1.5850, -1.3740, -1.5394, -1.6664, -1.3001, -1.1939,\n",
      "        -1.0756, -1.1154, -0.5747, -0.8708, -0.6465, -0.6048, -0.6172, -1.1733,\n",
      "        -1.0213, -1.1249, -1.3247, -1.2798, -1.2922, -1.3628, -1.4962, -1.3955,\n",
      "        -1.5417, -1.6316, -1.4487, -1.7752, -1.6987, -1.6082, -1.4187, -1.4545,\n",
      "        -1.7147, -1.6286, -1.7720, -1.5830, -1.4795, -1.5790, -1.7521, -1.3917,\n",
      "        -1.5826, -1.6638, -1.2167, -1.3704, -1.5968, -1.4586, -1.3615, -1.4810,\n",
      "        -1.3302, -1.7408, -1.5620, -1.3624, -1.6987, -1.8145, -1.5199, -1.3048,\n",
      "        -1.5426, -1.7535, -1.6262, -1.5344, -1.5133, -1.6336, -1.5280, -1.6651,\n",
      "        -1.5952, -1.5666, -1.7602, -1.7342, -1.4796, -1.7010, -1.6559, -1.5694,\n",
      "        -1.4536, -1.7304, -1.5111, -1.6288, -1.4799, -1.4670, -1.1898, -1.7451,\n",
      "        -1.4487, -1.5754, -1.5499, -1.4626, -1.3857, -1.5909, -1.5635, -1.2614,\n",
      "        -1.4040, -1.5211, -1.5789, -1.3729, -1.5737, -1.3147, -1.4316, -1.4922,\n",
      "        -1.6182, -1.2924, -1.5710, -1.3899, -1.5378, -1.6593, -1.4521, -1.3957,\n",
      "        -1.5202, -1.4859, -1.4852, -1.5051, -1.5038, -1.5250, -1.3820, -1.5798,\n",
      "        -1.4025, -1.2846, -1.5316, -1.6761, -1.4585, -1.3745, -1.6017, -1.3275,\n",
      "        -1.5930, -1.6022, -1.6783, -1.5447, -1.5345, -1.8165, -1.4699, -1.5638,\n",
      "        -1.5391, -1.7246, -1.5040, -1.5979, -1.6003, -1.6507, -1.4240, -1.7384,\n",
      "        -1.7541, -1.7832, -1.4547, -1.6115, -1.4549, -1.2746, -1.6513, -1.4544,\n",
      "        -1.2958, -1.3582, -1.3753, -1.2904, -1.6961, -1.5707, -1.6347, -1.4711,\n",
      "        -1.6436, -1.5730, -1.7564, -1.6574, -1.5618, -1.3382, -1.4743, -1.5573,\n",
      "        -1.7294, -1.6313, -1.6492, -1.4062, -1.3836, -1.5311, -1.6594, -1.6807,\n",
      "        -1.5781, -1.3811, -1.6655, -1.5730, -1.5122, -1.6103, -1.6930, -1.7291,\n",
      "        -1.7547, -1.5609, -1.4616, -1.5056, -1.6343, -1.3518, -1.5619, -1.5595,\n",
      "        -1.6123, -1.4950, -1.3568, -1.4703, -1.5103, -1.5974, -1.7266, -1.6150,\n",
      "        -1.4857, -1.5823, -1.5515, -1.4645, -1.4798, -1.6178, -1.4467, -1.3031,\n",
      "        -1.4586, -1.4180, -1.4977, -1.5579, -1.5632, -1.3650, -1.5311, -1.3915,\n",
      "        -1.5532, -1.4184, -1.5551, -1.6481, -1.3914, -1.2402, -1.2133, -1.4346,\n",
      "        -1.5263, -1.6103, -1.4990, -1.4684, -1.7269, -1.5466, -1.5456, -1.4803,\n",
      "        -1.4554, -1.6865, -1.7221, -1.5808, -1.6611, -1.5397, -1.5166, -1.5989,\n",
      "        -1.5449, -1.5461, -1.6384, -1.8641, -1.7899, -1.4193, -1.7852, -1.5308,\n",
      "        -1.7247, -1.6501, -1.5501, -1.5204, -1.6327, -1.5714, -1.6416, -1.5986,\n",
      "        -1.6671, -1.6498, -1.3258, -1.5754, -1.6839, -1.5139, -1.5660, -1.5219,\n",
      "        -1.6003, -1.5719, -1.7272, -1.7101, -1.7043, -1.4716, -1.4608, -1.4294,\n",
      "        -1.3821, -1.6420, -1.3568, -1.4853, -1.2072, -1.1733, -1.3757, -1.1278,\n",
      "        -1.0802, -1.1571, -1.0929, -0.8722, -1.0422, -1.1083, -0.8295, -1.0265,\n",
      "        -0.9858, -1.0778, -0.9599, -0.9974, -1.0396, -0.9118, -0.8421, -0.9824,\n",
      "        -0.8464, -0.9134, -0.7664, -1.2000, -1.2783, -1.0774, -1.4090, -1.4234,\n",
      "        -1.2754, -1.5343, -1.5678, -1.5638, -1.4999, -1.4500, -1.7606, -1.5367,\n",
      "        -1.8154, -1.5887, -1.4892, -1.5858, -1.8975, -1.5369, -1.4683, -1.5385,\n",
      "        -1.5523, -1.4177, -1.5719, -1.5675, -1.4101, -1.7285, -1.3714, -1.7460,\n",
      "        -1.4396, -1.3729, -1.9154, -1.6820, -1.7328, -1.6689, -1.6206, -1.4403,\n",
      "        -1.4748, -1.5677, -1.6700, -1.4599, -1.5757, -1.6180, -1.4505, -1.4595,\n",
      "        -1.5417, -1.5324, -1.7432, -1.4320, -1.6833, -1.6916, -1.2788, -1.4812,\n",
      "        -1.5730, -1.5243, -1.5944, -1.3333, -1.7153, -1.5679, -1.6058, -1.4051,\n",
      "        -1.4997, -1.3778, -1.7052, -1.5075, -1.6921, -1.3519, -1.6780, -1.6842,\n",
      "        -1.3733, -1.4313, -1.4920, -1.5290, -1.4597, -1.7982, -1.3915, -1.8103,\n",
      "        -1.3018, -1.4087, -1.5325, -1.5553, -1.6311, -1.4932, -1.5601, -1.4599,\n",
      "        -1.4756, -1.6552, -1.7872, -1.4380, -1.5288, -1.6537, -1.3542, -1.7028,\n",
      "        -1.8333, -1.5023, -1.7994, -1.5212, -1.5870, -1.5523, -1.5698, -1.5578,\n",
      "        -1.2033, -1.3121, -1.2371, -1.4591, -1.3103, -1.4709, -1.2621, -1.2847,\n",
      "        -1.1655, -1.0669, -0.8445, -1.2137, -1.0263, -1.0013, -0.8497, -1.0765,\n",
      "        -0.9253, -0.9633, -0.9969, -1.0149, -0.9182, -0.7146, -0.7526, -1.0218,\n",
      "        -1.3412, -1.2124, -1.4922, -1.3514, -1.2403, -1.1594, -1.1788, -1.2741,\n",
      "        -1.2413, -1.2818, -1.1588, -1.3348, -1.1379, -1.2334, -1.2397, -1.1182,\n",
      "        -1.0937, -1.1115, -1.0623, -1.3595, -1.2765, -1.1627, -1.0858, -0.7787,\n",
      "        -0.7284, -0.7160, -1.0843, -1.0641, -1.1984, -1.1291, -1.4044, -1.5965,\n",
      "        -1.6119, -1.5939, -1.4921, -1.5147, -1.5030, -1.5525, -1.6195, -1.7245,\n",
      "        -1.7359, -1.5843, -1.7167, -1.3516, -1.5020, -1.6827, -1.4871, -1.6842,\n",
      "        -1.1970, -1.5389, -1.6446, -1.5161, -1.3337, -1.9102, -1.3094, -1.6359,\n",
      "        -1.7018, -1.6276, -1.5792, -1.5739, -1.6289, -1.7143, -1.6414, -1.6386,\n",
      "        -1.6533, -1.5121, -1.8397, -1.3836, -1.6501, -1.7089, -1.6636, -1.4659,\n",
      "        -1.5357, -1.7703, -1.3176, -1.7665, -1.4897, -1.5277, -1.6024, -1.7435,\n",
      "        -1.5242, -1.6893, -1.6101, -1.7144, -1.4315, -1.5597, -1.5802, -1.4835,\n",
      "        -1.4894, -1.5595, -1.4747, -1.5456, -1.7674, -1.8965, -1.6968, -1.5151,\n",
      "        -1.8504, -1.7414, -1.5178, -1.6020, -1.6473, -1.5514, -1.5533, -1.7430,\n",
      "        -1.6703, -1.8149, -1.5067, -1.3950, -1.5208, -1.6051, -1.7344, -1.5569,\n",
      "        -1.6288, -1.7414, -1.7548, -1.9072, -1.8049, -1.4440, -1.7085, -1.6596,\n",
      "        -1.3558, -1.3833, -1.1881, -1.4518, -1.1752, -1.0716, -1.1198, -1.1718,\n",
      "        -0.8062, -1.2661, -1.1010, -1.0458, -0.9851, -1.2158, -1.2007, -1.2323,\n",
      "        -1.2286, -1.2437, -1.4505, -1.1449, -1.2621, -1.3512, -1.1209, -1.0833,\n",
      "        -0.9648, -1.2254, -1.1549, -1.2517, -1.1470, -1.3611, -1.2458, -1.1496,\n",
      "        -1.2297, -1.0299, -1.0572, -0.8774, -1.3086, -1.0505, -0.7888, -1.0690,\n",
      "        -1.3320, -1.3262, -1.2672, -1.2976, -1.3186, -1.3022, -1.4309, -1.2277,\n",
      "        -1.3380, -0.9502, -1.0755, -1.2078, -1.2201, -1.3298, -1.3684, -1.1911,\n",
      "        -1.3942, -1.3701, -1.3575, -1.5712, -1.6146, -1.5775, -1.7693, -1.4853,\n",
      "        -1.5717, -1.5766, -1.7301, -1.6923, -1.8201, -1.6813, -1.6696, -1.7175,\n",
      "        -1.6135, -1.5773, -1.5665, -1.6982, -1.3584, -1.5218, -1.8297, -1.5434,\n",
      "        -1.6625, -1.6231, -1.7214, -1.4894, -1.6206, -1.5743, -1.5790, -1.6848,\n",
      "        -1.5035, -1.7295, -1.7697, -1.6896, -1.4548, -1.6246, -1.7126, -1.6310,\n",
      "        -1.6281, -1.7392, -1.5570, -1.7442, -1.9008, -1.8460, -1.7163, -1.6574,\n",
      "        -1.5796, -1.7757, -1.7605, -1.6925, -1.5497, -1.8363, -1.6627, -1.8253,\n",
      "        -1.6266, -1.6724, -1.7751, -1.7360, -1.5110, -1.6026, -1.8213, -1.7383,\n",
      "        -1.8419, -1.8855, -1.6261, -1.7007, -2.0005, -1.6197, -1.7599, -1.6327,\n",
      "        -1.6319, -1.5237, -1.7345, -1.6565, -1.4211, -1.7403, -1.7285, -1.5377,\n",
      "        -1.5468, -1.5736, -1.7433, -1.6925, -1.4484, -1.8286, -1.7119, -1.7108,\n",
      "        -1.7237, -1.7047, -1.9052, -1.4869, -1.7491, -1.4327, -1.7155, -1.4352,\n",
      "        -1.4100, -1.3080, -1.5878, -1.1694, -1.0718, -0.9290, -0.9374, -1.1391,\n",
      "        -1.0779, -0.8553, -0.9811, -1.2987, -1.2958, -1.6518, -1.2704, -0.9812,\n",
      "        -1.2171, -1.0874, -1.1081, -1.3938, -1.1728, -1.2653, -1.2836, -1.3732,\n",
      "        -1.0861, -1.1426, -1.0199, -1.1510, -1.1403, -1.0511, -0.9981, -1.0967,\n",
      "        -1.0901, -1.1122, -1.3053, -1.2327, -1.2448, -1.5324, -1.5648, -1.2346,\n",
      "        -0.8801, -0.7462, -0.8232, -0.8845, -0.9616, -1.2531, -1.1925, -1.4498,\n",
      "        -1.4733, -1.5308, -1.6377, -1.6812, -1.4133, -1.4682, -1.3535, -1.6803,\n",
      "        -1.7210, -1.8034, -1.7090, -1.6725, -1.7335, -1.5406, -1.5340, -1.7276,\n",
      "        -1.4017, -1.6336, -1.5570, -1.4211, -1.5182, -1.3557, -1.4705, -1.5557,\n",
      "        -1.5695, -1.7760, -1.4841, -1.4906, -1.7738, -1.6256, -1.5961, -1.3993,\n",
      "        -1.5215, -1.9276, -1.6048, -1.7563, -1.6041, -1.5929, -1.5626, -1.7188,\n",
      "        -1.5203, -1.7665, -1.6867, -1.8533, -1.7181, -1.6007, -1.5910, -1.5842,\n",
      "        -1.5031, -1.6590, -1.6247, -1.6824, -1.5614, -1.7762, -1.9031, -1.6763,\n",
      "        -1.7047, -1.5925, -1.6136, -1.6427, -1.6660, -1.6533, -1.4830, -1.5876,\n",
      "        -1.4925, -1.5528, -1.7416, -1.6850, -1.4760, -1.7744, -1.5024, -1.6892,\n",
      "        -1.6647, -1.8849, -1.4109, -1.4635, -1.7161, -1.7461, -1.5390, -1.7762],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) # Small sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0797, -0.9949, -0.9238, -0.8581, -0.7988, -0.7402, -0.6864, -0.6413,\n",
      "        -0.6007, -0.5637, -0.5303, -0.5034, -0.4781, -0.4516, -0.4438, -0.4478,\n",
      "        -0.4525, -0.4622, -0.4775, -0.4831, -0.4824, -0.4824, -0.4840, -0.4829,\n",
      "        -0.4796, -0.4784, -0.4745, -0.4723, -0.4696, -0.4686, -0.4738, -0.4907,\n",
      "        -0.5179, -0.5654, -0.6264, -0.6943, -0.7702, -0.8488, -0.9312, -1.0187,\n",
      "        -1.1152, -1.2284, -1.3486, -1.4846, -1.6458, -1.8335, -2.0371, -2.2583,\n",
      "        -2.4803, -2.6908, -2.8747, -3.0401, -3.1835, -3.2991, -3.3773, -3.4140,\n",
      "        -3.4004, -3.3335, -3.2181, -3.0745, -2.9110, -2.7374, -2.5585, -2.3805,\n",
      "        -2.1968, -2.0178, -1.8431, -1.6839, -1.5378, -1.3989, -1.2665, -1.1307,\n",
      "        -0.9906, -0.8688, -0.7793, -0.7269, -0.7026, -0.7065, -0.7217, -0.7390,\n",
      "        -0.7422, -0.7402, -0.7296, -0.7031, -0.6639, -0.6237, -0.5844, -0.5494,\n",
      "        -0.5165, -0.4928, -0.4744, -0.4658, -0.4598, -0.4682, -0.4804, -0.4992,\n",
      "        -0.5257, -0.5651, -0.6205, -0.6941, -0.7820, -0.8762, -0.9792, -1.0830,\n",
      "        -1.1860, -1.2925, -1.3998, -1.5069, -1.6152, -1.7351, -1.8683, -2.0259,\n",
      "        -2.2131, -2.4256, -2.6556, -2.8895, -3.1145, -3.3130, -3.4665, -3.5678,\n",
      "        -3.6013, -3.5683, -3.4693, -3.3291, -3.1547, -2.9744, -2.7906, -2.6148,\n",
      "        -2.4369, -2.2563, -2.0721, -1.8849, -1.7012, -1.5338, -1.3869, -1.2611,\n",
      "        -1.1644, -1.0881, -1.0243, -0.9720, -0.9171, -0.8640, -0.8028, -0.7433,\n",
      "        -0.6825, -0.6341, -0.6034, -0.6033, -0.6143, -0.6381, -0.6638, -0.6799,\n",
      "        -0.6787, -0.6769, -0.6661, -0.6596, -0.6539, -0.6556, -0.6553, -0.6656,\n",
      "        -0.6793, -0.7042, -0.7418, -0.7945, -0.8589, -0.9346, -1.0123, -1.0908,\n",
      "        -1.1647, -1.2411, -1.3176, -1.4149, -1.5416, -1.7095, -1.9003, -2.1114,\n",
      "        -2.3280, -2.5345, -2.7175, -2.8897, -3.0498, -3.1876, -3.2975, -3.3698,\n",
      "        -3.3850, -3.3393, -3.2283, -3.0734, -2.8929, -2.7096, -2.5290, -2.3726,\n",
      "        -2.2205, -2.0771, -1.9308, -1.7917, -1.6456, -1.5045, -1.3665, -1.2382,\n",
      "        -1.1116, -1.0024, -0.9111, -0.8395, -0.7838, -0.7525, -0.7340, -0.7211,\n",
      "        -0.7133, -0.7147, -0.7136, -0.7130, -0.7133, -0.7065, -0.6910, -0.6715,\n",
      "        -0.6441, -0.6176, -0.5947, -0.5745, -0.5677, -0.5734, -0.5914, -0.6191,\n",
      "        -0.6562, -0.6960, -0.7349, -0.7681, -0.7970, -0.8236, -0.8433, -0.8648,\n",
      "        -0.8818, -0.9050, -0.9295, -0.9649, -1.0187, -1.1147, -1.2539, -1.4445,\n",
      "        -1.6810, -1.9478, -2.2324, -2.5187, -2.7749, -2.9926, -3.1669, -3.2939,\n",
      "        -3.3613, -3.3867, -3.3694, -3.3181, -3.2216, -3.1101, -2.9802, -2.8434,\n",
      "        -2.6935, -2.5383, -2.3763, -2.2236, -2.0704, -1.9252, -1.7893, -1.6546,\n",
      "        -1.5215, -1.3909, -1.2638, -1.1432, -1.0276, -0.9230, -0.8348, -0.7657,\n",
      "        -0.7085, -0.6715, -0.6397, -0.6155, -0.5917, -0.5774, -0.5623, -0.5571,\n",
      "        -0.5541, -0.5537, -0.5525, -0.5506, -0.5400, -0.5329, -0.5240, -0.5265,\n",
      "        -0.5309, -0.5408, -0.5493, -0.5667, -0.5783, -0.6042, -0.6412, -0.6991,\n",
      "        -0.7725, -0.8657, -0.9748, -1.1060, -1.2478, -1.4170, -1.6060, -1.8013,\n",
      "        -2.0041, -2.2108, -2.4030, -2.5873, -2.7657, -2.9232, -3.0525, -3.1439,\n",
      "        -3.1812, -3.1558, -3.0807, -2.9688, -2.8356, -2.6961, -2.5628, -2.4344,\n",
      "        -2.3059, -2.1768, -2.0468, -1.8984, -1.7287, -1.5682, -1.4171, -1.2679,\n",
      "        -1.1438, -1.0552, -0.9667, -0.8872, -0.8263, -0.7788, -0.7338, -0.7042,\n",
      "        -0.6748, -0.6494, -0.6202, -0.5968, -0.5709, -0.5468, -0.5222, -0.5032,\n",
      "        -0.4861, -0.4792, -0.4831, -0.4847, -0.4801, -0.4838, -0.4795, -0.4695,\n",
      "        -0.4577, -0.4593, -0.4465, -0.4412, -0.4448, -0.4661, -0.4885, -0.5210,\n",
      "        -0.5578, -0.5983, -0.6380, -0.7073, -0.8097, -0.9396, -1.0979, -1.2939,\n",
      "        -1.5008, -1.7206, -1.9423, -2.1616, -2.3708, -2.5623, -2.7179, -2.8489,\n",
      "        -2.9510, -3.0242, -3.0765, -3.1207, -3.1452, -3.1465, -3.1118, -3.0394,\n",
      "        -2.9214, -2.7769, -2.6068, -2.4276, -2.2343, -2.0452, -1.8542, -1.6732,\n",
      "        -1.4998, -1.3461, -1.2031, -1.0734, -0.9532, -0.8518, -0.7621, -0.6884,\n",
      "        -0.6280, -0.5819, -0.5368, -0.5053, -0.4781, -0.4612, -0.4546, -0.4563,\n",
      "        -0.4518, -0.4577, -0.4681, -0.4731, -0.4739, -0.4824, -0.4864, -0.4836,\n",
      "        -0.4826, -0.4855, -0.4862, -0.4853, -0.4914, -0.5074, -0.5273, -0.5593,\n",
      "        -0.5932, -0.6382, -0.7148, -0.8072, -0.9102, -1.0490, -1.2074, -1.3741,\n",
      "        -1.5799, -1.8253, -2.0765, -2.3253, -2.5468, -2.7286, -2.8550, -2.9493,\n",
      "        -3.0117, -3.0501, -3.0596, -3.0502, -3.0049, -2.9375, -2.8491, -2.7457,\n",
      "        -2.6359, -2.5251, -2.4159, -2.2942, -2.1545, -1.9922, -1.8248, -1.6577,\n",
      "        -1.5015, -1.3710, -1.2643, -1.1738, -1.0765, -0.9886, -0.9018, -0.8206,\n",
      "        -0.7427, -0.6827, -0.6381, -0.6044, -0.5763, -0.5544, -0.5360, -0.5163,\n",
      "        -0.5010, -0.4921, -0.4828, -0.4796, -0.4773, -0.4737, -0.4725, -0.4779,\n",
      "        -0.4796, -0.4833, -0.4889, -0.4964, -0.5063, -0.5192, -0.5337, -0.5501,\n",
      "        -0.5675, -0.5911, -0.6238, -0.6711, -0.7433, -0.8307, -0.9233, -1.0302,\n",
      "        -1.1394, -1.2457, -1.3570, -1.4870, -1.6171, -1.7593, -1.9088, -2.0682,\n",
      "        -2.2142, -2.3499, -2.4622, -2.5452, -2.5990, -2.6452, -2.6904, -2.7559,\n",
      "        -2.8504, -2.9750, -3.1111, -3.2609, -3.4012, -3.5267, -3.6280, -3.7084,\n",
      "        -3.7348, -3.7219, -3.6740, -3.5761, -3.4307, -3.2712, -3.0921, -2.8892,\n",
      "        -2.6808, -2.4697, -2.2520, -2.0380, -1.8317, -1.6474, -1.4899, -1.3601,\n",
      "        -1.2295, -1.1218, -1.0319, -0.9567, -0.8910, -0.8587, -0.8319, -0.8033,\n",
      "        -0.7660, -0.7373, -0.7042, -0.6735, -0.6461, -0.6349, -0.6148, -0.5984,\n",
      "        -0.5808, -0.5644, -0.5438, -0.5314, -0.5170, -0.5029, -0.4875, -0.4744,\n",
      "        -0.4700, -0.4703, -0.4691, -0.4714, -0.4752, -0.4666, -0.4598, -0.4624,\n",
      "        -0.4609, -0.4575, -0.4565, -0.4555, -0.4496, -0.4517, -0.4520, -0.4539,\n",
      "        -0.4585, -0.4663, -0.4735, -0.4913, -0.5101, -0.5289, -0.5503, -0.5745,\n",
      "        -0.5933, -0.6146, -0.6417, -0.7120, -0.8152, -0.9495, -1.1176, -1.3219,\n",
      "        -1.5269, -1.7501, -1.9895, -2.2339, -2.4507, -2.6322, -2.7601, -2.8289,\n",
      "        -2.8360, -2.8096, -2.7489, -2.6599, -2.5532, -2.4386, -2.3101, -2.1800,\n",
      "        -2.0514, -1.9234, -1.7977, -1.6771, -1.5579, -1.4421, -1.3242, -1.2106,\n",
      "        -1.0998, -0.9936, -0.8876, -0.7970, -0.7131, -0.6433, -0.5836, -0.5366,\n",
      "        -0.4972, -0.4721, -0.4523, -0.4402, -0.4308, -0.4326, -0.4298, -0.4297,\n",
      "        -0.4276, -0.4341, -0.4329, -0.4472, -0.4647, -0.4971, -0.5333, -0.5821,\n",
      "        -0.6374, -0.7066, -0.7751, -0.8523, -0.9373, -1.0420, -1.1624, -1.3157,\n",
      "        -1.4915, -1.6990, -1.9085, -2.1276, -2.3406, -2.5464, -2.7233, -2.8816,\n",
      "        -3.0023, -3.0875, -3.1368, -3.1630, -3.1513, -3.1118, -3.0371, -2.9328,\n",
      "        -2.7909, -2.6360, -2.4692, -2.3029, -2.1351, -1.9783, -1.8201, -1.6628,\n",
      "        -1.5014, -1.3578, -1.2180, -1.0926, -0.9863, -0.9090, -0.8404, -0.7861,\n",
      "        -0.7390, -0.6997, -0.6598, -0.6272, -0.5984, -0.5724, -0.5445, -0.5191,\n",
      "        -0.4880, -0.4606, -0.4381, -0.4268, -0.4158, -0.4147, -0.4228, -0.4436,\n",
      "        -0.4735, -0.5244, -0.5852, -0.6547, -0.7317, -0.8032, -0.8702, -0.9543,\n",
      "        -1.0574, -1.1781, -1.3417, -1.5422, -1.7678, -2.0123, -2.2693, -2.5161,\n",
      "        -2.7524, -2.9676, -3.1483, -3.2924, -3.3852, -3.4185, -3.3792, -3.2817,\n",
      "        -3.1328, -2.9521, -2.7454, -2.5399, -2.3371, -2.1475, -1.9793, -1.8398,\n",
      "        -1.7234, -1.6245, -1.5328, -1.4434, -1.3488, -1.2492, -1.1487, -1.0531,\n",
      "        -0.9642, -0.8915, -0.8301, -0.7808, -0.7365, -0.6968, -0.6527, -0.6161,\n",
      "        -0.5792, -0.5525, -0.5289, -0.5200, -0.5027, -0.4896, -0.4742, -0.4635,\n",
      "        -0.4477, -0.4460, -0.4504, -0.4617, -0.4751, -0.4929, -0.5073, -0.5212,\n",
      "        -0.5351, -0.5524, -0.5722, -0.6014, -0.6456, -0.6988, -0.7617, -0.8450,\n",
      "        -0.9553, -1.0998, -1.2831, -1.5168, -1.7941, -2.1110, -2.4486, -2.8074,\n",
      "        -3.1460, -3.4454, -3.6746, -3.8203, -3.8681, -3.8324, -3.7302, -3.5838,\n",
      "        -3.4108, -3.2261, -3.0425, -2.8690, -2.7049, -2.5481, -2.3970, -2.2551,\n",
      "        -2.1055, -1.9532, -1.7976, -1.6423, -1.4824, -1.3360, -1.1965, -1.0738,\n",
      "        -0.9588, -0.8660, -0.7814, -0.7115, -0.6566, -0.6216, -0.5949, -0.5757,\n",
      "        -0.5609, -0.5440, -0.5266, -0.5137, -0.4984, -0.4881, -0.4841, -0.4825,\n",
      "        -0.4820, -0.4949, -0.5113, -0.5411, -0.5865, -0.6468, -0.7258, -0.8322,\n",
      "        -0.9649, -1.1396, -1.3659, -1.6431, -1.9573, -2.2880, -2.5986, -2.8725,\n",
      "        -3.0817, -3.2273, -3.3033, -3.3248, -3.2905, -3.2235, -3.1266, -3.0227,\n",
      "        -2.9121, -2.8111, -2.7043, -2.5995, -2.4813, -2.3539, -2.2103, -2.0718,\n",
      "        -1.9267, -1.7962, -1.6674, -1.5426, -1.4104, -1.2835, -1.1550, -1.0463,\n",
      "        -0.9525, -0.8839, -0.8325, -0.7998, -0.7711, -0.7478, -0.7221, -0.6980,\n",
      "        -0.6712, -0.6601, -0.6577, -0.6756, -0.7150, -0.7785, -0.8431, -0.9244,\n",
      "        -1.0262, -1.1561, -1.3145, -1.5219, -1.7822, -2.0763, -2.3806, -2.6867,\n",
      "        -2.9663, -3.1816, -3.3204, -3.3875, -3.3812, -3.3179, -3.2147, -3.0858,\n",
      "        -2.9392, -2.7912, -2.6484, -2.5212, -2.4108, -2.3228, -2.2409, -2.1528,\n",
      "        -2.0390, -1.9094, -1.7548, -1.5906, -1.4208, -1.2712, -1.1401, -1.0338,\n",
      "        -0.9433, -0.8721, -0.8090, -0.7549, -0.7109, -0.6793, -0.6647, -0.6722,\n",
      "        -0.6969, -0.7316, -0.7818, -0.8461, -0.9158, -1.0143, -1.1508, -1.3142,\n",
      "        -1.5064, -1.7410, -2.0027, -2.2849, -2.5764, -2.8717, -3.1556, -3.4071,\n",
      "        -3.6075, -3.7559, -3.8407, -3.8533, -3.7934, -3.6755, -3.5125, -3.3245,\n",
      "        -3.1273, -2.9421, -2.7701, -2.6051, -2.4408, -2.2784, -2.1063, -1.9308,\n",
      "        -1.7572, -1.5961, -1.4427, -1.3063, -1.1842, -1.0813, -0.9990, -0.9365,\n",
      "        -0.8839, -0.8450, -0.8202, -0.8072, -0.8151, -0.8479, -0.9070, -0.9924,\n",
      "        -1.0971, -1.2188, -1.3647, -1.5358, -1.7322, -1.9559, -2.2003, -2.4591,\n",
      "        -2.7193, -2.9683, -3.1741, -3.3155, -3.3714, -3.3465, -3.2477, -3.0938,\n",
      "        -2.9204, -2.7492, -2.5781, -2.4217, -2.2841, -2.1466, -2.0104, -1.8793,\n",
      "        -1.7383, -1.5977, -1.4585, -1.3294, -1.2158, -1.1181, -1.0417, -1.0054,\n",
      "        -1.0037, -1.0384, -1.1167, -1.2438, -1.4030, -1.5846, -1.7942, -2.0435])\n"
     ]
    }
   ],
   "source": [
    "print(labels.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Summary: 2 Conv&Pool layers, RNN with 2 hidden states of 200 size, 1000 sequence length, 5 epochs: 1.05 RMSE. First good performance, albeit on training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
