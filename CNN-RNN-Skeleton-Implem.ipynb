{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "\n",
    "# relative path to npz files\n",
    "path = 'Measurements'\n",
    "file_name = 'output_batch_%d.npz'\n",
    "\n",
    "# Training\n",
    "# how many frames to load\n",
    "frames_num = 10\n",
    "data_frames = []\n",
    "data_forces = []\n",
    "for i in range(frames_num):\n",
    "    file_path = os.path.join(path,file_name %i)\n",
    "    data = np.load(file_path)\n",
    "    data_frames.append(data['frames'])\n",
    "    data_forces.append(data['forces'])\n",
    "\n",
    "combined_frames = np.concatenate(data_frames, axis=0)\n",
    "combined_forces = np.concatenate(data_forces, axis=0)\n",
    "\n",
    "# Test\n",
    "# file_path = os.path.join(path, file_name %11)\n",
    "# test_data = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = combined_frames\n",
    "targets_train = combined_forces[:,2]\n",
    "# features_test = test_data['frames']\n",
    "# targets_test = test_data['forces']\n",
    "# targets_test = targets_test[:,2]\n",
    "\n",
    "# Normalization\n",
    "# frames_mean = np.mean(features_train)\n",
    "# frames_std = np.std(features_train)\n",
    "\n",
    "# forces_z_mean = np.mean(targets_train)\n",
    "# forces_z_std = np.std(targets_train)\n",
    "\n",
    "# features_train = (features_train-frames_mean)/frames_std\n",
    "# targets_train = (targets_train-forces_z_mean)/forces_z_std\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train)\n",
    "\n",
    "# create feature and targets tensor for test set. \n",
    "# featuresTest = torch.from_numpy(features_test)\n",
    "# targetsTest = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 9, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(9, 18, kernel_size = 5, stride = 1, padding = 2)\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(18 * 16 * 30, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ELU()(self.conv2(x)))\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (100, 10, 18 * 16 * 30))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        \n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        #print(\"Size of h0:\", h0.size())  # Print size of h0 - Debugging - Obsolete\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out) \n",
    "        out = torch.reshape(out, (1000, 1)) # go back to compare to labels\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1000\n",
    "num_epochs = 5\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "# test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 200  # hidden layer dimension\n",
    "layer_dim = 2     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 1  Loss: 4.382874011993408\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 2  Loss: 29.53947639465332\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 3  Loss: 3.8794803619384766\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 4  Loss: 3.9011552333831787\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 5  Loss: 11.054900169372559\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 6  Loss: 9.62796688079834\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 7  Loss: 3.55464506149292\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 8  Loss: 1.0004968643188477\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 9  Loss: 2.4621083736419678\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 10  Loss: 4.139577865600586\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 11  Loss: 4.493381977081299\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 12  Loss: 2.222580671310425\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 13  Loss: 1.2223070859909058\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 14  Loss: 1.2486510276794434\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 15  Loss: 2.060047149658203\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 16  Loss: 2.7435965538024902\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 17  Loss: 1.8221111297607422\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 18  Loss: 1.0457526445388794\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 19  Loss: 1.0686774253845215\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 20  Loss: 1.3369929790496826\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 21  Loss: 1.795830488204956\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 22  Loss: 1.3004722595214844\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 23  Loss: 1.2495152950286865\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 24  Loss: 0.9700207710266113\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 25  Loss: 1.1631911993026733\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 26  Loss: 1.671054482460022\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 27  Loss: 1.3234663009643555\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 28  Loss: 1.0239081382751465\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 29  Loss: 0.9398297071456909\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 30  Loss: 1.046042561531067\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 31  Loss: 1.3021944761276245\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 32  Loss: 1.0712897777557373\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 33  Loss: 1.2061976194381714\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 34  Loss: 0.9725638031959534\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 35  Loss: 1.086976170539856\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 36  Loss: 1.4314186573028564\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 37  Loss: 1.090449333190918\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 38  Loss: 0.9484027028083801\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 39  Loss: 0.9362361431121826\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 40  Loss: 1.0237829685211182\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 41  Loss: 1.165683388710022\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 42  Loss: 1.0053797960281372\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 43  Loss: 1.2071905136108398\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 44  Loss: 0.9609887599945068\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 45  Loss: 1.0957081317901611\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 46  Loss: 1.422011375427246\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 47  Loss: 1.0480988025665283\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 48  Loss: 0.9415493011474609\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 49  Loss: 0.9629098176956177\n",
      "Size of x: torch.Size([100, 10, 8640])\n",
      "Iteration: 50  Loss: 1.066089391708374\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 100\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('~~~BEGINNING OF DATASET~~~')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        # print(images.shape)  # Add this line to check the shape of images - Debugging purposes\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data) #* (forces_z_std^2)\n",
    "        # Print Loss\n",
    "        if count % 1 == 0: # for now print for every iteration\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item())) # * (forces_z_std^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(4.3829), tensor(29.5395), tensor(3.8795), tensor(3.9012), tensor(11.0549), tensor(9.6280), tensor(3.5546), tensor(1.0005), tensor(2.4621), tensor(4.1396), tensor(4.4934), tensor(2.2226), tensor(1.2223), tensor(1.2487), tensor(2.0600), tensor(2.7436), tensor(1.8221), tensor(1.0458), tensor(1.0687), tensor(1.3370), tensor(1.7958), tensor(1.3005), tensor(1.2495), tensor(0.9700), tensor(1.1632), tensor(1.6711), tensor(1.3235), tensor(1.0239), tensor(0.9398), tensor(1.0460), tensor(1.3022), tensor(1.0713), tensor(1.2062), tensor(0.9726), tensor(1.0870), tensor(1.4314), tensor(1.0904), tensor(0.9484), tensor(0.9362), tensor(1.0238), tensor(1.1657), tensor(1.0054), tensor(1.2072), tensor(0.9610), tensor(1.0957), tensor(1.4220), tensor(1.0481), tensor(0.9415), tensor(0.9629), tensor(1.0661)]\n",
      "1.0428618\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)\n",
    "print(np.sqrt(np.mean(loss_list[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6003, -1.7937, -1.8790, -2.0598, -1.7151, -2.0651, -2.0095, -1.9034,\n",
      "        -1.8396, -1.9650, -1.7734, -1.7206, -1.6702, -1.6905, -1.5918, -1.6743,\n",
      "        -1.3881, -1.3098, -1.6614, -1.6196, -1.7900, -1.5775, -1.7072, -1.9229,\n",
      "        -1.7543, -1.9563, -1.8537, -1.9506, -1.9106, -1.9426, -1.7179, -1.8091,\n",
      "        -1.8920, -2.0089, -1.8024, -1.9717, -1.9195, -1.8307, -1.8884, -1.8129,\n",
      "        -1.4820, -1.5080, -1.6399, -1.6472, -1.4307, -1.5776, -1.5851, -1.7084,\n",
      "        -1.7957, -1.8191, -1.6695, -1.6784, -1.8468, -2.1363, -1.8025, -1.8080,\n",
      "        -1.7862, -1.9253, -1.8555, -1.8694, -1.8122, -1.7325, -1.9357, -1.8187,\n",
      "        -1.6443, -1.7599, -1.2610, -1.6594, -1.6417, -1.5341, -1.4309, -1.5400,\n",
      "        -1.8200, -1.8791, -1.9626, -1.7221, -1.9307, -1.7624, -1.9269, -1.8967,\n",
      "        -1.7374, -1.7857, -1.8224, -1.9707, -1.9288, -1.9164, -1.7287, -1.8290,\n",
      "        -1.7544, -1.7362, -1.5518, -1.4585, -1.6048, -1.6975, -1.5010, -1.6555,\n",
      "        -1.5630, -1.5473, -1.6018, -1.7613, -1.7883, -1.6439, -1.6930, -1.9708,\n",
      "        -1.8993, -1.8717, -1.8750, -1.6603, -1.6645, -1.7836, -1.6936, -1.6400,\n",
      "        -2.1456, -1.8345, -1.5974, -1.7252, -1.3665, -1.6806, -1.6751, -1.5746,\n",
      "        -1.6619, -1.6470, -1.8249, -1.6718, -1.9665, -1.8625, -1.6604, -1.7777,\n",
      "        -1.9201, -1.8693, -1.6785, -1.7868, -1.7481, -1.9151, -1.6945, -1.8304,\n",
      "        -1.7986, -1.7569, -1.4512, -1.5632, -1.4254, -1.5829, -1.7110, -1.5323,\n",
      "        -1.6330, -1.7885, -1.5853, -1.7566, -1.8158, -1.9642, -1.5617, -1.7574,\n",
      "        -1.8699, -1.9168, -2.0348, -1.9115, -2.0257, -1.5807, -1.9573, -1.8968,\n",
      "        -1.8674, -1.7584, -1.9948, -1.6432, -1.5845, -1.4166, -1.5823, -1.7777,\n",
      "        -1.4716, -1.6289, -1.4504, -1.4129, -1.7249, -1.5983, -1.6616, -1.6534,\n",
      "        -1.9192, -1.5915, -1.8062, -2.0203, -1.7115, -1.7933, -1.7680, -1.9631,\n",
      "        -1.9978, -1.8877, -1.7646, -1.7796, -1.8160, -1.7610, -1.7218, -1.6950,\n",
      "        -1.5055, -1.5650, -1.6238, -1.6659, -1.6720, -1.6913, -1.5780, -1.5802,\n",
      "        -1.5533, -1.5127, -1.8130, -1.7127, -2.0606, -1.7649, -1.6672, -1.9269,\n",
      "        -1.8167, -1.9754, -1.8240, -1.7498, -2.0435, -1.7915, -1.6958, -1.8822,\n",
      "        -1.8415, -1.8684, -1.8324, -1.8181, -1.7759, -1.6192, -2.0394, -1.8425,\n",
      "        -1.7972, -1.6556, -1.8941, -1.8339, -1.8338, -1.9255, -1.7420, -1.8115,\n",
      "        -1.8616, -1.8765, -1.8599, -1.8134, -1.8758, -1.7644, -1.7384, -1.9105,\n",
      "        -1.7385, -1.8523, -1.8959, -1.9677, -1.8017, -1.7871, -1.6625, -1.7430,\n",
      "        -1.8173, -1.7470, -1.5303, -1.7109, -2.0479, -1.9412, -1.6678, -1.7626,\n",
      "        -1.5981, -1.5499, -1.9062, -1.7009, -1.9130, -1.5971, -1.8862, -1.9209,\n",
      "        -1.8049, -1.7687, -1.7200, -1.7863, -1.7170, -1.7228, -1.7057, -1.7585,\n",
      "        -1.7202, -1.9772, -1.8173, -1.7558, -1.7827, -1.6579, -1.5527, -1.8250,\n",
      "        -1.6657, -1.5142, -1.8798, -1.8644, -1.7628, -1.7617, -1.7817, -1.5638,\n",
      "        -1.7147, -1.6328, -1.8181, -1.5424, -1.7551, -1.9486, -1.6779, -2.0137,\n",
      "        -1.8863, -1.5652, -1.8880, -1.7980, -1.7571, -1.8335, -2.0014, -1.8181,\n",
      "        -1.8025, -1.8013, -1.8097, -1.8487, -1.7340, -1.7315, -1.6868, -1.5989,\n",
      "        -1.6574, -1.7290, -1.7538, -1.6586, -1.7094, -1.8422, -1.7448, -2.0038,\n",
      "        -1.7789, -1.8321, -1.8191, -1.8013, -1.9334, -1.7898, -1.6972, -1.7541,\n",
      "        -1.7237, -1.7672, -1.7957, -1.8416, -1.8399, -2.1523, -1.9600, -1.7841,\n",
      "        -1.8131, -1.9732, -1.8173, -1.7608, -1.7543, -1.8515, -1.9732, -2.0524,\n",
      "        -1.9464, -1.8342, -1.8283, -1.7902, -1.8813, -1.7994, -1.8241, -1.5302,\n",
      "        -1.9448, -1.8137, -1.8565, -1.9176, -1.7490, -1.8477, -1.9273, -1.7126,\n",
      "        -1.8826, -1.5935, -1.8551, -1.7671, -1.6546, -1.7735, -1.6596, -1.7527,\n",
      "        -1.6493, -1.8933, -1.6878, -1.4961, -1.7550, -1.9838, -1.9585, -1.7009,\n",
      "        -1.7592, -1.9306, -1.7939, -1.8764, -1.7616, -1.5421, -1.8256, -1.9028,\n",
      "        -1.7418, -1.5155, -1.6455, -1.7944, -1.8463, -1.6216, -1.6372, -1.5893,\n",
      "        -1.8865, -2.0781, -1.8291, -1.5861, -1.7808, -1.5570, -1.6023, -1.5947,\n",
      "        -1.7374, -1.5711, -1.6785, -1.9051, -1.7816, -1.7016, -1.6402, -1.7858,\n",
      "        -1.4645, -1.5786, -1.6613, -1.4551, -1.8020, -1.7998, -1.7275, -1.7887,\n",
      "        -1.7257, -1.8759, -1.7186, -1.8556, -1.7432, -1.7531, -1.8549, -1.7821,\n",
      "        -1.9223, -1.6750, -1.8618, -1.9365, -1.7845, -1.8430, -1.7357, -1.7965,\n",
      "        -1.8632, -1.9496, -1.7563, -1.7976, -1.9780, -1.6564, -1.8124, -1.8652,\n",
      "        -1.9060, -1.7292, -2.0447, -1.9611, -1.8379, -1.8510, -1.9687, -1.6637,\n",
      "        -1.7087, -1.7836, -1.6792, -1.6126, -1.9238, -1.9740, -1.6594, -1.6796,\n",
      "        -1.6717, -1.7516, -1.6649, -1.7050, -1.7322, -1.5720, -1.7281, -1.6248,\n",
      "        -1.7696, -1.6953, -1.4940, -1.7134, -1.7029, -1.5999, -1.5563, -1.4347,\n",
      "        -1.5001, -1.6179, -1.5756, -1.7311, -1.5152, -1.6649, -1.6123, -1.6529,\n",
      "        -1.6817, -1.6786, -1.7174, -1.8517, -1.8469, -1.7621, -1.8735, -2.1205,\n",
      "        -1.8371, -2.1383, -1.8226, -1.7964, -1.9797, -1.9253, -1.8737, -1.9827,\n",
      "        -1.8365, -1.7044, -1.7316, -1.6182, -1.6863, -1.7095, -1.7639, -2.0372,\n",
      "        -1.7446, -1.8738, -1.8278, -1.7326, -1.9642, -1.4749, -1.6326, -1.6398,\n",
      "        -1.9232, -1.8009, -1.7625, -1.9457, -1.6597, -1.6594, -1.6985, -1.7513,\n",
      "        -1.6402, -1.8072, -1.7008, -1.8280, -1.8003, -1.6039, -1.6607, -1.6499,\n",
      "        -1.6931, -1.7455, -1.6653, -1.5430, -1.8393, -1.9378, -1.8455, -1.6360,\n",
      "        -1.6596, -1.6262, -1.8249, -1.6492, -1.6883, -1.5355, -2.0312, -1.7830,\n",
      "        -1.7125, -1.6194, -1.8624, -1.8039, -1.7098, -1.5455, -1.8613, -1.6771,\n",
      "        -1.8351, -1.9870, -1.6595, -1.8554, -1.6559, -1.8073, -1.6505, -1.7962,\n",
      "        -1.7943, -1.8738, -1.9913, -1.8859, -1.8564, -1.8977, -1.6988, -1.8604,\n",
      "        -1.8801, -1.8532, -1.7288, -1.8234, -1.6888, -1.9660, -1.6726, -1.5890,\n",
      "        -1.8383, -1.8405, -1.8771, -1.8669, -1.8039, -1.6632, -1.7336, -1.9619,\n",
      "        -1.7535, -1.5761, -1.6713, -1.7513, -1.7486, -1.7026, -1.6162, -1.6624,\n",
      "        -1.6109, -1.5389, -1.4137, -1.5142, -1.7588, -1.5333, -1.5566, -1.7625,\n",
      "        -1.4980, -1.6269, -1.5200, -1.6977, -1.6677, -1.6193, -1.2675, -1.4894,\n",
      "        -1.5795, -1.5749, -1.2830, -1.2901, -1.3728, -1.4603, -1.3511, -1.5147,\n",
      "        -1.5220, -1.5599, -1.5469, -1.4217, -1.4266, -1.4246, -1.7518, -1.9481,\n",
      "        -1.4510, -1.6135, -1.7625, -1.5635, -1.6687, -1.8173, -1.6205, -1.7656,\n",
      "        -1.7681, -2.0013, -1.8930, -1.9860, -1.8214, -1.9432, -2.0411, -1.9900,\n",
      "        -1.7927, -1.7884, -2.1196, -1.9683, -1.9735, -1.9327, -1.9842, -1.8229,\n",
      "        -1.7173, -1.8244, -1.7012, -1.6266, -1.7750, -1.9314, -1.6733, -1.6654,\n",
      "        -1.7225, -1.7577, -1.6782, -1.7446, -1.8593, -1.5562, -1.8907, -2.0272,\n",
      "        -1.6669, -1.7719, -2.0482, -1.6613, -1.8937, -1.7892, -1.7294, -1.5459,\n",
      "        -1.7579, -1.9638, -1.5885, -1.8115, -1.8406, -1.7313, -1.6535, -1.8289,\n",
      "        -1.7463, -1.7128, -1.8997, -2.1206, -1.6327, -1.7116, -1.7848, -1.7799,\n",
      "        -1.8475, -1.6857, -1.7007, -1.6011, -1.7333, -1.6983, -1.8480, -1.8543,\n",
      "        -1.8272, -1.7298, -1.6859, -1.7149, -1.6389, -1.5878, -1.9274, -1.9530,\n",
      "        -1.6580, -1.8314, -1.6508, -1.9331, -1.8310, -1.8151, -1.7393, -1.6946,\n",
      "        -1.7902, -1.8827, -1.9153, -1.7429, -1.8796, -1.9077, -1.7080, -1.7587,\n",
      "        -1.8039, -1.7639, -1.7043, -1.7540, -1.6665, -1.7725, -1.6283, -1.6922,\n",
      "        -1.7186, -1.5691, -1.6987, -1.4884, -1.6860, -1.4965, -1.7479, -1.7369,\n",
      "        -1.5941, -1.4014, -1.6614, -1.4565, -1.3889, -1.2490, -1.4926, -1.4658,\n",
      "        -1.7083, -1.6244, -1.6623, -1.4694, -1.4851, -1.5166, -0.9408, -1.3987,\n",
      "        -1.4385, -1.5532, -1.5700, -1.2264, -1.1301, -1.2560, -1.4627, -1.3694,\n",
      "        -0.9764, -1.2838, -1.4778, -1.6409, -1.4739, -1.6516, -1.5538, -1.7071,\n",
      "        -1.6102, -1.6577, -1.5231, -1.6551, -1.7127, -1.6778, -1.6674, -1.8211,\n",
      "        -1.4279, -1.8672, -1.6708, -1.7411, -1.7020, -1.6996, -1.9371, -1.8628,\n",
      "        -1.7629, -2.0063, -1.9644, -1.8931, -1.8679, -1.9712, -1.8805, -1.8220,\n",
      "        -2.1190, -1.8809, -1.7556, -1.8438, -1.8154, -1.8532, -1.6446, -1.7153,\n",
      "        -1.8275, -1.6867, -1.9496, -2.0026, -1.8000, -1.9852, -1.7196, -1.8639,\n",
      "        -1.8823, -1.7030, -1.7377, -1.7454, -1.9720, -1.6985, -1.7836, -1.7320,\n",
      "        -1.8430, -1.9776, -1.7986, -1.9286, -1.8543, -1.6170, -1.7710, -1.8758,\n",
      "        -1.7133, -1.7275, -1.7862, -1.9759, -1.8035, -1.7499, -1.8159, -1.6570,\n",
      "        -1.8195, -1.6757, -1.7722, -1.5594, -1.9395, -1.5591, -1.7871, -1.6533,\n",
      "        -1.6530, -1.5049, -1.8538, -1.6920, -2.0544, -1.8965, -1.7444, -1.7611,\n",
      "        -1.6498, -1.7082, -1.9598, -1.6885, -1.8387, -1.7399, -1.9568, -1.6556,\n",
      "        -1.8025, -1.6844, -1.7114, -1.8587, -1.7548, -1.8490, -1.8756, -1.8313,\n",
      "        -1.8584, -1.7930, -1.7974, -1.8688, -1.6255, -1.8462, -1.7012, -1.7372,\n",
      "        -2.0045, -1.8476, -1.9530, -1.9076, -1.7920, -1.6701, -1.6394, -1.4482,\n",
      "        -1.6622, -1.4648, -1.6090, -1.7490, -1.5304, -1.5845, -1.5845, -1.6002,\n",
      "        -1.6128, -1.6133, -1.3558, -1.3670, -1.6635, -1.6431, -1.5173, -1.2620,\n",
      "        -1.3731, -1.2465, -1.4425, -1.1595, -0.7913, -1.3204, -1.2280, -1.5303,\n",
      "        -1.4922, -1.3616, -1.1978, -1.4264, -1.5118, -1.5604, -1.3975, -1.3919,\n",
      "        -1.7320, -1.8185, -1.6365, -1.6576, -1.5945, -1.7959, -1.7995, -1.8174,\n",
      "        -1.7973, -1.7013, -1.7482, -1.8361, -1.8531, -1.8402, -1.8052, -1.9120,\n",
      "        -1.8020, -1.8447, -1.8155, -1.8794, -2.0383, -2.2705, -2.0059, -1.6592,\n",
      "        -1.8892, -1.7967, -1.9746, -1.8014, -1.8504, -1.9514, -1.9346, -2.0875,\n",
      "        -1.6680, -1.9205, -1.9812, -1.8732, -1.7389, -1.8577, -1.7576, -1.6488,\n",
      "        -1.7183, -1.9108, -1.9130, -1.7687, -1.7957, -1.9366, -1.8270, -1.7893,\n",
      "        -1.6392, -1.6419, -1.6647, -1.9868, -1.6324, -1.9160, -1.7316, -1.7926,\n",
      "        -1.7343, -1.5949, -1.6905, -1.8141, -1.8451, -1.8178, -1.8828, -1.6675,\n",
      "        -1.6494, -1.6616, -1.7615, -1.8291, -1.9484, -1.5149, -1.8824, -1.9587,\n",
      "        -1.6857, -1.8288, -1.6647, -1.9194, -1.8078, -1.8132, -1.9000, -1.6377,\n",
      "        -1.7698, -1.6834, -1.9321, -1.6700, -1.8107, -1.6941, -1.7505, -1.8538],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) # Small sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0797, -0.9949, -0.9238, -0.8581, -0.7988, -0.7402, -0.6864, -0.6413,\n",
      "        -0.6007, -0.5637, -0.5303, -0.5034, -0.4781, -0.4516, -0.4438, -0.4478,\n",
      "        -0.4525, -0.4622, -0.4775, -0.4831, -0.4824, -0.4824, -0.4840, -0.4829,\n",
      "        -0.4796, -0.4784, -0.4745, -0.4723, -0.4696, -0.4686, -0.4738, -0.4907,\n",
      "        -0.5179, -0.5654, -0.6264, -0.6943, -0.7702, -0.8488, -0.9312, -1.0187,\n",
      "        -1.1152, -1.2284, -1.3486, -1.4846, -1.6458, -1.8335, -2.0371, -2.2583,\n",
      "        -2.4803, -2.6908, -2.8747, -3.0401, -3.1835, -3.2991, -3.3773, -3.4140,\n",
      "        -3.4004, -3.3335, -3.2181, -3.0745, -2.9110, -2.7374, -2.5585, -2.3805,\n",
      "        -2.1968, -2.0178, -1.8431, -1.6839, -1.5378, -1.3989, -1.2665, -1.1307,\n",
      "        -0.9906, -0.8688, -0.7793, -0.7269, -0.7026, -0.7065, -0.7217, -0.7390,\n",
      "        -0.7422, -0.7402, -0.7296, -0.7031, -0.6639, -0.6237, -0.5844, -0.5494,\n",
      "        -0.5165, -0.4928, -0.4744, -0.4658, -0.4598, -0.4682, -0.4804, -0.4992,\n",
      "        -0.5257, -0.5651, -0.6205, -0.6941, -0.7820, -0.8762, -0.9792, -1.0830,\n",
      "        -1.1860, -1.2925, -1.3998, -1.5069, -1.6152, -1.7351, -1.8683, -2.0259,\n",
      "        -2.2131, -2.4256, -2.6556, -2.8895, -3.1145, -3.3130, -3.4665, -3.5678,\n",
      "        -3.6013, -3.5683, -3.4693, -3.3291, -3.1547, -2.9744, -2.7906, -2.6148,\n",
      "        -2.4369, -2.2563, -2.0721, -1.8849, -1.7012, -1.5338, -1.3869, -1.2611,\n",
      "        -1.1644, -1.0881, -1.0243, -0.9720, -0.9171, -0.8640, -0.8028, -0.7433,\n",
      "        -0.6825, -0.6341, -0.6034, -0.6033, -0.6143, -0.6381, -0.6638, -0.6799,\n",
      "        -0.6787, -0.6769, -0.6661, -0.6596, -0.6539, -0.6556, -0.6553, -0.6656,\n",
      "        -0.6793, -0.7042, -0.7418, -0.7945, -0.8589, -0.9346, -1.0123, -1.0908,\n",
      "        -1.1647, -1.2411, -1.3176, -1.4149, -1.5416, -1.7095, -1.9003, -2.1114,\n",
      "        -2.3280, -2.5345, -2.7175, -2.8897, -3.0498, -3.1876, -3.2975, -3.3698,\n",
      "        -3.3850, -3.3393, -3.2283, -3.0734, -2.8929, -2.7096, -2.5290, -2.3726,\n",
      "        -2.2205, -2.0771, -1.9308, -1.7917, -1.6456, -1.5045, -1.3665, -1.2382,\n",
      "        -1.1116, -1.0024, -0.9111, -0.8395, -0.7838, -0.7525, -0.7340, -0.7211,\n",
      "        -0.7133, -0.7147, -0.7136, -0.7130, -0.7133, -0.7065, -0.6910, -0.6715,\n",
      "        -0.6441, -0.6176, -0.5947, -0.5745, -0.5677, -0.5734, -0.5914, -0.6191,\n",
      "        -0.6562, -0.6960, -0.7349, -0.7681, -0.7970, -0.8236, -0.8433, -0.8648,\n",
      "        -0.8818, -0.9050, -0.9295, -0.9649, -1.0187, -1.1147, -1.2539, -1.4445,\n",
      "        -1.6810, -1.9478, -2.2324, -2.5187, -2.7749, -2.9926, -3.1669, -3.2939,\n",
      "        -3.3613, -3.3867, -3.3694, -3.3181, -3.2216, -3.1101, -2.9802, -2.8434,\n",
      "        -2.6935, -2.5383, -2.3763, -2.2236, -2.0704, -1.9252, -1.7893, -1.6546,\n",
      "        -1.5215, -1.3909, -1.2638, -1.1432, -1.0276, -0.9230, -0.8348, -0.7657,\n",
      "        -0.7085, -0.6715, -0.6397, -0.6155, -0.5917, -0.5774, -0.5623, -0.5571,\n",
      "        -0.5541, -0.5537, -0.5525, -0.5506, -0.5400, -0.5329, -0.5240, -0.5265,\n",
      "        -0.5309, -0.5408, -0.5493, -0.5667, -0.5783, -0.6042, -0.6412, -0.6991,\n",
      "        -0.7725, -0.8657, -0.9748, -1.1060, -1.2478, -1.4170, -1.6060, -1.8013,\n",
      "        -2.0041, -2.2108, -2.4030, -2.5873, -2.7657, -2.9232, -3.0525, -3.1439,\n",
      "        -3.1812, -3.1558, -3.0807, -2.9688, -2.8356, -2.6961, -2.5628, -2.4344,\n",
      "        -2.3059, -2.1768, -2.0468, -1.8984, -1.7287, -1.5682, -1.4171, -1.2679,\n",
      "        -1.1438, -1.0552, -0.9667, -0.8872, -0.8263, -0.7788, -0.7338, -0.7042,\n",
      "        -0.6748, -0.6494, -0.6202, -0.5968, -0.5709, -0.5468, -0.5222, -0.5032,\n",
      "        -0.4861, -0.4792, -0.4831, -0.4847, -0.4801, -0.4838, -0.4795, -0.4695,\n",
      "        -0.4577, -0.4593, -0.4465, -0.4412, -0.4448, -0.4661, -0.4885, -0.5210,\n",
      "        -0.5578, -0.5983, -0.6380, -0.7073, -0.8097, -0.9396, -1.0979, -1.2939,\n",
      "        -1.5008, -1.7206, -1.9423, -2.1616, -2.3708, -2.5623, -2.7179, -2.8489,\n",
      "        -2.9510, -3.0242, -3.0765, -3.1207, -3.1452, -3.1465, -3.1118, -3.0394,\n",
      "        -2.9214, -2.7769, -2.6068, -2.4276, -2.2343, -2.0452, -1.8542, -1.6732,\n",
      "        -1.4998, -1.3461, -1.2031, -1.0734, -0.9532, -0.8518, -0.7621, -0.6884,\n",
      "        -0.6280, -0.5819, -0.5368, -0.5053, -0.4781, -0.4612, -0.4546, -0.4563,\n",
      "        -0.4518, -0.4577, -0.4681, -0.4731, -0.4739, -0.4824, -0.4864, -0.4836,\n",
      "        -0.4826, -0.4855, -0.4862, -0.4853, -0.4914, -0.5074, -0.5273, -0.5593,\n",
      "        -0.5932, -0.6382, -0.7148, -0.8072, -0.9102, -1.0490, -1.2074, -1.3741,\n",
      "        -1.5799, -1.8253, -2.0765, -2.3253, -2.5468, -2.7286, -2.8550, -2.9493,\n",
      "        -3.0117, -3.0501, -3.0596, -3.0502, -3.0049, -2.9375, -2.8491, -2.7457,\n",
      "        -2.6359, -2.5251, -2.4159, -2.2942, -2.1545, -1.9922, -1.8248, -1.6577,\n",
      "        -1.5015, -1.3710, -1.2643, -1.1738, -1.0765, -0.9886, -0.9018, -0.8206,\n",
      "        -0.7427, -0.6827, -0.6381, -0.6044, -0.5763, -0.5544, -0.5360, -0.5163,\n",
      "        -0.5010, -0.4921, -0.4828, -0.4796, -0.4773, -0.4737, -0.4725, -0.4779,\n",
      "        -0.4796, -0.4833, -0.4889, -0.4964, -0.5063, -0.5192, -0.5337, -0.5501,\n",
      "        -0.5675, -0.5911, -0.6238, -0.6711, -0.7433, -0.8307, -0.9233, -1.0302,\n",
      "        -1.1394, -1.2457, -1.3570, -1.4870, -1.6171, -1.7593, -1.9088, -2.0682,\n",
      "        -2.2142, -2.3499, -2.4622, -2.5452, -2.5990, -2.6452, -2.6904, -2.7559,\n",
      "        -2.8504, -2.9750, -3.1111, -3.2609, -3.4012, -3.5267, -3.6280, -3.7084,\n",
      "        -3.7348, -3.7219, -3.6740, -3.5761, -3.4307, -3.2712, -3.0921, -2.8892,\n",
      "        -2.6808, -2.4697, -2.2520, -2.0380, -1.8317, -1.6474, -1.4899, -1.3601,\n",
      "        -1.2295, -1.1218, -1.0319, -0.9567, -0.8910, -0.8587, -0.8319, -0.8033,\n",
      "        -0.7660, -0.7373, -0.7042, -0.6735, -0.6461, -0.6349, -0.6148, -0.5984,\n",
      "        -0.5808, -0.5644, -0.5438, -0.5314, -0.5170, -0.5029, -0.4875, -0.4744,\n",
      "        -0.4700, -0.4703, -0.4691, -0.4714, -0.4752, -0.4666, -0.4598, -0.4624,\n",
      "        -0.4609, -0.4575, -0.4565, -0.4555, -0.4496, -0.4517, -0.4520, -0.4539,\n",
      "        -0.4585, -0.4663, -0.4735, -0.4913, -0.5101, -0.5289, -0.5503, -0.5745,\n",
      "        -0.5933, -0.6146, -0.6417, -0.7120, -0.8152, -0.9495, -1.1176, -1.3219,\n",
      "        -1.5269, -1.7501, -1.9895, -2.2339, -2.4507, -2.6322, -2.7601, -2.8289,\n",
      "        -2.8360, -2.8096, -2.7489, -2.6599, -2.5532, -2.4386, -2.3101, -2.1800,\n",
      "        -2.0514, -1.9234, -1.7977, -1.6771, -1.5579, -1.4421, -1.3242, -1.2106,\n",
      "        -1.0998, -0.9936, -0.8876, -0.7970, -0.7131, -0.6433, -0.5836, -0.5366,\n",
      "        -0.4972, -0.4721, -0.4523, -0.4402, -0.4308, -0.4326, -0.4298, -0.4297,\n",
      "        -0.4276, -0.4341, -0.4329, -0.4472, -0.4647, -0.4971, -0.5333, -0.5821,\n",
      "        -0.6374, -0.7066, -0.7751, -0.8523, -0.9373, -1.0420, -1.1624, -1.3157,\n",
      "        -1.4915, -1.6990, -1.9085, -2.1276, -2.3406, -2.5464, -2.7233, -2.8816,\n",
      "        -3.0023, -3.0875, -3.1368, -3.1630, -3.1513, -3.1118, -3.0371, -2.9328,\n",
      "        -2.7909, -2.6360, -2.4692, -2.3029, -2.1351, -1.9783, -1.8201, -1.6628,\n",
      "        -1.5014, -1.3578, -1.2180, -1.0926, -0.9863, -0.9090, -0.8404, -0.7861,\n",
      "        -0.7390, -0.6997, -0.6598, -0.6272, -0.5984, -0.5724, -0.5445, -0.5191,\n",
      "        -0.4880, -0.4606, -0.4381, -0.4268, -0.4158, -0.4147, -0.4228, -0.4436,\n",
      "        -0.4735, -0.5244, -0.5852, -0.6547, -0.7317, -0.8032, -0.8702, -0.9543,\n",
      "        -1.0574, -1.1781, -1.3417, -1.5422, -1.7678, -2.0123, -2.2693, -2.5161,\n",
      "        -2.7524, -2.9676, -3.1483, -3.2924, -3.3852, -3.4185, -3.3792, -3.2817,\n",
      "        -3.1328, -2.9521, -2.7454, -2.5399, -2.3371, -2.1475, -1.9793, -1.8398,\n",
      "        -1.7234, -1.6245, -1.5328, -1.4434, -1.3488, -1.2492, -1.1487, -1.0531,\n",
      "        -0.9642, -0.8915, -0.8301, -0.7808, -0.7365, -0.6968, -0.6527, -0.6161,\n",
      "        -0.5792, -0.5525, -0.5289, -0.5200, -0.5027, -0.4896, -0.4742, -0.4635,\n",
      "        -0.4477, -0.4460, -0.4504, -0.4617, -0.4751, -0.4929, -0.5073, -0.5212,\n",
      "        -0.5351, -0.5524, -0.5722, -0.6014, -0.6456, -0.6988, -0.7617, -0.8450,\n",
      "        -0.9553, -1.0998, -1.2831, -1.5168, -1.7941, -2.1110, -2.4486, -2.8074,\n",
      "        -3.1460, -3.4454, -3.6746, -3.8203, -3.8681, -3.8324, -3.7302, -3.5838,\n",
      "        -3.4108, -3.2261, -3.0425, -2.8690, -2.7049, -2.5481, -2.3970, -2.2551,\n",
      "        -2.1055, -1.9532, -1.7976, -1.6423, -1.4824, -1.3360, -1.1965, -1.0738,\n",
      "        -0.9588, -0.8660, -0.7814, -0.7115, -0.6566, -0.6216, -0.5949, -0.5757,\n",
      "        -0.5609, -0.5440, -0.5266, -0.5137, -0.4984, -0.4881, -0.4841, -0.4825,\n",
      "        -0.4820, -0.4949, -0.5113, -0.5411, -0.5865, -0.6468, -0.7258, -0.8322,\n",
      "        -0.9649, -1.1396, -1.3659, -1.6431, -1.9573, -2.2880, -2.5986, -2.8725,\n",
      "        -3.0817, -3.2273, -3.3033, -3.3248, -3.2905, -3.2235, -3.1266, -3.0227,\n",
      "        -2.9121, -2.8111, -2.7043, -2.5995, -2.4813, -2.3539, -2.2103, -2.0718,\n",
      "        -1.9267, -1.7962, -1.6674, -1.5426, -1.4104, -1.2835, -1.1550, -1.0463,\n",
      "        -0.9525, -0.8839, -0.8325, -0.7998, -0.7711, -0.7478, -0.7221, -0.6980,\n",
      "        -0.6712, -0.6601, -0.6577, -0.6756, -0.7150, -0.7785, -0.8431, -0.9244,\n",
      "        -1.0262, -1.1561, -1.3145, -1.5219, -1.7822, -2.0763, -2.3806, -2.6867,\n",
      "        -2.9663, -3.1816, -3.3204, -3.3875, -3.3812, -3.3179, -3.2147, -3.0858,\n",
      "        -2.9392, -2.7912, -2.6484, -2.5212, -2.4108, -2.3228, -2.2409, -2.1528,\n",
      "        -2.0390, -1.9094, -1.7548, -1.5906, -1.4208, -1.2712, -1.1401, -1.0338,\n",
      "        -0.9433, -0.8721, -0.8090, -0.7549, -0.7109, -0.6793, -0.6647, -0.6722,\n",
      "        -0.6969, -0.7316, -0.7818, -0.8461, -0.9158, -1.0143, -1.1508, -1.3142,\n",
      "        -1.5064, -1.7410, -2.0027, -2.2849, -2.5764, -2.8717, -3.1556, -3.4071,\n",
      "        -3.6075, -3.7559, -3.8407, -3.8533, -3.7934, -3.6755, -3.5125, -3.3245,\n",
      "        -3.1273, -2.9421, -2.7701, -2.6051, -2.4408, -2.2784, -2.1063, -1.9308,\n",
      "        -1.7572, -1.5961, -1.4427, -1.3063, -1.1842, -1.0813, -0.9990, -0.9365,\n",
      "        -0.8839, -0.8450, -0.8202, -0.8072, -0.8151, -0.8479, -0.9070, -0.9924,\n",
      "        -1.0971, -1.2188, -1.3647, -1.5358, -1.7322, -1.9559, -2.2003, -2.4591,\n",
      "        -2.7193, -2.9683, -3.1741, -3.3155, -3.3714, -3.3465, -3.2477, -3.0938,\n",
      "        -2.9204, -2.7492, -2.5781, -2.4217, -2.2841, -2.1466, -2.0104, -1.8793,\n",
      "        -1.7383, -1.5977, -1.4585, -1.3294, -1.2158, -1.1181, -1.0417, -1.0054,\n",
      "        -1.0037, -1.0384, -1.1167, -1.2438, -1.4030, -1.5846, -1.7942, -2.0435])\n"
     ]
    }
   ],
   "source": [
    "print(labels.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Summary: 2 Conv&Pool layers, RNN with 2 hidden states of 200 size, 10 sequence length, 5 epochs: 1.042 RMSE. Best performance by a small margin, albeit on training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
