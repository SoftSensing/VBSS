{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "\n",
    "# relative path to npz files\n",
    "path = 'Measurements'\n",
    "file_name = 'output_batch_%d.npz'\n",
    "\n",
    "# Training\n",
    "# how many frames to load\n",
    "frames_num = 10\n",
    "data_frames = []\n",
    "data_forces = []\n",
    "for i in range(frames_num):\n",
    "    file_path = os.path.join(path,file_name %i)\n",
    "    data = np.load(file_path)\n",
    "    data_frames.append(data['frames'])\n",
    "    data_forces.append(data['forces'])\n",
    "\n",
    "combined_frames = np.concatenate(data_frames, axis=0)\n",
    "combined_forces = np.concatenate(data_forces, axis=0)\n",
    "\n",
    "# Test\n",
    "# file_path = os.path.join(path, file_name %11)\n",
    "# test_data = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split. Size of train data is 10/11 and size of test data is 1/11 \n",
    "# features_train, features_test, targets_train, targets_test = train_test_split(combined_frames,\n",
    "#                                                                              combined_forces[:,2],\n",
    "#                                                                              test_size = (1/11),\n",
    "#                                                                              random_state = 42) \n",
    "# train_test_split NOT RECOMMENDED, because it shuffles temporally dependent data\n",
    "features_train = combined_frames\n",
    "targets_train = combined_forces[:,2]\n",
    "# features_test = test_data['frames']\n",
    "# targets_test = test_data['forces']\n",
    "# targets_test = targets_test[:,2]\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train)\n",
    "\n",
    "# create feature and targets tensor for test set. \n",
    "# featuresTest = torch.from_numpy(features_test)\n",
    "# targetsTest = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 9, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(9, 18, kernel_size = 5, stride = 1, padding = 2) # If needed\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(18 * 16 * 30, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ELU()(self.conv2(x))) # If needed\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (-1, 1, 18 * 16 * 30))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        \n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        #print(\"Size of h0:\", h0.size())  # Print size of h0 - Debugging - Obsolete\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1000\n",
    "num_epochs = 5\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "# test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 1  Loss: 1469.69970703125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 2  Loss: 2391.631103515625\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 3  Loss: 1126.598388671875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 4  Loss: 6.230687618255615\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 5  Loss: 814.4215087890625\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 6  Loss: 1036.4716796875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 7  Loss: 344.6268310546875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 8  Loss: 4.114468574523926\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 9  Loss: 311.7105712890625\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 10  Loss: 616.9794921875\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 11  Loss: 377.4425354003906\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 12  Loss: 60.99824905395508\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 13  Loss: 32.94886779785156\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 14  Loss: 245.6324005126953\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 15  Loss: 328.9977111816406\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 16  Loss: 190.01034545898438\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 17  Loss: 23.783039093017578\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 18  Loss: 25.27839469909668\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 19  Loss: 124.582763671875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 20  Loss: 192.76153564453125\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 21  Loss: 102.90518188476562\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 22  Loss: 16.511932373046875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 23  Loss: 11.370388984680176\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 24  Loss: 75.74278259277344\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 25  Loss: 101.09996795654297\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 26  Loss: 61.94324493408203\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 27  Loss: 9.567343711853027\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 28  Loss: 7.584070682525635\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 29  Loss: 34.54928970336914\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 30  Loss: 56.76667785644531\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 31  Loss: 24.18267059326172\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 32  Loss: 3.0860702991485596\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 33  Loss: 8.995644569396973\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 34  Loss: 28.97785186767578\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 35  Loss: 23.709474563598633\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 36  Loss: 7.248345375061035\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 37  Loss: 2.4198052883148193\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 38  Loss: 12.26333999633789\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 39  Loss: 12.835132598876953\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 40  Loss: 10.90761947631836\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 41  Loss: 1.8964227437973022\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 42  Loss: 4.74405574798584\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 43  Loss: 8.126972198486328\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 44  Loss: 7.957091331481934\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 45  Loss: 2.4155335426330566\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 46  Loss: 2.5700786113739014\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 47  Loss: 5.9614787101745605\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 48  Loss: 5.633178234100342\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 49  Loss: 2.093752384185791\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 50  Loss: 1.7911601066589355\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 50 # Consider the whole batch to be temporally correlated\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('~~~BEGINNING OF DATASET~~~')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        # print(images.shape)  # Add this line to check the shape of images - Debugging purposes\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data)\n",
    "        # Print Loss\n",
    "        if count % 1 == 0: # for now print for every iteration\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1469.6997), tensor(2391.6311), tensor(1126.5984), tensor(6.2307), tensor(814.4215), tensor(1036.4717), tensor(344.6268), tensor(4.1145), tensor(311.7106), tensor(616.9795), tensor(377.4425), tensor(60.9982), tensor(32.9489), tensor(245.6324), tensor(328.9977), tensor(190.0103), tensor(23.7830), tensor(25.2784), tensor(124.5828), tensor(192.7615), tensor(102.9052), tensor(16.5119), tensor(11.3704), tensor(75.7428), tensor(101.1000), tensor(61.9432), tensor(9.5673), tensor(7.5841), tensor(34.5493), tensor(56.7667), tensor(24.1827), tensor(3.0861), tensor(8.9956), tensor(28.9779), tensor(23.7095), tensor(7.2483), tensor(2.4198), tensor(12.2633), tensor(12.8351), tensor(10.9076), tensor(1.8964), tensor(4.7441), tensor(8.1270), tensor(7.9571), tensor(2.4155), tensor(2.5701), tensor(5.9615), tensor(5.6332), tensor(2.0938), tensor(1.7912)]\n",
      "2.0782137\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)\n",
    "print(np.sqrt(np.mean(loss_list[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4929, -1.5412, -2.1978, -2.1698, -2.3976, -2.0895, -2.1512, -1.4423,\n",
      "        -2.7281, -2.4608, -2.4110, -3.2214, -1.4060, -3.4376, -4.0289, -4.1386,\n",
      "        -3.3118, -2.6025, -1.8170, -3.1059, -1.8080, -2.4604, -1.3627, -2.2984,\n",
      "        -2.3862, -2.5800, -1.4575, -1.7955, -1.8638, -1.5818, -1.4018, -2.4109,\n",
      "        -1.9005, -2.2340, -3.0158, -1.6643, -2.4143, -2.8597, -2.3950, -1.5725,\n",
      "        -2.0098, -3.7721, -2.6353, -3.3689, -2.3851, -2.9648, -2.1515, -2.7832,\n",
      "        -2.0951, -2.2783, -2.1410, -2.0654, -1.8210, -1.6418, -1.4474, -1.7092,\n",
      "        -1.8115, -2.2257, -1.7392, -1.7605, -2.1168, -2.5319, -2.1835, -3.2506,\n",
      "        -3.2351, -2.6224, -4.0558, -2.8631, -2.3781, -2.2907, -3.4886, -1.3027,\n",
      "        -2.7322, -2.0699, -2.2706, -1.6914, -3.3931, -3.0189, -0.7092, -1.1881,\n",
      "        -0.8689, -1.5976, -1.6396, -3.2310, -1.8672, -3.2908, -2.4769, -2.8107,\n",
      "        -2.7464, -2.7845, -2.9820, -3.9309, -3.0694, -3.1747, -2.9800, -4.4822,\n",
      "        -2.4932, -2.5690, -1.7546, -3.1835, -3.2605, -1.6758, -2.1075, -2.1146,\n",
      "        -2.3422, -1.1878, -2.5648, -2.6402, -2.8167, -1.6552, -2.7298, -2.4771,\n",
      "        -2.8776, -2.2404, -3.5416, -2.7485, -3.4917, -3.6991, -2.0643, -4.2166,\n",
      "        -2.7019, -2.4368, -1.8870, -2.5821, -2.7165, -1.9791, -2.7480, -1.8984,\n",
      "        -1.0802, -1.7426, -2.7969, -1.3230, -3.0098, -1.5351, -1.8410, -2.0875,\n",
      "        -2.5388, -2.5381, -2.4135, -4.3122, -3.0570, -2.0982, -3.5946, -1.7001,\n",
      "        -2.3517, -2.0783, -2.8326, -2.2347, -2.1367, -2.0977, -0.1326, -2.5842,\n",
      "        -2.1223, -3.0212, -1.1681, -1.9230, -2.2543, -2.1655, -2.1406, -2.0611,\n",
      "        -2.4998, -2.2558, -2.2621, -1.6246, -3.2953, -3.0388, -4.3354, -3.3652,\n",
      "        -2.8517, -2.7119, -3.3992, -3.2328, -2.8542, -2.2992, -2.6464, -2.4399,\n",
      "        -1.8495, -1.5534, -2.1499, -1.2126, -2.3674, -2.2785, -2.1198, -2.7825,\n",
      "        -2.4363, -1.2754, -2.9586, -2.2056, -1.5119, -1.5190, -3.9817, -1.9779,\n",
      "        -3.2023, -4.2302, -3.6711, -1.5259, -3.6795, -3.1167, -2.8012, -2.7529,\n",
      "        -2.5577, -1.0368, -2.1683, -2.3976, -1.3671, -2.8359, -2.3621, -2.1971,\n",
      "        -2.3637, -2.2828, -1.5931, -1.3757, -1.6089, -1.5836, -1.2415, -0.2675,\n",
      "        -2.2454, -0.9398, -1.8639, -1.1432, -1.5600, -0.6279, -2.0576, -2.0362,\n",
      "        -1.4758,  0.0112,  0.0781, -1.6766, -2.4371, -1.0392, -1.6999,  0.0732,\n",
      "        -2.2565, -0.8993, -1.9370, -2.4386, -1.3776, -1.2978, -1.4399, -2.2341,\n",
      "        -1.2060, -1.8070, -1.4196, -2.4229, -2.2989, -1.2570, -2.1975, -1.1830,\n",
      "        -1.7778, -1.6244, -1.5131, -1.0789, -2.0345, -1.9120, -0.6259, -0.3272,\n",
      "        -1.1483, -0.0715, -2.0774, -0.9540, -0.7281, -1.0186, -1.2686, -0.5414,\n",
      "        -1.1446, -0.9380, -1.0666, -1.9016, -1.5028, -0.5920, -0.7417, -1.8446,\n",
      "        -1.1374, -0.4298, -0.7804, -1.0449, -1.5934, -0.8825, -1.3502, -1.2521,\n",
      "        -0.8938, -0.1126, -1.5431, -0.2176, -0.7255, -1.5762, -1.2915, -2.2900,\n",
      "        -1.7523, -1.3788, -1.1794, -0.3168, -2.8052, -1.2816, -0.8733, -1.6679,\n",
      "        -0.5945, -1.7310, -2.3635, -0.9892, -1.6294, -2.5514, -1.0427, -1.0578,\n",
      "        -1.6120, -1.5789, -0.8862, -1.8386, -1.3185, -3.3828, -1.0786, -2.3502,\n",
      "        -0.2672, -1.9135, -2.0626, -2.2417, -0.9005, -2.5623, -0.1203, -0.9489,\n",
      "        -0.4777, -1.0663, -1.1862, -2.3429, -1.3979, -2.5286, -2.4555, -2.2798,\n",
      "        -1.9254, -2.7659, -2.0234, -1.8395, -1.7171, -2.0157, -0.4178, -2.3653,\n",
      "        -2.1935, -1.5322, -0.9692, -0.5440, -1.6357, -2.2187, -1.7788, -2.5883,\n",
      "        -0.7326, -1.5988, -1.2634, -1.5140, -1.1311, -1.0031, -1.8139, -1.1847,\n",
      "        -2.7972, -1.0045, -1.8121, -1.0662, -1.9352, -1.3149, -1.1814, -1.7656,\n",
      "        -1.0963, -1.3983, -0.6200, -1.1512, -0.8419, -1.0908, -0.7024, -2.2374,\n",
      "        -1.0297, -0.9591, -0.9856, -1.3328, -1.6458, -2.7865, -1.1738, -2.1637,\n",
      "        -1.5692, -0.7277, -1.1391, -1.5471, -1.0690, -2.2474, -1.0478, -1.7269,\n",
      "        -1.3017, -1.7422, -2.2432, -1.2581, -1.2651, -1.1957, -0.2721, -0.8650,\n",
      "        -1.8635, -1.4871, -1.4067, -1.1769, -2.4921, -2.0298, -1.3848, -1.6028,\n",
      "        -1.5168, -0.5901, -1.8178, -1.4935, -1.4165, -0.8762, -0.5100, -1.7085,\n",
      "        -2.1369, -0.9970, -1.1722, -0.7589,  0.2499, -1.2292, -1.4276, -1.1948,\n",
      "        -2.2044, -2.1669, -1.2530, -0.5956, -0.6158, -2.2851, -0.7072, -0.6830,\n",
      "        -1.5046, -2.0562, -1.7523, -1.4341, -0.6541, -1.1613, -1.4980, -1.2838,\n",
      "        -1.8078, -1.6659, -2.6987, -1.1983, -0.1120, -0.2965, -1.1552, -1.6774,\n",
      "        -3.1658, -0.9965, -1.7070, -1.6974, -2.0884, -1.4753, -2.5801, -1.8194,\n",
      "        -2.2996, -2.7711, -1.6742, -3.1252, -3.5565, -0.9102, -4.3065, -3.2740,\n",
      "        -2.2731, -3.8223, -3.5306, -2.0763, -2.8824, -1.5965, -2.6585, -2.2346,\n",
      "        -3.9495, -2.4626, -3.1435, -3.9270, -3.7355, -2.9133, -4.0706, -2.2655,\n",
      "        -2.7460, -2.9707, -3.3033, -2.4586, -1.7351, -2.5317, -1.0265, -2.3920,\n",
      "        -2.0759, -1.0181, -1.7915, -1.1317, -1.3197, -1.2822, -1.9234, -2.8335,\n",
      "        -1.4015, -2.2384, -1.1373, -0.9327, -0.1935, -1.7145, -0.8916, -1.1783,\n",
      "        -0.8325, -0.6716, -2.3949, -0.6900, -1.9204, -1.3327, -0.6074, -1.2873,\n",
      "        -1.0892, -2.7138, -0.4735, -0.5524, -1.8365, -2.4146, -0.7980, -1.5338,\n",
      "        -1.7391, -2.3089, -0.8761, -2.4603, -1.9744, -1.0087, -0.9603, -1.2363,\n",
      "        -1.4097, -1.4188, -0.4202, -1.0778, -1.1355, -1.3884, -1.9717, -2.1672,\n",
      "        -1.4294, -1.6853, -1.1374, -2.1531, -1.7848, -0.7743, -1.1246, -0.6067,\n",
      "        -0.7699, -2.5034, -0.4993, -1.4103, -2.2199, -1.6227, -0.4891, -1.5919,\n",
      "        -0.3778, -1.7844, -0.2926, -0.1762, -1.6882, -1.1246, -1.1524, -1.5077,\n",
      "        -1.4246, -0.6276, -1.6276, -0.4485, -1.4758, -0.3052, -1.5895, -2.4393,\n",
      "        -1.6262, -1.2726, -0.7913, -1.1181, -0.9471, -0.9726, -0.4575, -0.4966,\n",
      "        -0.8469, -0.8940, -0.4482, -1.9594, -1.3108, -1.8759, -1.3982, -1.9465,\n",
      "        -2.3493, -2.0540, -2.6087, -2.3331, -2.0706, -3.2003, -2.3728, -2.5463,\n",
      "        -1.4813, -2.7374, -3.1087, -2.4947, -3.2320, -3.1101, -3.3066, -4.0974,\n",
      "        -3.0197, -3.9418, -3.5947, -2.8960, -2.0945, -3.4689, -2.7331, -2.6697,\n",
      "        -2.8510, -1.5343, -1.8607, -1.6228, -1.2820, -2.1236, -1.6004, -0.6055,\n",
      "        -1.4774, -0.1753, -0.3365,  1.4315,  0.6465,  0.2020,  0.6011,  0.4422,\n",
      "         0.1945,  0.0054, -0.2497, -0.0984, -1.0003, -1.4412, -1.1850, -0.7156,\n",
      "        -2.3376, -2.4761, -1.7192, -1.6132, -2.1355, -2.1885, -1.6304, -0.6787,\n",
      "        -1.2218, -1.8271, -3.2770, -2.3513, -2.4820, -2.2577, -2.3920, -1.6827,\n",
      "        -1.3372, -1.8839, -1.1148, -2.2501, -1.2209, -1.7307, -0.6714, -1.3415,\n",
      "        -1.8740, -1.4459, -0.7963, -1.3489, -2.3758, -1.5495, -1.9330, -1.6785,\n",
      "        -1.3760, -0.8130, -1.8413, -1.8480, -1.4162, -2.5494, -1.0610, -0.9007,\n",
      "        -0.6770, -2.2311, -1.0385, -1.9996, -0.7666, -1.3115, -0.2681, -1.7472,\n",
      "        -1.3504, -1.2386, -1.5251, -1.3883, -1.3731, -1.7066, -1.6100, -1.9484,\n",
      "        -1.7311, -1.2276, -1.8404, -1.2601, -2.1116, -1.2840, -2.0719, -1.7448,\n",
      "        -0.8819, -0.6903, -0.2485, -0.9818, -2.5912, -2.3129, -1.4079, -1.7404,\n",
      "        -2.0167, -1.1572, -1.6549, -1.6933, -0.9744, -1.2807, -2.2485, -2.5658,\n",
      "        -2.5232, -1.2592, -1.5765, -2.2033, -0.7266, -1.8537, -1.0702, -0.5797,\n",
      "        -1.4997, -1.4832, -1.6890, -1.6967, -2.6232, -2.1017, -1.9107, -1.7161,\n",
      "        -2.7472, -1.5664, -1.8876, -2.2712, -1.2181, -2.5210, -2.6221, -2.9637,\n",
      "        -2.8173, -3.0157, -2.7397, -2.9789, -2.1180, -1.8540, -2.5591, -2.0210,\n",
      "        -1.6668, -0.4447, -1.5374,  0.1441, -1.3867, -0.3777, -1.0054, -0.8241,\n",
      "        -0.3195, -1.0541, -1.3293, -0.6868, -1.2354, -0.0483,  0.5277,  0.2216,\n",
      "        -0.1683, -0.2198,  0.1485, -0.5034, -0.1098, -1.0100,  0.4305,  0.1120,\n",
      "        -0.6836, -0.9409, -1.1710, -0.6972, -1.5994, -1.4046,  0.2105, -1.3108,\n",
      "        -0.6013, -0.6275, -1.7923, -1.2712, -1.4207, -2.1725, -1.8805, -0.7085,\n",
      "        -1.2921, -1.7819, -1.2268, -1.5273, -2.0632, -1.8364, -2.2226, -1.8781,\n",
      "        -3.2712, -1.3565, -1.4588, -0.6404, -1.2656, -0.1344, -1.2846, -1.1854,\n",
      "        -1.0318, -0.7313, -0.8076, -1.8031, -2.0710, -0.8396, -1.8089, -1.3389,\n",
      "        -1.3469, -1.6753, -2.1348, -1.5724, -1.5817, -1.2004, -2.0485, -1.8509,\n",
      "        -1.0646, -2.1392, -1.6887, -0.8157, -0.9813, -1.2881, -1.3595, -1.1257,\n",
      "        -0.9646, -0.4990, -2.1463, -1.0053, -0.8914, -1.0509, -2.4523, -1.1031,\n",
      "        -0.7806, -1.0483, -0.8298, -0.3286, -1.7653, -1.1816, -1.9063, -1.5996,\n",
      "        -1.1258, -0.8706, -0.8102, -1.4763,  0.1432, -1.0264, -0.7358, -1.5092,\n",
      "        -1.1572, -1.4305, -1.8787, -2.1145, -0.4605, -1.4647, -2.6661, -0.6737,\n",
      "        -0.3694, -1.8341, -1.1597, -1.2810, -2.7460, -1.4802, -0.8029, -0.9221,\n",
      "        -1.5762, -1.7954, -0.8643, -0.9173, -1.0157, -0.1394, -0.7806, -1.1631,\n",
      "        -1.4640, -0.9067, -1.5712,  0.0555, -1.8542, -2.3525, -1.1392, -2.0416,\n",
      "        -2.3698, -1.8771, -1.2846, -1.5421, -2.2552, -2.0568, -3.0379, -3.2551,\n",
      "        -2.2424, -2.5601, -2.1534, -1.2741, -0.8760, -1.4781, -0.8764, -0.1136,\n",
      "        -0.9804, -1.1399, -0.0123, -0.8639, -0.5000,  0.6955,  0.9166,  0.3283,\n",
      "        -0.5669,  0.6609, -0.6343, -0.3685,  0.2981,  0.1608, -0.1811,  0.2938,\n",
      "         0.6942,  0.5116, -0.6071,  0.0362, -0.3808, -0.1786, -0.2724,  0.1032,\n",
      "        -1.0503, -2.3532, -2.7434, -1.6459, -1.0222, -0.9458, -0.4568, -0.7183,\n",
      "        -2.1700, -1.6978, -1.1888, -2.4949, -2.6198, -2.2335, -2.9357, -1.0925,\n",
      "        -1.7835, -1.6911, -1.4609, -0.8948, -1.0915, -0.6872, -3.1422, -1.0859,\n",
      "        -1.2221, -1.6029, -1.9497, -0.7706, -1.1778, -2.1951, -1.6313, -1.0564,\n",
      "        -1.6708, -2.2267, -1.4692, -1.0583, -0.1202, -0.9184, -0.6978, -0.8359,\n",
      "        -1.7279, -0.5523, -2.9966, -0.9878, -1.7334, -1.9357, -1.6535, -0.2906,\n",
      "        -1.5271, -0.4238, -1.6263, -0.3204, -1.8040, -1.5054, -2.2663, -1.1221,\n",
      "        -1.7828, -0.4523, -1.2692, -2.1410, -1.3550, -0.2343, -1.3553, -1.1179,\n",
      "        -0.7679, -1.2170, -2.1102, -1.0533, -1.5093, -1.7667, -2.4924, -2.8201,\n",
      "        -1.6002, -0.3243, -2.4440, -0.9623, -1.9312, -1.5830, -2.2182, -1.4005,\n",
      "        -1.9387, -0.8053, -1.8525, -1.5824, -1.6105, -0.7055, -0.4144,  0.3363],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) # Small sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0797, -0.9949, -0.9238, -0.8581, -0.7988, -0.7402, -0.6864, -0.6413,\n",
      "        -0.6007, -0.5637, -0.5303, -0.5034, -0.4781, -0.4516, -0.4438, -0.4478,\n",
      "        -0.4525, -0.4622, -0.4775, -0.4831, -0.4824, -0.4824, -0.4840, -0.4829,\n",
      "        -0.4796, -0.4784, -0.4745, -0.4723, -0.4696, -0.4686, -0.4738, -0.4907,\n",
      "        -0.5179, -0.5654, -0.6264, -0.6943, -0.7702, -0.8488, -0.9312, -1.0187,\n",
      "        -1.1152, -1.2284, -1.3486, -1.4846, -1.6458, -1.8335, -2.0371, -2.2583,\n",
      "        -2.4803, -2.6908, -2.8747, -3.0401, -3.1835, -3.2991, -3.3773, -3.4140,\n",
      "        -3.4004, -3.3335, -3.2181, -3.0745, -2.9110, -2.7374, -2.5585, -2.3805,\n",
      "        -2.1968, -2.0178, -1.8431, -1.6839, -1.5378, -1.3989, -1.2665, -1.1307,\n",
      "        -0.9906, -0.8688, -0.7793, -0.7269, -0.7026, -0.7065, -0.7217, -0.7390,\n",
      "        -0.7422, -0.7402, -0.7296, -0.7031, -0.6639, -0.6237, -0.5844, -0.5494,\n",
      "        -0.5165, -0.4928, -0.4744, -0.4658, -0.4598, -0.4682, -0.4804, -0.4992,\n",
      "        -0.5257, -0.5651, -0.6205, -0.6941, -0.7820, -0.8762, -0.9792, -1.0830,\n",
      "        -1.1860, -1.2925, -1.3998, -1.5069, -1.6152, -1.7351, -1.8683, -2.0259,\n",
      "        -2.2131, -2.4256, -2.6556, -2.8895, -3.1145, -3.3130, -3.4665, -3.5678,\n",
      "        -3.6013, -3.5683, -3.4693, -3.3291, -3.1547, -2.9744, -2.7906, -2.6148,\n",
      "        -2.4369, -2.2563, -2.0721, -1.8849, -1.7012, -1.5338, -1.3869, -1.2611,\n",
      "        -1.1644, -1.0881, -1.0243, -0.9720, -0.9171, -0.8640, -0.8028, -0.7433,\n",
      "        -0.6825, -0.6341, -0.6034, -0.6033, -0.6143, -0.6381, -0.6638, -0.6799,\n",
      "        -0.6787, -0.6769, -0.6661, -0.6596, -0.6539, -0.6556, -0.6553, -0.6656,\n",
      "        -0.6793, -0.7042, -0.7418, -0.7945, -0.8589, -0.9346, -1.0123, -1.0908,\n",
      "        -1.1647, -1.2411, -1.3176, -1.4149, -1.5416, -1.7095, -1.9003, -2.1114,\n",
      "        -2.3280, -2.5345, -2.7175, -2.8897, -3.0498, -3.1876, -3.2975, -3.3698,\n",
      "        -3.3850, -3.3393, -3.2283, -3.0734, -2.8929, -2.7096, -2.5290, -2.3726,\n",
      "        -2.2205, -2.0771, -1.9308, -1.7917, -1.6456, -1.5045, -1.3665, -1.2382,\n",
      "        -1.1116, -1.0024, -0.9111, -0.8395, -0.7838, -0.7525, -0.7340, -0.7211,\n",
      "        -0.7133, -0.7147, -0.7136, -0.7130, -0.7133, -0.7065, -0.6910, -0.6715,\n",
      "        -0.6441, -0.6176, -0.5947, -0.5745, -0.5677, -0.5734, -0.5914, -0.6191,\n",
      "        -0.6562, -0.6960, -0.7349, -0.7681, -0.7970, -0.8236, -0.8433, -0.8648,\n",
      "        -0.8818, -0.9050, -0.9295, -0.9649, -1.0187, -1.1147, -1.2539, -1.4445,\n",
      "        -1.6810, -1.9478, -2.2324, -2.5187, -2.7749, -2.9926, -3.1669, -3.2939,\n",
      "        -3.3613, -3.3867, -3.3694, -3.3181, -3.2216, -3.1101, -2.9802, -2.8434,\n",
      "        -2.6935, -2.5383, -2.3763, -2.2236, -2.0704, -1.9252, -1.7893, -1.6546,\n",
      "        -1.5215, -1.3909, -1.2638, -1.1432, -1.0276, -0.9230, -0.8348, -0.7657,\n",
      "        -0.7085, -0.6715, -0.6397, -0.6155, -0.5917, -0.5774, -0.5623, -0.5571,\n",
      "        -0.5541, -0.5537, -0.5525, -0.5506, -0.5400, -0.5329, -0.5240, -0.5265,\n",
      "        -0.5309, -0.5408, -0.5493, -0.5667, -0.5783, -0.6042, -0.6412, -0.6991,\n",
      "        -0.7725, -0.8657, -0.9748, -1.1060, -1.2478, -1.4170, -1.6060, -1.8013,\n",
      "        -2.0041, -2.2108, -2.4030, -2.5873, -2.7657, -2.9232, -3.0525, -3.1439,\n",
      "        -3.1812, -3.1558, -3.0807, -2.9688, -2.8356, -2.6961, -2.5628, -2.4344,\n",
      "        -2.3059, -2.1768, -2.0468, -1.8984, -1.7287, -1.5682, -1.4171, -1.2679,\n",
      "        -1.1438, -1.0552, -0.9667, -0.8872, -0.8263, -0.7788, -0.7338, -0.7042,\n",
      "        -0.6748, -0.6494, -0.6202, -0.5968, -0.5709, -0.5468, -0.5222, -0.5032,\n",
      "        -0.4861, -0.4792, -0.4831, -0.4847, -0.4801, -0.4838, -0.4795, -0.4695,\n",
      "        -0.4577, -0.4593, -0.4465, -0.4412, -0.4448, -0.4661, -0.4885, -0.5210,\n",
      "        -0.5578, -0.5983, -0.6380, -0.7073, -0.8097, -0.9396, -1.0979, -1.2939,\n",
      "        -1.5008, -1.7206, -1.9423, -2.1616, -2.3708, -2.5623, -2.7179, -2.8489,\n",
      "        -2.9510, -3.0242, -3.0765, -3.1207, -3.1452, -3.1465, -3.1118, -3.0394,\n",
      "        -2.9214, -2.7769, -2.6068, -2.4276, -2.2343, -2.0452, -1.8542, -1.6732,\n",
      "        -1.4998, -1.3461, -1.2031, -1.0734, -0.9532, -0.8518, -0.7621, -0.6884,\n",
      "        -0.6280, -0.5819, -0.5368, -0.5053, -0.4781, -0.4612, -0.4546, -0.4563,\n",
      "        -0.4518, -0.4577, -0.4681, -0.4731, -0.4739, -0.4824, -0.4864, -0.4836,\n",
      "        -0.4826, -0.4855, -0.4862, -0.4853, -0.4914, -0.5074, -0.5273, -0.5593,\n",
      "        -0.5932, -0.6382, -0.7148, -0.8072, -0.9102, -1.0490, -1.2074, -1.3741,\n",
      "        -1.5799, -1.8253, -2.0765, -2.3253, -2.5468, -2.7286, -2.8550, -2.9493,\n",
      "        -3.0117, -3.0501, -3.0596, -3.0502, -3.0049, -2.9375, -2.8491, -2.7457,\n",
      "        -2.6359, -2.5251, -2.4159, -2.2942, -2.1545, -1.9922, -1.8248, -1.6577,\n",
      "        -1.5015, -1.3710, -1.2643, -1.1738, -1.0765, -0.9886, -0.9018, -0.8206,\n",
      "        -0.7427, -0.6827, -0.6381, -0.6044, -0.5763, -0.5544, -0.5360, -0.5163,\n",
      "        -0.5010, -0.4921, -0.4828, -0.4796, -0.4773, -0.4737, -0.4725, -0.4779,\n",
      "        -0.4796, -0.4833, -0.4889, -0.4964, -0.5063, -0.5192, -0.5337, -0.5501,\n",
      "        -0.5675, -0.5911, -0.6238, -0.6711, -0.7433, -0.8307, -0.9233, -1.0302,\n",
      "        -1.1394, -1.2457, -1.3570, -1.4870, -1.6171, -1.7593, -1.9088, -2.0682,\n",
      "        -2.2142, -2.3499, -2.4622, -2.5452, -2.5990, -2.6452, -2.6904, -2.7559,\n",
      "        -2.8504, -2.9750, -3.1111, -3.2609, -3.4012, -3.5267, -3.6280, -3.7084,\n",
      "        -3.7348, -3.7219, -3.6740, -3.5761, -3.4307, -3.2712, -3.0921, -2.8892,\n",
      "        -2.6808, -2.4697, -2.2520, -2.0380, -1.8317, -1.6474, -1.4899, -1.3601,\n",
      "        -1.2295, -1.1218, -1.0319, -0.9567, -0.8910, -0.8587, -0.8319, -0.8033,\n",
      "        -0.7660, -0.7373, -0.7042, -0.6735, -0.6461, -0.6349, -0.6148, -0.5984,\n",
      "        -0.5808, -0.5644, -0.5438, -0.5314, -0.5170, -0.5029, -0.4875, -0.4744,\n",
      "        -0.4700, -0.4703, -0.4691, -0.4714, -0.4752, -0.4666, -0.4598, -0.4624,\n",
      "        -0.4609, -0.4575, -0.4565, -0.4555, -0.4496, -0.4517, -0.4520, -0.4539,\n",
      "        -0.4585, -0.4663, -0.4735, -0.4913, -0.5101, -0.5289, -0.5503, -0.5745,\n",
      "        -0.5933, -0.6146, -0.6417, -0.7120, -0.8152, -0.9495, -1.1176, -1.3219,\n",
      "        -1.5269, -1.7501, -1.9895, -2.2339, -2.4507, -2.6322, -2.7601, -2.8289,\n",
      "        -2.8360, -2.8096, -2.7489, -2.6599, -2.5532, -2.4386, -2.3101, -2.1800,\n",
      "        -2.0514, -1.9234, -1.7977, -1.6771, -1.5579, -1.4421, -1.3242, -1.2106,\n",
      "        -1.0998, -0.9936, -0.8876, -0.7970, -0.7131, -0.6433, -0.5836, -0.5366,\n",
      "        -0.4972, -0.4721, -0.4523, -0.4402, -0.4308, -0.4326, -0.4298, -0.4297,\n",
      "        -0.4276, -0.4341, -0.4329, -0.4472, -0.4647, -0.4971, -0.5333, -0.5821,\n",
      "        -0.6374, -0.7066, -0.7751, -0.8523, -0.9373, -1.0420, -1.1624, -1.3157,\n",
      "        -1.4915, -1.6990, -1.9085, -2.1276, -2.3406, -2.5464, -2.7233, -2.8816,\n",
      "        -3.0023, -3.0875, -3.1368, -3.1630, -3.1513, -3.1118, -3.0371, -2.9328,\n",
      "        -2.7909, -2.6360, -2.4692, -2.3029, -2.1351, -1.9783, -1.8201, -1.6628,\n",
      "        -1.5014, -1.3578, -1.2180, -1.0926, -0.9863, -0.9090, -0.8404, -0.7861,\n",
      "        -0.7390, -0.6997, -0.6598, -0.6272, -0.5984, -0.5724, -0.5445, -0.5191,\n",
      "        -0.4880, -0.4606, -0.4381, -0.4268, -0.4158, -0.4147, -0.4228, -0.4436,\n",
      "        -0.4735, -0.5244, -0.5852, -0.6547, -0.7317, -0.8032, -0.8702, -0.9543,\n",
      "        -1.0574, -1.1781, -1.3417, -1.5422, -1.7678, -2.0123, -2.2693, -2.5161,\n",
      "        -2.7524, -2.9676, -3.1483, -3.2924, -3.3852, -3.4185, -3.3792, -3.2817,\n",
      "        -3.1328, -2.9521, -2.7454, -2.5399, -2.3371, -2.1475, -1.9793, -1.8398,\n",
      "        -1.7234, -1.6245, -1.5328, -1.4434, -1.3488, -1.2492, -1.1487, -1.0531,\n",
      "        -0.9642, -0.8915, -0.8301, -0.7808, -0.7365, -0.6968, -0.6527, -0.6161,\n",
      "        -0.5792, -0.5525, -0.5289, -0.5200, -0.5027, -0.4896, -0.4742, -0.4635,\n",
      "        -0.4477, -0.4460, -0.4504, -0.4617, -0.4751, -0.4929, -0.5073, -0.5212,\n",
      "        -0.5351, -0.5524, -0.5722, -0.6014, -0.6456, -0.6988, -0.7617, -0.8450,\n",
      "        -0.9553, -1.0998, -1.2831, -1.5168, -1.7941, -2.1110, -2.4486, -2.8074,\n",
      "        -3.1460, -3.4454, -3.6746, -3.8203, -3.8681, -3.8324, -3.7302, -3.5838,\n",
      "        -3.4108, -3.2261, -3.0425, -2.8690, -2.7049, -2.5481, -2.3970, -2.2551,\n",
      "        -2.1055, -1.9532, -1.7976, -1.6423, -1.4824, -1.3360, -1.1965, -1.0738,\n",
      "        -0.9588, -0.8660, -0.7814, -0.7115, -0.6566, -0.6216, -0.5949, -0.5757,\n",
      "        -0.5609, -0.5440, -0.5266, -0.5137, -0.4984, -0.4881, -0.4841, -0.4825,\n",
      "        -0.4820, -0.4949, -0.5113, -0.5411, -0.5865, -0.6468, -0.7258, -0.8322,\n",
      "        -0.9649, -1.1396, -1.3659, -1.6431, -1.9573, -2.2880, -2.5986, -2.8725,\n",
      "        -3.0817, -3.2273, -3.3033, -3.3248, -3.2905, -3.2235, -3.1266, -3.0227,\n",
      "        -2.9121, -2.8111, -2.7043, -2.5995, -2.4813, -2.3539, -2.2103, -2.0718,\n",
      "        -1.9267, -1.7962, -1.6674, -1.5426, -1.4104, -1.2835, -1.1550, -1.0463,\n",
      "        -0.9525, -0.8839, -0.8325, -0.7998, -0.7711, -0.7478, -0.7221, -0.6980,\n",
      "        -0.6712, -0.6601, -0.6577, -0.6756, -0.7150, -0.7785, -0.8431, -0.9244,\n",
      "        -1.0262, -1.1561, -1.3145, -1.5219, -1.7822, -2.0763, -2.3806, -2.6867,\n",
      "        -2.9663, -3.1816, -3.3204, -3.3875, -3.3812, -3.3179, -3.2147, -3.0858,\n",
      "        -2.9392, -2.7912, -2.6484, -2.5212, -2.4108, -2.3228, -2.2409, -2.1528,\n",
      "        -2.0390, -1.9094, -1.7548, -1.5906, -1.4208, -1.2712, -1.1401, -1.0338,\n",
      "        -0.9433, -0.8721, -0.8090, -0.7549, -0.7109, -0.6793, -0.6647, -0.6722,\n",
      "        -0.6969, -0.7316, -0.7818, -0.8461, -0.9158, -1.0143, -1.1508, -1.3142,\n",
      "        -1.5064, -1.7410, -2.0027, -2.2849, -2.5764, -2.8717, -3.1556, -3.4071,\n",
      "        -3.6075, -3.7559, -3.8407, -3.8533, -3.7934, -3.6755, -3.5125, -3.3245,\n",
      "        -3.1273, -2.9421, -2.7701, -2.6051, -2.4408, -2.2784, -2.1063, -1.9308,\n",
      "        -1.7572, -1.5961, -1.4427, -1.3063, -1.1842, -1.0813, -0.9990, -0.9365,\n",
      "        -0.8839, -0.8450, -0.8202, -0.8072, -0.8151, -0.8479, -0.9070, -0.9924,\n",
      "        -1.0971, -1.2188, -1.3647, -1.5358, -1.7322, -1.9559, -2.2003, -2.4591,\n",
      "        -2.7193, -2.9683, -3.1741, -3.3155, -3.3714, -3.3465, -3.2477, -3.0938,\n",
      "        -2.9204, -2.7492, -2.5781, -2.4217, -2.2841, -2.1466, -2.0104, -1.8793,\n",
      "        -1.7383, -1.5977, -1.4585, -1.3294, -1.2158, -1.1181, -1.0417, -1.0054,\n",
      "        -1.0037, -1.0384, -1.1167, -1.2438, -1.4030, -1.5846, -1.7942, -2.0435])\n"
     ]
    }
   ],
   "source": [
    "print(labels.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Summary: Semi-automated data loading procedure and additional convolutional-pooling layer to the CNN, reducing the output nodes that are fed to RNN by a factor of 10. Learning rate reduced tenfold (1e-4). Run over 5 epochs shows significant improvement. Root of average MSE in last epoch is 2.07, at that instance it seems it could be further lowered by learning.\n",
    "Note: Assuming values are equally distributed between -3 and 0, a random force predictor would have the metric above equal to 1.5 . Below 1.5 is needed in a test setting means so that the force predictor is relevant. Still, significant improvement must be noted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
