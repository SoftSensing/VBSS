{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "filename_0 = 'output_batch_0.npz'\n",
    "filename_1 = 'output_batch_1.npz'\n",
    "filename_2 = 'output_batch_2.npz'\n",
    "filename_3 = 'output_batch_3.npz'\n",
    "filename_4 = 'output_batch_4.npz'\n",
    "filename_5 = 'output_batch_5.npz'\n",
    "filename_6 = 'output_batch_6.npz'\n",
    "filename_7 = 'output_batch_7.npz'\n",
    "filename_8 = 'output_batch_8.npz'\n",
    "filename_9 = 'output_batch_9.npz'\n",
    "filename_10 = 'output_batch_10.npz'\n",
    "filename_11 = 'output_batch_11.npz'\n",
    "folder_path = 'Measurements'\n",
    "current_directory = 'c:/Users/20201558/Desktop/Work related stuff/Master/Year 4/Quartile 3/Interdisciplinary Team Project [5ARIP10]/Code/VBSS'\n",
    "file_path_0 = os.path.join(current_directory,folder_path, filename_0)\n",
    "file_path_1 = os.path.join(current_directory,folder_path, filename_1)\n",
    "file_path_2 = os.path.join(current_directory,folder_path, filename_2)\n",
    "file_path_3 = os.path.join(current_directory,folder_path, filename_3)\n",
    "file_path_4 = os.path.join(current_directory,folder_path, filename_4)\n",
    "file_path_5 = os.path.join(current_directory,folder_path, filename_5)\n",
    "file_path_6 = os.path.join(current_directory,folder_path, filename_6)\n",
    "file_path_7 = os.path.join(current_directory,folder_path, filename_7)\n",
    "file_path_8 = os.path.join(current_directory,folder_path, filename_8)\n",
    "file_path_9 = os.path.join(current_directory,folder_path, filename_9)\n",
    "file_path_10 = os.path.join(current_directory,folder_path, filename_10)\n",
    "file_path_11 = os.path.join(current_directory,folder_path, filename_11)\n",
    "data_0 = np.load(file_path_0)\n",
    "data_1 = np.load(file_path_1)\n",
    "data_2 = np.load(file_path_2)\n",
    "data_3 = np.load(file_path_3)\n",
    "data_4 = np.load(file_path_4)\n",
    "data_5 = np.load(file_path_5)\n",
    "data_6 = np.load(file_path_6)\n",
    "data_7 = np.load(file_path_7)\n",
    "data_8 = np.load(file_path_8)\n",
    "data_9 = np.load(file_path_9)\n",
    "data_10 = np.load(file_path_10)\n",
    "#data_11 = np.load(file_path_11)\n",
    "data_frames = [data_0['frames'], data_1['frames'], data_2['frames'], data_3['frames'], data_4['frames'],\n",
    "               data_5['frames'], data_6['frames'], data_7['frames'], data_8['frames'], data_9['frames'], data_10['frames']]\n",
    "\n",
    "data_forces = [data_0['forces'], data_1['forces'], data_2['forces'], data_3['forces'], data_4['forces'],\n",
    "               data_5['forces'], data_6['forces'], data_7['forces'], data_8['forces'], data_9['forces'], data_10['forces']]\n",
    "\n",
    "combined_frames = np.concatenate(data_frames, axis=0)\n",
    "combined_forces = np.concatenate(data_forces, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "# features_train, features_test, targets_train, targets_test = train_test_split(combined_frames,\n",
    "#                                                                              combined_forces[:,2],\n",
    "#                                                                              test_size = (1/11),\n",
    "#                                                                              random_state = 42) \n",
    "features_train = combined_frames\n",
    "targets_train = combined_forces[:,2]\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set. \n",
    "# featuresTest = torch.from_numpy(features_test)\n",
    "# targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 12, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "        #self.conv2 = nn.Conv2d(12, 36, kernel_size=5, stride=1, padding=2) # If needed\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(12 * 67 * 120, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        #x = self.pool(nn.ELU()(self.conv2(x))) # If needed\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (-1, 1, 12 * 67 * 120))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        \n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        #print(\"Size of h0:\", h0.size())  # Print size of h0 - Debugging - Obsolete\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1000\n",
    "num_epochs = 2\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "# test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 1  Loss: 421208.8125\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 2  Loss: 49158796.0\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 3  Loss: 6980169.0\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 4  Loss: 201368.734375\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 5  Loss: 482554.9375\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 6  Loss: 231398.203125\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 7  Loss: 52016.48046875\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 8  Loss: 90032.65625\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 9  Loss: 25411.142578125\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 10  Loss: 30439.662109375\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 11  Loss: 15626.3466796875\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 12  Loss: 1724.7554931640625\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 13  Loss: 2254.01171875\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 14  Loss: 10604.4189453125\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 15  Loss: 12004.666015625\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 16  Loss: 6137.05517578125\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 17  Loss: 847.7996826171875\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 18  Loss: 510.7109680175781\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 19  Loss: 3534.572509765625\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 20  Loss: 5471.47998046875\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 21  Loss: 4011.550537109375\n",
      "Size of x: torch.Size([1000, 1, 96480])\n",
      "Iteration: 22  Loss: 2414.406982421875\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 50 # Consider the whole batch to be temporally correlated\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('~~~BEGINNING OF DATASET~~~')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        # print(images.shape)  # Add this line to check the shape of images - Debugging purposes\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data)\n",
    "        # Print Loss\n",
    "        if count % 1 == 0: # for now print for every iteration\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(421208.8125), tensor(49158796.), tensor(6980169.), tensor(201368.7344), tensor(482554.9375), tensor(231398.2031), tensor(52016.4805), tensor(90032.6562), tensor(25411.1426), tensor(30439.6621), tensor(15626.3467), tensor(1724.7555), tensor(2254.0117), tensor(10604.4189), tensor(12004.6660), tensor(6137.0552), tensor(847.7997), tensor(510.7110), tensor(3534.5725), tensor(5471.4800), tensor(4011.5505), tensor(2414.4070)]\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-48.2465, -51.8231, -49.5471, -52.0802, -48.4703, -49.5593, -49.8716,\n",
      "        -46.6503, -43.4998, -45.6609, -48.6962, -49.9108, -48.3651, -51.0996,\n",
      "        -51.7064, -51.6190, -54.2974, -51.8446, -55.0375, -56.5656, -52.4609,\n",
      "        -50.7762, -54.7804, -56.6121, -50.5061, -54.9269, -55.8640, -52.9303,\n",
      "        -53.9789, -55.3998, -48.7527, -55.5217, -54.0393, -57.2656, -53.8654,\n",
      "        -58.3008, -60.1141, -57.1390, -61.8564, -56.9197, -58.4042, -58.9843,\n",
      "        -61.0719, -62.6067, -59.5601, -55.9377, -54.3970, -56.7759, -53.9485,\n",
      "        -51.6389, -53.9924, -54.8955, -51.4584, -50.4302, -52.6790, -53.6582,\n",
      "        -54.9213, -54.4041, -46.4554, -47.5317, -45.7191, -47.9839, -47.3402,\n",
      "        -47.9602, -44.3272, -44.5654, -44.0790, -44.5206, -44.9143, -43.8154,\n",
      "        -45.2762, -41.7671, -43.1913, -44.5996, -42.7648, -45.2048, -46.9114,\n",
      "        -43.7299, -44.2434, -44.8311, -45.7046, -49.3543, -44.3602, -48.2489,\n",
      "        -46.5252, -48.1691, -46.6436, -47.3927, -47.7946, -48.3311, -47.1605,\n",
      "        -47.6801, -46.1123, -48.4291, -46.7487, -51.1465, -49.0338, -49.2686,\n",
      "        -50.4395, -48.5347, -52.7913, -51.8223, -49.1909, -51.2688, -50.2705,\n",
      "        -49.2727, -51.0858, -46.0303, -48.0341, -50.1436, -48.9275, -50.8330,\n",
      "        -50.9926, -54.1864, -53.6448, -51.6519, -49.0575, -53.2909, -52.8783,\n",
      "        -53.2470, -53.0435, -51.8042, -48.0178, -49.6747, -48.1353, -51.0544,\n",
      "        -53.9172, -51.4252, -51.8757, -53.7598, -52.3215, -50.3800, -51.9179,\n",
      "        -52.3563, -52.4320, -54.8007, -51.0852, -46.8185, -49.4106, -52.9428,\n",
      "        -51.0238, -49.0513, -54.0054, -52.0753, -50.7145, -51.8811, -49.1470,\n",
      "        -48.6102, -50.6173, -51.2333, -47.5759, -48.7022, -49.9319, -46.4036,\n",
      "        -44.9823, -46.7255, -45.1400, -46.8518, -49.5283, -49.3838, -47.6332,\n",
      "        -46.4385, -49.0653, -47.7410, -53.6216, -53.5512, -54.8971, -55.4231,\n",
      "        -52.5554, -52.3110, -55.1744, -51.5787, -54.9482, -56.1482, -55.0147,\n",
      "        -54.9642, -51.8091, -53.5850, -54.2814, -53.3748, -56.2026, -58.0403,\n",
      "        -55.8793, -56.3217, -63.3555, -60.8789, -60.0742, -60.6534, -60.4408,\n",
      "        -63.1034, -54.7482, -53.2540, -50.9263, -52.0106, -55.2786, -51.1735,\n",
      "        -54.1088, -52.5142, -52.7877, -53.7953, -52.6613, -54.2412, -46.9291,\n",
      "        -49.4288, -45.6847, -44.2565, -45.0767, -43.0582, -42.5635, -40.0054,\n",
      "        -42.2028, -44.0995, -45.4966, -43.1522, -44.6567, -46.6504, -44.7570,\n",
      "        -48.9585, -46.0822, -43.1849, -45.3995, -47.6574, -44.2350, -47.3302,\n",
      "        -45.8878, -45.0125, -45.5004, -45.2095, -48.7262, -41.9641, -47.8682,\n",
      "        -45.8698, -48.4622, -47.4988, -47.5763, -46.2465, -46.1196, -48.6085,\n",
      "        -49.8939, -48.2625, -50.2164, -49.2496, -48.0477, -47.5835, -49.3297,\n",
      "        -50.1826, -46.3557, -48.8396, -50.5529, -50.6950, -47.0061, -48.6550,\n",
      "        -51.0499, -49.9707, -50.6331, -47.6616, -50.9605, -51.9761, -49.4940,\n",
      "        -52.7561, -51.4551, -53.0613, -52.6823, -52.2728, -49.3877, -49.7849,\n",
      "        -52.5557, -53.1112, -50.2670, -49.8654, -51.8875, -49.4490, -51.8807,\n",
      "        -53.6489, -50.8495, -51.3481, -52.8937, -51.8462, -53.8536, -53.4631,\n",
      "        -53.2704, -52.0624, -47.7301, -48.8375, -48.8757, -50.1503, -51.8436,\n",
      "        -49.2050, -50.1058, -51.0739, -45.1301, -47.3025, -47.4683, -47.5660,\n",
      "        -44.5546, -46.4939, -45.2485, -45.6331, -47.1425, -47.8627, -50.5460,\n",
      "        -50.0105, -53.7179, -55.6669, -53.6489, -55.7402, -53.1753, -51.7208,\n",
      "        -52.3652, -53.0770, -52.0193, -50.8500, -56.2228, -52.9067, -52.7120,\n",
      "        -55.7397, -55.2537, -53.8331, -53.5176, -51.5675, -52.5825, -51.2852,\n",
      "        -56.2031, -50.0186, -53.1266, -53.4528, -50.4321, -52.4511, -51.9005,\n",
      "        -53.0457, -51.7701, -51.9877, -53.0125, -54.5953, -53.2004, -50.4694,\n",
      "        -49.6925, -54.7493, -47.1991, -48.4294, -44.9589, -47.8694, -45.2183,\n",
      "        -45.5536, -43.2314, -44.5492, -46.6232, -47.1390, -45.5376, -43.7294,\n",
      "        -45.9211, -46.0616, -45.3755, -45.9103, -48.4324, -47.4304, -49.4603,\n",
      "        -49.8787, -47.0654, -46.8849, -48.3635, -49.7484, -48.0439, -49.8043,\n",
      "        -51.6520, -49.7722, -47.6840, -51.1149, -45.3942, -51.0669, -48.8718,\n",
      "        -47.1876, -49.5313, -48.6281, -49.8356, -50.3236, -49.7278, -48.1980,\n",
      "        -53.3530, -53.2924, -50.9354, -54.3505, -49.6381, -51.3477, -51.6400,\n",
      "        -49.8311, -49.9111, -51.3016, -48.6467, -49.9299, -51.7272, -51.4552,\n",
      "        -49.9095, -53.1638, -51.8141, -51.4989, -53.9863, -54.1604, -49.7156,\n",
      "        -53.2865, -50.6834, -49.9717, -51.6677, -54.5638, -53.2006, -52.1388,\n",
      "        -49.0792, -51.9248, -51.6840, -49.9168, -48.9282, -52.5186, -52.5903,\n",
      "        -55.2715, -53.6668, -54.1630, -53.6869, -51.6936, -54.2507, -55.3632,\n",
      "        -49.3560, -48.1193, -53.0347, -51.9838, -52.3145, -48.6389, -52.0792,\n",
      "        -49.8322, -46.6824, -49.2517, -45.7965, -46.6144, -48.6576, -48.7061,\n",
      "        -46.9220, -49.0137, -48.1981, -46.8446, -50.0870, -49.7239, -46.3253,\n",
      "        -46.2076, -45.3116, -48.8295, -46.1498, -51.3922, -52.2306, -49.0109,\n",
      "        -50.1704, -50.4478, -44.9595, -47.3881, -53.3333, -45.9011, -48.1493,\n",
      "        -45.7647, -47.6284, -47.1749, -50.1842, -50.2023, -49.8258, -48.8901,\n",
      "        -43.3739, -42.2590, -46.4940, -44.6715, -49.8193, -45.6134, -46.6080,\n",
      "        -43.8143, -47.4537, -46.1730, -48.5436, -47.1005, -46.5449, -46.3585,\n",
      "        -45.6445, -43.4115, -47.5670, -49.0250, -53.7806, -52.9725, -49.7891,\n",
      "        -52.8190, -53.8972, -50.9005, -49.8240, -46.8221, -49.0890, -46.6604,\n",
      "        -47.9800, -46.5703, -42.5018, -44.7902, -46.2435, -44.6018, -45.2304,\n",
      "        -45.8945, -49.0184, -47.1263, -45.8460, -49.4324, -46.9888, -46.3573,\n",
      "        -47.1964, -49.1539, -50.9119, -48.0936, -54.5237, -52.3826, -53.5538,\n",
      "        -49.7253, -52.4750, -50.0065, -47.5826, -51.7937, -49.8543, -48.4450,\n",
      "        -45.3890, -46.1689, -45.5571, -43.4634, -45.7279, -48.1958, -47.7860,\n",
      "        -47.5448, -46.7738, -47.4076, -49.1294, -50.8586, -48.8890, -45.0536,\n",
      "        -46.9358, -45.8223, -46.9230, -51.5236, -49.1877, -51.5187, -52.6078,\n",
      "        -56.2529, -50.5758, -51.0670, -52.6747, -47.7712, -48.9519, -46.1138,\n",
      "        -44.8555, -46.2083, -46.0027, -44.3345, -47.3254, -46.1721, -49.3745,\n",
      "        -47.8180, -47.3032, -50.0795, -46.5916, -45.5154, -48.6322, -47.6613,\n",
      "        -46.1786, -49.1880, -48.1725, -48.5526, -48.1585, -49.6478, -51.7479,\n",
      "        -49.7361, -55.1901, -49.0243, -52.5396, -50.3936, -46.5577, -46.2822,\n",
      "        -49.1303, -47.2197, -46.0814, -39.9900, -47.7006, -45.4437, -46.4100,\n",
      "        -48.8638, -45.3889, -46.3683, -43.5103, -41.9255, -43.3964, -45.4674,\n",
      "        -46.0557, -48.8032, -49.9534, -47.0538, -51.1736, -51.7336, -54.3502,\n",
      "        -52.3463, -50.4503, -54.0717, -53.3345, -50.2251, -52.0023, -48.0302,\n",
      "        -46.5120, -44.8417, -45.2388, -41.9042, -45.6384, -43.1007, -44.3900,\n",
      "        -46.0017, -48.1903, -48.8717, -46.8865, -48.5873, -48.4042, -47.2169,\n",
      "        -48.4650, -48.6814, -44.7487, -45.2271, -47.7935, -44.8485, -43.5931,\n",
      "        -48.1700, -47.7372, -48.8242, -52.7017, -52.0317, -53.3892, -52.7228,\n",
      "        -55.5726, -51.5835, -50.6625, -50.3773, -55.3895, -50.7222, -51.1879,\n",
      "        -47.5337, -47.1260, -46.8783, -45.0780, -45.2011, -40.9598, -43.5188,\n",
      "        -46.1815, -47.3976, -49.8153, -46.0293, -45.5532, -47.5413, -48.7761,\n",
      "        -49.2097, -46.6738, -47.2852, -50.2228, -49.2181, -47.8619, -50.5963,\n",
      "        -48.5941, -47.4534, -49.3871, -48.3731, -47.8413, -46.4662, -51.6937,\n",
      "        -47.6625, -49.3453, -44.9908, -51.5645, -49.9442, -50.3186, -45.0453,\n",
      "        -51.4816, -42.8074, -47.8389, -45.1631, -47.1506, -45.7744, -44.4739,\n",
      "        -48.7238, -49.6417, -48.1016, -49.2369, -50.9616, -55.0667, -53.8190,\n",
      "        -57.6497, -50.6646, -54.9954, -51.8616, -50.6073, -52.7714, -56.4127,\n",
      "        -56.1370, -52.6627, -53.2201, -51.8074, -54.5129, -50.3517, -51.3999,\n",
      "        -53.9111, -53.2271, -54.1314, -51.6051, -50.6566, -50.7505, -49.6467,\n",
      "        -48.4528, -45.6119, -42.4316, -46.0123, -44.8350, -44.7756, -47.0555,\n",
      "        -45.0802, -45.2289, -47.3466, -49.2677, -49.1041, -48.0508, -46.2269,\n",
      "        -44.4259, -45.2827, -50.9811, -48.5626, -50.0110, -50.2108, -52.4905,\n",
      "        -50.4727, -48.4233, -49.0256, -49.9093, -52.4147, -48.2821, -55.4254,\n",
      "        -49.5894, -50.2300, -47.9069, -52.2548, -48.4396, -51.1235, -48.2679,\n",
      "        -50.7499, -48.1577, -49.8156, -51.3607, -53.4661, -49.3958, -53.4788,\n",
      "        -52.2621, -50.4246, -48.9236, -53.7280, -49.7914, -53.7972, -50.2937,\n",
      "        -52.0825, -50.5986, -52.7136, -54.0981, -56.2278, -55.8688, -52.5032,\n",
      "        -51.5314, -53.8212, -50.4272, -48.9878, -49.9471, -51.1433, -53.8848,\n",
      "        -54.0654, -49.5461, -52.1057, -50.2159, -52.7423, -51.6084, -50.2191,\n",
      "        -50.4854, -50.5073, -51.9736, -52.8121, -50.3049, -49.6089, -51.0963,\n",
      "        -51.8358, -52.3226, -55.6914, -52.6083, -51.1779, -51.5243, -51.1973,\n",
      "        -49.0103, -51.6075, -51.6631, -51.6714, -50.4302, -51.6293, -49.7212,\n",
      "        -50.2540, -47.6488, -50.6528, -47.5159, -46.0718, -47.5925, -45.7162,\n",
      "        -45.1339, -44.6461, -47.9522, -49.7814, -49.5673, -51.6433, -50.8915,\n",
      "        -52.5107, -50.1434, -53.7144, -53.9331, -55.2985, -52.7362, -50.7112,\n",
      "        -53.0498, -57.2073, -52.3572, -53.0595, -53.7754, -57.1935, -54.9111,\n",
      "        -55.4248, -52.9589, -53.0974, -58.7035, -56.2257, -50.3258, -51.7363,\n",
      "        -47.2944, -48.1731, -46.4573, -46.8462, -44.8746, -44.0358, -42.8494,\n",
      "        -48.0062, -42.0408, -44.9677, -45.7837, -41.5331, -44.9033, -44.4772,\n",
      "        -44.7999, -48.3833, -47.9618, -48.0974, -43.4022, -48.3381, -47.9053,\n",
      "        -50.6704, -49.1158, -46.2661, -49.5249, -51.0884, -46.5936, -47.2302,\n",
      "        -53.2876, -49.3070, -51.3878, -49.8060, -49.7882, -49.3175, -50.5076,\n",
      "        -50.4044, -49.2080, -50.8263, -51.1279, -49.5369, -51.4636, -52.7025,\n",
      "        -53.3035, -48.3060, -51.0952, -53.1502, -52.2070, -48.7915, -54.8420,\n",
      "        -53.2991, -54.8060, -49.2130, -52.9067, -56.5697, -52.5836, -54.2742,\n",
      "        -50.7977, -49.0160, -50.8655, -50.4193, -51.5436, -51.2341, -49.9375,\n",
      "        -51.9664, -52.3727, -51.7459, -51.8679, -51.2804, -54.0664, -49.9445,\n",
      "        -49.8646, -49.8001, -51.0079, -51.1958, -51.9551, -49.8090, -50.1547,\n",
      "        -50.0295, -53.5868, -49.8184, -55.7953, -51.1722, -52.8925, -52.1163,\n",
      "        -55.4750, -48.7257, -51.7300, -53.6320, -53.4823, -50.5096, -49.6119,\n",
      "        -52.9453, -52.1746, -50.6499, -51.2784, -52.1411, -51.2825, -53.4627,\n",
      "        -51.6458, -52.8424, -50.1465, -46.8638, -50.4568, -52.4788, -51.5598,\n",
      "        -51.0690, -53.2396, -53.6939, -51.0282, -51.8240, -51.1658, -51.1891,\n",
      "        -50.2284, -54.5078, -51.5919, -53.2665, -53.8228, -51.4083, -54.2825,\n",
      "        -49.4147, -51.6814, -51.2084, -56.7183, -52.1897, -49.6969, -51.0579,\n",
      "        -53.8721, -52.8252, -52.9170, -55.1169, -53.2929, -52.1234, -50.4509,\n",
      "        -54.9347, -48.5663, -49.0689, -54.4196, -55.5651, -47.6659, -50.6950,\n",
      "        -51.2380, -52.6876, -50.8495, -49.0414, -50.4232, -51.1753, -49.7416,\n",
      "        -53.1874, -51.5193, -54.1451, -52.7161, -53.2159, -50.8735, -50.4325,\n",
      "        -54.2560, -50.1029, -51.8749, -50.7637, -54.8446, -55.6429, -46.8016,\n",
      "        -50.2445, -51.8594, -56.4492, -52.3723, -55.0129, -51.9713],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) # Small sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -2., -2., -3., -3., -3., -3., -3., -3., -3., -3., -2., -2., -2.,\n",
      "        -2., -2., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1., -1., -1., -2.,\n",
      "        -2., -2., -2., -2., -3., -3., -3., -3., -3., -3., -3., -2., -2., -2.,\n",
      "        -2., -2., -2., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1., -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "        -2., -2., -2., -2., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "        -2., -2., -2., -2., -2., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -3., -3., -3., -3., -3., -2., -2., -2., -2., -2., -2.,\n",
      "        -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -3., -3., -3., -3., -3., -3., -3., -2., -2., -2., -2., -2., -2.,\n",
      "        -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1.,\n",
      "        -1., -1., -1., -1., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -2., -1., -1., -1., -1., -1., -1.,\n",
      "        -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2., -3., -3.,\n",
      "        -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -2., -2., -2., -2.,\n",
      "        -2., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1., -1., -1., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -2., -2., -1., -1., -1., -1., -1.,\n",
      "        -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1.,\n",
      "        -1., -1., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -1., -1., -1., -1., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1., -1., -1., -1., -2.,\n",
      "        -2., -2., -2., -2., -2., -3., -3., -3., -3., -2., -2., -2., -2., -2.,\n",
      "        -2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0., -1., -1., -1., -1., -1., -1., -1., -1., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -3., -3., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1., -1., -1.,\n",
      "        -1., -1., -1., -1., -1., -1., -1., -2., -2., -2., -2., -2., -2., -2.,\n",
      "        -2., -2., -2., -2., -2., -2., -1., -1., -1., -1., -1., -1., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1., -1., -1., -1.])\n"
     ]
    }
   ],
   "source": [
    "print(labels.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Summary: Run over the whole dataset (without batch 11) 2 times completed successfully. The model does learn from the dataset. Interesting shift (50 RMSE) in outputs highlighted in the cells above. It could be that simply more time is needed for the model to approach the (-5,0) Newton range. Data is not shuffled (to preserve time-related aspects), overfitting likely to occur in further runs. We can use batch_11.npz as test set maybe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
