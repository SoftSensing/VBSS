{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "filename_1 = 'output_batch_1.npz'\n",
    "filename_0 = 'output_batch_0.npz'\n",
    "filename_2 = 'output_batch_2.npz'\n",
    "filename_3 = 'output_batch_3.npz'\n",
    "filename_4 = 'output_batch_4.npz'\n",
    "filename_5 = 'output_batch_5.npz'\n",
    "filename_6 = 'output_batch_6.npz'\n",
    "filename_8 = 'output_batch_8.npz'\n",
    "filename_9 = 'output_batch_9.npz'\n",
    "filename_10 = 'output_batch_10.npz'\n",
    "filename_11 = 'output_batch_11.npz'\n",
    "folder_path = 'Measurements'\n",
    "current_directory = 'c:/Users/20201558/Desktop/Work related stuff/Master/Year 4/Quartile 3/Interdisciplinary Team Project [5ARIP10]/Code/CNN-RNN'\n",
    "file_path_0 = os.path.join(current_directory,folder_path, filename_0)\n",
    "file_path_1 = os.path.join(current_directory,folder_path, filename_1)\n",
    "file_path_2 = os.path.join(current_directory,folder_path, filename_2)\n",
    "file_path_3 = os.path.join(current_directory,folder_path, filename_3)\n",
    "file_path_4 = os.path.join(current_directory,folder_path, filename_4)\n",
    "file_path_5 = os.path.join(current_directory,folder_path, filename_5)\n",
    "file_path_6 = os.path.join(current_directory,folder_path, filename_6)\n",
    "file_path_8 = os.path.join(current_directory,folder_path, filename_8)\n",
    "file_path_9 = os.path.join(current_directory,folder_path, filename_9)\n",
    "file_path_10 = os.path.join(current_directory,folder_path, filename_10)\n",
    "file_path_11 = os.path.join(current_directory,folder_path, filename_11)\n",
    "data_0 = np.load(file_path_0)\n",
    "data_1 = np.load(file_path_1)\n",
    "data_2 = np.load(file_path_2)\n",
    "data_3 = np.load(file_path_3)\n",
    "data_4 = np.load(file_path_4)\n",
    "data_5 = np.load(file_path_5)\n",
    "data_6 = np.load(file_path_6)\n",
    "data_8 = np.load(file_path_8)\n",
    "data_9 = np.load(file_path_9)\n",
    "data_10 = np.load(file_path_10)\n",
    "#data_11 = np.load(file_path_11)\n",
    "#data = [*data_0,*data_1,*data_2,*data_3,*data_4,*data_5,*data_6,*data_8,*data_9,*data_10]\n",
    "data = data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = data['frames']\n",
    "forces_data = data['forces']\n",
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "features_train, features_test, targets_train, targets_test = train_test_split(frames,\n",
    "                                                                             forces_data[:,2],\n",
    "                                                                             test_size = 0.2,\n",
    "                                                                             random_state = 42) \n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(features_test)\n",
    "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 30, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.conv2 = nn.Conv2d(30, 60, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(30 * 135 * 240, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        #x = self.pool(nn.ELU()(self.conv2(x)))\n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (-1, 1, 30 * 135 * 240))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        print(\"Size of h0:\", h0.size())  # Print size of h0\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 800\n",
    "n_iters = 5\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 270, 480, 3])\n",
      "Size of x: torch.Size([800, 1, 972000])\n",
      "Size of h0: torch.Size([1, 800, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\my-env4\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([800])) that is different to the input size (torch.Size([800, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6220800000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m error(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\my-env4\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\my-env4\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6220800000 bytes."
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 50 # Consider the whole batch to be temporally correlated\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        print(images.shape)  # Add this line to check the shape of images\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Summary: Running ended at backwards propagation, most probably due to memory issues. The hyperparameters and layers of the CNN must be changed in order to reduce the number of input features in the RNN (currently 30 x 135 x 240), i.e. not overload the computer. Still, it reaching this point with correct partitioning of data and correct passing through the network indicates that this code could be used to obtain the first results with a slightly different network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
