{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "\n",
    "# relative path to npz files\n",
    "path = 'Measurements'\n",
    "file_name = 'output_batch_%d.npz'\n",
    "\n",
    "# Training\n",
    "# how many frames to load\n",
    "frames_num = 10\n",
    "data_frames = []\n",
    "data_forces = []\n",
    "for i in range(frames_num):\n",
    "    file_path = os.path.join(path,file_name %i)\n",
    "    data = np.load(file_path)\n",
    "    data_frames.append(data['frames'])\n",
    "    data_forces.append(data['forces'])\n",
    "\n",
    "combined_frames = np.concatenate(data_frames, axis=0)\n",
    "combined_forces = np.concatenate(data_forces, axis=0)\n",
    "\n",
    "# Test\n",
    "# file_path = os.path.join(path, file_name %11)\n",
    "# test_data = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split. Size of train data is 10/11 and size of test data is 1/11 \n",
    "# features_train, features_test, targets_train, targets_test = train_test_split(combined_frames,\n",
    "#                                                                              combined_forces[:,2],\n",
    "#                                                                              test_size = (1/11),\n",
    "#                                                                              random_state = 42) \n",
    "# train_test_split NOT RECOMMENDED, because it shuffles temporally dependent data\n",
    "features_train = combined_frames\n",
    "targets_train = combined_forces[:,2]\n",
    "# features_test = test_data['frames']\n",
    "# targets_test = test_data['forces']\n",
    "# targets_test = targets_test[:,2]\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train)\n",
    "\n",
    "# create feature and targets tensor for test set. \n",
    "# featuresTest = torch.from_numpy(features_test)\n",
    "# targetsTest = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 9, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(9, 18, kernel_size = 5, stride = 1, padding = 2) # If needed\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(18 * 16 * 30, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ELU()(self.conv2(x))) # If needed\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (-1, 1, 18 * 16 * 30))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        \n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        #print(\"Size of h0:\", h0.size())  # Print size of h0 - Debugging - Obsolete\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1000\n",
    "num_epochs = 10\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "# test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 1  Loss: 1681.076171875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 2  Loss: 141.35411071777344\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 3  Loss: 145.57406616210938\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 4  Loss: 607.0349731445312\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 5  Loss: 654.3024291992188\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 6  Loss: 386.0529479980469\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 7  Loss: 117.34428405761719\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 8  Loss: 5.088362216949463\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 9  Loss: 37.02680587768555\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 10  Loss: 162.85350036621094\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 11  Loss: 277.4777526855469\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 12  Loss: 259.69415283203125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 13  Loss: 181.89541625976562\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 14  Loss: 73.65056610107422\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 15  Loss: 12.50928020477295\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 16  Loss: 4.843990802764893\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 17  Loss: 33.556190490722656\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 18  Loss: 74.45561981201172\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 19  Loss: 114.46280670166016\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 20  Loss: 96.45388793945312\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 21  Loss: 56.17400360107422\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 22  Loss: 28.43064308166504\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 23  Loss: 6.467336654663086\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 24  Loss: 2.1124041080474854\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 25  Loss: 10.438414573669434\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 26  Loss: 29.202117919921875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 27  Loss: 43.75541305541992\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 28  Loss: 45.64220428466797\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 29  Loss: 25.574438095092773\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 30  Loss: 18.19537353515625\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 31  Loss: 10.041228294372559\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 32  Loss: 2.3739473819732666\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 33  Loss: 4.8092942237854\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 34  Loss: 16.010311126708984\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 35  Loss: 23.850696563720703\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 36  Loss: 22.451040267944336\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 37  Loss: 15.617663383483887\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 38  Loss: 9.03918743133545\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 39  Loss: 5.293463706970215\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 40  Loss: 2.333911418914795\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 41  Loss: 7.279001235961914\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 42  Loss: 9.961345672607422\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 43  Loss: 12.088615417480469\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 44  Loss: 8.153507232666016\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 45  Loss: 5.537775039672852\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 46  Loss: 4.246316432952881\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 47  Loss: 2.4784903526306152\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 48  Loss: 2.751389741897583\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 49  Loss: 5.048028469085693\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 50  Loss: 5.277371883392334\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 51  Loss: 3.9240217208862305\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 52  Loss: 4.294951438903809\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 53  Loss: 3.1566665172576904\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 54  Loss: 3.0832388401031494\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 55  Loss: 2.178253412246704\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 56  Loss: 2.4627304077148438\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 57  Loss: 3.3120744228363037\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 58  Loss: 4.810332298278809\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 59  Loss: 2.1948556900024414\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 60  Loss: 3.4677631855010986\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 61  Loss: 4.256723403930664\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 62  Loss: 2.4405040740966797\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 63  Loss: 1.9305752515792847\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 64  Loss: 2.9426372051239014\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 65  Loss: 3.837033271789551\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 66  Loss: 3.558587074279785\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 67  Loss: 2.541078805923462\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 68  Loss: 2.708526372909546\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 69  Loss: 2.1454906463623047\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 70  Loss: 2.2218925952911377\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 71  Loss: 3.721700668334961\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 72  Loss: 3.136636972427368\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 73  Loss: 2.781446933746338\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 74  Loss: 1.8292655944824219\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 75  Loss: 1.8635927438735962\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 76  Loss: 2.228013753890991\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 77  Loss: 1.955634355545044\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 78  Loss: 2.554344415664673\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 79  Loss: 2.645927667617798\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 80  Loss: 2.25650691986084\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 81  Loss: 2.350947380065918\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 82  Loss: 2.0806946754455566\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 83  Loss: 1.9330041408538818\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 84  Loss: 1.8003218173980713\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 85  Loss: 1.842869520187378\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 86  Loss: 2.184004783630371\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 87  Loss: 2.0153965950012207\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 88  Loss: 2.5556068420410156\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 89  Loss: 1.806453824043274\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 90  Loss: 2.0534889698028564\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 91  Loss: 2.46492862701416\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 92  Loss: 2.021785259246826\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 93  Loss: 1.8246355056762695\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 94  Loss: 2.0380215644836426\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 95  Loss: 2.1230854988098145\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 96  Loss: 2.1655352115631104\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 97  Loss: 1.863050103187561\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 98  Loss: 2.4390134811401367\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 99  Loss: 1.705880880355835\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 100  Loss: 2.051814079284668\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 50 # Consider the whole batch to be temporally correlated\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('~~~BEGINNING OF DATASET~~~')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        # print(images.shape)  # Add this line to check the shape of images - Debugging purposes\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data)\n",
    "        # Print Loss\n",
    "        if count % 1 == 0: # for now print for every iteration\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1681.0762), tensor(141.3541), tensor(145.5741), tensor(607.0350), tensor(654.3024), tensor(386.0529), tensor(117.3443), tensor(5.0884), tensor(37.0268), tensor(162.8535), tensor(277.4778), tensor(259.6942), tensor(181.8954), tensor(73.6506), tensor(12.5093), tensor(4.8440), tensor(33.5562), tensor(74.4556), tensor(114.4628), tensor(96.4539), tensor(56.1740), tensor(28.4306), tensor(6.4673), tensor(2.1124), tensor(10.4384), tensor(29.2021), tensor(43.7554), tensor(45.6422), tensor(25.5744), tensor(18.1954), tensor(10.0412), tensor(2.3739), tensor(4.8093), tensor(16.0103), tensor(23.8507), tensor(22.4510), tensor(15.6177), tensor(9.0392), tensor(5.2935), tensor(2.3339), tensor(7.2790), tensor(9.9613), tensor(12.0886), tensor(8.1535), tensor(5.5378), tensor(4.2463), tensor(2.4785), tensor(2.7514), tensor(5.0480), tensor(5.2774), tensor(3.9240), tensor(4.2950), tensor(3.1567), tensor(3.0832), tensor(2.1783), tensor(2.4627), tensor(3.3121), tensor(4.8103), tensor(2.1949), tensor(3.4678), tensor(4.2567), tensor(2.4405), tensor(1.9306), tensor(2.9426), tensor(3.8370), tensor(3.5586), tensor(2.5411), tensor(2.7085), tensor(2.1455), tensor(2.2219), tensor(3.7217), tensor(3.1366), tensor(2.7814), tensor(1.8293), tensor(1.8636), tensor(2.2280), tensor(1.9556), tensor(2.5543), tensor(2.6459), tensor(2.2565), tensor(2.3509), tensor(2.0807), tensor(1.9330), tensor(1.8003), tensor(1.8429), tensor(2.1840), tensor(2.0154), tensor(2.5556), tensor(1.8065), tensor(2.0535), tensor(2.4649), tensor(2.0218), tensor(1.8246), tensor(2.0380), tensor(2.1231), tensor(2.1655), tensor(1.8631), tensor(2.4390), tensor(1.7059), tensor(2.0518)]\n",
      "1.4386714\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)\n",
    "print(np.sqrt(np.mean(loss_list[-10:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0124e+00, -1.6255e+00, -2.4482e+00, -2.2931e+00, -2.1744e+00,\n",
      "        -1.8728e+00, -2.1315e+00, -1.9730e+00, -1.7552e+00, -1.6828e+00,\n",
      "        -1.6514e+00, -2.0475e+00, -2.1053e+00, -1.4348e+00, -2.5052e+00,\n",
      "        -1.6313e+00, -1.8625e+00, -2.6759e+00, -1.5475e+00, -1.6707e+00,\n",
      "        -1.3077e+00, -1.9264e+00, -1.5310e+00, -1.3943e+00, -2.7603e+00,\n",
      "        -1.6588e+00, -1.5918e+00, -2.4011e+00, -2.9571e+00, -2.4022e+00,\n",
      "        -2.2389e+00, -2.9246e+00, -2.0929e+00, -1.0412e+00, -2.5827e+00,\n",
      "        -1.9896e+00, -1.9685e+00, -2.5351e+00, -1.8889e+00, -2.0059e+00,\n",
      "        -1.1405e+00, -1.8913e+00, -1.6616e+00, -2.4287e+00, -2.0257e+00,\n",
      "        -1.3838e+00, -1.6378e+00, -2.4027e+00, -1.6009e+00, -1.6436e+00,\n",
      "        -1.9559e+00, -1.5582e+00, -1.6805e+00, -2.5315e+00, -2.2803e+00,\n",
      "        -3.1635e+00, -1.6446e+00, -2.0440e+00, -1.4603e+00, -2.2316e+00,\n",
      "        -1.1591e+00, -1.2438e+00, -2.4220e+00, -1.8457e+00, -1.8852e+00,\n",
      "        -2.0594e+00, -2.0096e+00, -1.8593e+00, -2.1260e+00, -1.2326e+00,\n",
      "        -1.3179e+00, -2.1175e+00, -2.0821e+00, -2.0105e+00, -1.2793e+00,\n",
      "        -1.4457e+00, -2.0430e+00, -1.9360e+00, -1.8935e+00, -2.9924e+00,\n",
      "        -2.1484e+00, -2.1840e+00, -1.9362e+00, -2.2884e+00, -1.6471e+00,\n",
      "        -2.3163e+00, -1.8315e+00, -1.6920e+00, -2.0065e+00, -1.5275e+00,\n",
      "        -1.6910e+00, -2.1993e+00, -1.2989e+00, -2.2288e+00, -1.5930e+00,\n",
      "        -2.1243e+00, -1.8684e+00, -1.7311e+00, -2.1404e+00, -1.8662e+00,\n",
      "        -1.8538e+00, -2.1165e+00, -1.4057e+00, -2.0910e+00, -2.5188e+00,\n",
      "        -2.1792e+00, -2.1613e+00, -2.0826e+00, -1.8548e+00, -1.3970e+00,\n",
      "        -2.1333e+00, -1.5886e+00, -2.7461e+00, -1.4838e+00, -1.7715e+00,\n",
      "        -2.5328e+00, -2.2818e+00, -1.8518e+00, -2.1874e+00, -1.7357e+00,\n",
      "        -1.9270e+00, -1.6717e+00, -1.4589e+00, -1.9532e+00, -2.5422e+00,\n",
      "        -1.9516e+00, -2.4337e+00, -1.6800e+00, -2.8754e+00, -2.1327e+00,\n",
      "        -2.6800e+00, -1.7786e+00, -2.0109e+00, -1.6527e+00, -1.6441e+00,\n",
      "        -1.7987e+00, -1.8468e+00, -1.6600e+00, -2.1506e+00, -2.0059e+00,\n",
      "        -2.0949e+00, -2.4573e+00, -2.5350e+00, -1.4505e+00, -2.0230e+00,\n",
      "        -1.8403e+00, -2.0118e+00, -2.3919e+00, -1.5411e+00, -2.1022e+00,\n",
      "        -1.0580e+00, -2.0328e+00, -1.6512e+00, -1.4304e+00, -1.9911e+00,\n",
      "        -2.4942e+00, -2.6770e+00, -2.3578e+00, -2.3578e+00, -2.2165e+00,\n",
      "        -1.8686e+00, -2.6283e+00, -2.2658e+00, -1.0607e+00, -2.7672e+00,\n",
      "        -8.8106e-01, -2.0909e+00, -2.1100e+00, -1.3327e+00, -9.7446e-01,\n",
      "        -2.2606e+00, -2.6254e+00, -2.4116e+00, -2.9824e+00, -2.3995e+00,\n",
      "        -1.7583e+00, -2.0135e+00, -1.8025e+00, -2.3453e+00, -1.4733e+00,\n",
      "        -2.0326e+00, -1.7660e+00, -1.6823e+00, -1.7934e+00, -1.4855e+00,\n",
      "        -1.6221e+00, -2.2854e+00, -1.3758e+00, -1.7771e+00, -1.3010e+00,\n",
      "        -1.6374e+00, -1.3818e+00, -2.3036e+00, -1.2705e+00, -2.2685e+00,\n",
      "        -1.3564e+00, -1.6033e+00, -1.4934e+00, -2.0876e+00, -1.6961e+00,\n",
      "        -2.4829e+00, -2.1657e+00, -2.3659e+00, -2.0080e+00, -2.1616e+00,\n",
      "        -1.4105e+00, -1.6517e+00, -2.4367e+00, -1.9682e+00, -1.6840e+00,\n",
      "        -2.4243e+00, -1.7154e+00, -2.8919e+00, -2.1430e+00, -1.7481e+00,\n",
      "        -2.6712e+00, -2.6371e+00, -1.9244e+00, -1.8950e+00, -1.9522e+00,\n",
      "        -2.2712e+00, -1.7271e+00, -1.4179e+00, -1.3358e+00, -1.2433e+00,\n",
      "        -9.0551e-01, -8.7673e-01, -1.1851e+00, -1.0448e+00, -7.2421e-01,\n",
      "        -1.4614e+00, -4.9973e-01, -1.0863e+00, -1.2455e+00,  1.5046e-01,\n",
      "        -1.3299e+00, -1.1672e+00, -4.5236e-01, -3.0217e-01, -4.4095e-02,\n",
      "        -2.5520e-01, -1.5079e+00, -9.8033e-01, -6.7792e-01,  2.0375e-01,\n",
      "        -1.1876e+00, -1.3707e+00,  1.0057e-01, -8.1716e-02, -7.5260e-02,\n",
      "         4.4758e-01, -4.7545e-01, -7.7331e-01,  1.8987e-01, -4.7183e-01,\n",
      "        -9.4673e-01, -5.5221e-01,  3.8515e-01,  2.1310e-01,  3.2379e-02,\n",
      "         2.8621e-01, -1.8855e-01, -1.3154e+00, -3.1584e-04, -4.1470e-01,\n",
      "        -2.7450e-01,  4.1055e-01, -9.1611e-01,  2.1019e-01,  4.4050e-01,\n",
      "        -1.0797e-01, -6.4272e-01, -1.0416e-01, -1.2478e-01,  1.2574e-01,\n",
      "        -3.6075e-01,  3.9587e-01, -7.7731e-01,  5.0384e-01, -4.2410e-01,\n",
      "        -1.6511e-01,  2.4737e-01,  2.9908e-01,  2.9788e-01, -3.9437e-01,\n",
      "         6.4701e-01, -5.6264e-01, -4.7518e-01,  7.8956e-01, -4.4195e-01,\n",
      "        -2.9116e-01,  3.1793e-01, -2.9034e-01,  7.2733e-02, -1.4622e-02,\n",
      "         6.2894e-01, -1.1462e+00, -5.8919e-01,  5.2023e-01,  9.7418e-02,\n",
      "        -7.2165e-01,  1.3625e-01,  5.1788e-01, -3.0615e-01, -4.3146e-01,\n",
      "        -4.8753e-01, -2.7514e-01, -3.7354e-01, -4.7236e-01,  1.6021e-02,\n",
      "        -4.0981e-03, -1.1512e+00, -4.1889e-01, -1.1044e+00,  6.8970e-02,\n",
      "        -1.0916e+00, -6.3826e-01, -1.1362e+00, -1.2159e+00, -9.8281e-01,\n",
      "        -1.3106e+00, -2.1036e+00, -2.0609e+00, -1.1766e+00, -2.3393e+00,\n",
      "        -1.1014e+00, -1.1157e+00, -2.0815e+00, -1.5517e+00, -1.6872e+00,\n",
      "        -1.1499e+00, -1.8030e+00, -1.6319e+00, -1.6396e+00, -1.9113e+00,\n",
      "        -2.2916e+00, -2.3230e+00, -1.3990e+00, -1.3457e+00, -1.2439e+00,\n",
      "        -2.0070e+00, -1.1883e+00, -8.9728e-01, -6.9179e-01, -5.0420e-01,\n",
      "         6.8193e-02, -1.2706e+00, -9.5995e-01, -5.3135e-01, -5.6323e-01,\n",
      "        -6.0982e-01, -2.9346e-01, -4.1279e-01, -7.9185e-01,  1.1209e-01,\n",
      "        -2.3746e-01, -1.2828e-01, -8.0647e-01,  2.8903e-01,  1.4324e-01,\n",
      "        -5.4269e-01,  2.4777e-01, -3.4592e-01,  4.3962e-01,  3.3902e-01,\n",
      "        -1.8706e-02, -1.9848e-01,  3.4325e-01, -8.0300e-01, -2.4840e-01,\n",
      "         4.1030e-01, -2.1061e-01, -4.1457e-01, -3.4065e-01,  5.2676e-01,\n",
      "        -1.1910e+00,  7.8442e-01,  6.7569e-01,  3.3299e-01, -3.8421e-01,\n",
      "        -2.8217e-01, -6.1547e-01,  1.8499e-02,  2.8612e-01,  2.2632e-01,\n",
      "         2.8598e-01,  7.5875e-02,  6.2685e-01,  8.2648e-03,  5.3113e-01,\n",
      "        -5.1089e-02,  1.3703e-01, -2.6502e-01,  1.1545e-01, -1.0065e-01,\n",
      "         3.0440e-01,  1.2464e+00,  6.5533e-01, -4.8732e-02,  7.2734e-01,\n",
      "         7.7784e-01,  1.0433e+00,  3.3429e-01,  6.0829e-01,  3.6602e-01,\n",
      "         3.1569e-01,  3.0373e-01,  1.0269e+00, -8.4173e-02,  2.8739e-02,\n",
      "         8.2254e-01, -8.8593e-03,  1.7307e-01,  2.1696e-01,  1.1157e-01,\n",
      "         8.7253e-01, -2.4328e-01, -6.2396e-01, -8.0232e-01, -1.1399e+00,\n",
      "         2.9053e-01,  5.6159e-01, -3.7813e-01, -1.1390e+00, -6.2454e-01,\n",
      "        -4.7194e-01, -8.8223e-01, -1.0995e+00, -3.7186e-01, -1.1255e+00,\n",
      "        -1.1389e+00, -9.0317e-01, -1.0859e+00, -2.0240e+00, -1.0589e+00,\n",
      "        -1.0100e+00, -2.0980e+00, -1.6206e+00, -8.3089e-01, -2.2184e+00,\n",
      "        -1.8384e+00, -2.4624e+00, -3.0086e+00, -3.1085e+00, -1.4914e+00,\n",
      "        -1.3620e+00, -1.1187e+00, -1.6017e+00, -9.7097e-01, -1.5647e+00,\n",
      "        -2.3034e+00, -1.9815e+00, -1.8535e+00, -1.3282e+00, -1.9543e+00,\n",
      "        -1.2078e+00, -2.2438e+00, -1.9194e+00, -1.3265e+00, -2.1385e+00,\n",
      "        -1.1961e+00, -1.8512e+00, -2.0709e+00, -1.6952e+00, -2.3778e+00,\n",
      "        -2.3872e+00, -1.8657e+00, -2.5843e+00, -2.5649e+00, -2.2451e+00,\n",
      "        -2.1146e+00, -2.4079e+00, -1.9930e+00, -1.2572e+00, -2.3448e+00,\n",
      "        -1.7192e+00, -1.0987e+00, -1.3451e+00, -2.3481e+00, -1.1479e+00,\n",
      "        -1.9697e+00, -1.2766e+00, -1.4487e+00, -1.6458e+00, -2.5548e+00,\n",
      "        -1.8430e+00, -2.7177e+00, -2.6215e+00, -1.8478e+00, -2.5334e+00,\n",
      "        -1.7532e+00, -1.6068e+00, -2.4649e+00, -1.9407e+00, -1.3132e+00,\n",
      "        -1.1484e+00, -1.9610e+00, -1.6932e+00, -6.7590e-01, -8.1584e-01,\n",
      "        -7.0058e-01, -1.0218e+00, -1.6187e+00, -8.6105e-01, -2.1217e-01,\n",
      "        -6.3317e-01, -2.2224e-01, -8.4076e-01, -7.4993e-01, -2.3011e-01,\n",
      "        -3.7750e-01, -3.4872e-01, -5.1953e-01, -7.5536e-01, -1.8068e-01,\n",
      "        -8.1178e-01,  3.2995e-02, -3.5438e-01, -6.7548e-01,  4.0739e-01,\n",
      "         4.4259e-01,  1.3108e-01, -2.5014e-01, -4.4888e-01, -4.9355e-01,\n",
      "        -1.9818e-01, -8.4430e-01, -4.2860e-01,  4.5206e-01,  8.2320e-01,\n",
      "         2.6750e-01,  4.9760e-01,  1.6520e-01, -7.0420e-02, -1.6054e-01,\n",
      "         5.0758e-01,  8.4159e-02, -8.5912e-02,  1.0492e+00,  6.6765e-01,\n",
      "         1.5760e-02, -1.8476e-01,  5.9194e-01, -1.8049e-01, -4.1608e-01,\n",
      "         1.2446e+00,  6.4764e-01, -4.0884e-01,  7.4343e-01, -4.3290e-02,\n",
      "         5.0467e-01,  2.9197e-01, -3.1890e-01, -9.8950e-02, -1.7134e-01,\n",
      "        -7.8343e-01, -7.3344e-01, -2.2727e-01, -3.8098e-01, -5.7187e-01,\n",
      "        -1.0606e+00,  2.6225e-02, -5.8221e-01, -8.2570e-01, -6.1851e-01,\n",
      "        -1.3414e+00, -1.6349e+00, -1.2484e+00, -2.3805e+00, -1.8644e+00,\n",
      "        -2.6137e+00, -2.1506e+00, -2.4990e+00, -1.8662e+00, -1.2792e+00,\n",
      "        -1.1032e+00, -1.5271e+00, -1.0836e+00, -1.5763e+00, -1.0390e+00,\n",
      "        -2.5419e+00, -2.1592e+00, -2.3618e+00, -1.8395e+00, -2.1438e+00,\n",
      "        -1.3833e+00, -2.6961e+00, -1.9696e+00, -2.3263e+00, -1.4791e+00,\n",
      "        -1.9618e+00, -2.1941e+00, -2.4616e+00, -1.9092e+00, -2.7264e+00,\n",
      "        -1.8452e+00, -2.6065e+00, -2.8197e+00, -1.9039e+00, -2.5012e+00,\n",
      "        -2.7677e+00, -3.1253e+00, -2.7846e+00, -2.0663e+00, -2.6904e+00,\n",
      "        -2.6831e+00, -1.9900e+00, -1.9632e+00, -1.6782e+00, -3.0536e+00,\n",
      "        -2.1918e+00, -2.1802e+00, -2.1441e+00, -1.9489e+00, -2.7872e+00,\n",
      "        -2.3582e+00, -2.5307e+00, -2.2572e+00, -2.1976e+00, -3.1533e+00,\n",
      "        -2.1599e+00, -2.2950e+00, -3.1928e+00, -2.5876e+00, -2.9285e+00,\n",
      "        -1.7872e+00, -1.5761e+00, -1.9085e+00, -1.8401e+00, -2.1864e+00,\n",
      "        -2.3543e+00, -1.9957e+00, -2.0233e+00, -1.1334e+00, -1.7186e+00,\n",
      "        -2.7877e+00, -2.2442e+00, -2.6418e+00, -2.0210e+00, -2.2127e+00,\n",
      "        -1.3165e+00, -2.2938e+00, -2.4799e+00, -2.1138e+00, -1.2287e+00,\n",
      "        -1.8618e+00, -1.0539e+00, -1.0642e+00, -1.2934e+00, -2.7157e-01,\n",
      "        -9.2129e-01, -7.1887e-01, -8.3901e-01, -2.0345e-01, -1.9372e+00,\n",
      "        -1.5679e+00, -8.9198e-01, -9.5427e-01, -1.0307e-01, -5.2126e-01,\n",
      "        -1.2296e+00, -8.4373e-01, -6.4925e-01, -1.1076e-01, -1.3983e+00,\n",
      "        -4.4344e-01, -4.7723e-01,  3.9348e-01, -6.5695e-01,  5.8625e-02,\n",
      "        -6.1196e-01, -1.3239e+00, -9.2954e-02,  6.6355e-02, -1.7272e-01,\n",
      "         4.1447e-01, -4.0593e-01, -2.5534e-01,  5.3478e-02,  2.9254e-01,\n",
      "        -6.4374e-01,  2.9895e-01,  2.1323e-01,  3.1125e-01, -5.2338e-01,\n",
      "         2.7252e-01, -3.9586e-02,  5.3526e-01, -2.7886e-01,  2.6898e-01,\n",
      "        -1.6005e-01, -4.4106e-01, -3.2139e-01, -5.0199e-02,  9.3293e-02,\n",
      "         5.2180e-01,  1.8274e-01, -1.1473e-01, -5.5308e-01, -1.0583e+00,\n",
      "         1.1349e-01, -3.8748e-01, -5.4578e-02, -9.2105e-01, -2.3553e-01,\n",
      "        -9.7679e-01, -4.1693e-01, -1.2341e+00, -9.8513e-01, -1.3711e+00,\n",
      "        -2.2033e+00, -1.5324e+00, -1.8893e+00, -1.9586e+00, -2.1017e+00,\n",
      "        -2.2943e+00, -2.7962e+00, -2.6455e+00, -1.7380e+00, -1.3431e+00,\n",
      "        -1.6436e+00, -1.8191e+00, -1.7717e+00, -2.1145e+00, -1.3834e+00,\n",
      "        -2.4592e+00, -1.8140e+00, -2.4426e+00, -2.5160e+00, -2.2744e+00,\n",
      "        -2.1283e+00, -2.8180e+00, -2.7070e+00, -3.4932e+00, -2.9165e+00,\n",
      "        -2.8223e+00, -2.1226e+00, -2.4492e+00, -2.3183e+00, -1.7782e+00,\n",
      "        -2.6796e+00, -2.8622e+00, -2.3278e+00, -1.4781e+00, -2.6783e+00,\n",
      "        -1.1687e+00, -1.7983e+00, -2.2360e+00, -2.2466e+00, -1.4150e+00,\n",
      "        -2.5264e+00, -2.2266e+00, -1.7576e+00, -3.2533e+00, -2.6025e+00,\n",
      "        -2.4939e+00, -2.6589e+00, -2.9947e+00, -3.1426e+00, -2.2765e+00,\n",
      "        -2.2706e+00, -2.8379e+00, -2.1667e+00, -2.0404e+00, -2.6143e+00,\n",
      "        -2.3231e+00, -3.0831e+00, -2.7027e+00, -2.6776e+00, -3.3479e+00,\n",
      "        -2.3826e+00, -2.4890e+00, -2.3415e+00, -2.5806e+00, -1.8860e+00,\n",
      "        -2.0340e+00, -2.0895e+00, -1.7479e+00, -1.8072e+00, -9.6485e-01,\n",
      "        -2.3646e+00, -2.6106e+00, -2.6069e+00, -2.0161e+00, -2.1471e+00,\n",
      "        -2.2094e+00, -1.9256e+00, -2.4044e+00, -1.9093e+00, -2.3404e+00,\n",
      "        -2.3524e+00, -2.5862e+00, -2.3881e+00, -1.2012e+00, -1.5778e+00,\n",
      "        -1.3743e+00, -2.4878e+00, -9.7026e-01, -9.2727e-01, -9.8973e-01,\n",
      "        -1.3091e+00, -1.1062e+00, -1.4431e+00, -1.0472e+00, -1.0725e+00,\n",
      "        -5.7797e-01, -1.1448e+00,  5.8270e-01, -5.1352e-01, -6.7550e-01,\n",
      "        -6.0189e-01, -4.7386e-01, -2.0898e-01,  6.1321e-01, -2.4037e-01,\n",
      "        -6.7473e-01, -1.2856e-01, -1.3149e-01, -9.4777e-01, -5.6824e-01,\n",
      "        -3.0577e-01, -7.1992e-02, -1.3694e-01,  6.8166e-01, -2.8703e-01,\n",
      "        -2.3415e-01, -3.4698e-01,  2.9177e-01, -2.9373e-01, -1.4487e-02,\n",
      "        -2.6316e-01, -2.6037e-01, -1.4955e-01, -3.0514e-01,  1.9115e-01,\n",
      "        -1.9758e-01, -1.1224e-01, -5.2829e-01, -3.6992e-01, -2.2000e-01,\n",
      "        -1.0992e+00,  3.8967e-01, -3.8168e-01, -7.8361e-02, -4.5461e-01,\n",
      "        -1.0450e+00, -6.1093e-01, -4.4057e-01, -8.5425e-01, -4.3299e-01,\n",
      "        -4.3432e-01, -5.8106e-01,  5.5350e-03, -1.0505e+00, -3.9848e-01,\n",
      "         4.3142e-02, -6.6452e-01, -1.3387e+00, -1.0755e+00, -3.4758e-01,\n",
      "        -2.1138e-01, -8.2905e-01, -1.0216e+00, -1.3336e+00, -1.0407e+00,\n",
      "        -1.2760e+00, -2.2946e+00, -2.2388e+00, -2.4787e+00, -2.3086e+00,\n",
      "        -1.7833e+00, -1.8151e+00, -2.0499e+00, -1.8811e+00, -1.5016e+00,\n",
      "        -1.7727e+00, -2.1591e+00, -1.2141e+00, -1.8459e+00, -2.3974e+00,\n",
      "        -2.5559e+00, -2.5232e+00, -3.2277e+00, -2.9527e+00, -2.4661e+00,\n",
      "        -2.2864e+00, -1.9844e+00, -1.5340e+00, -2.4738e+00, -2.7234e+00,\n",
      "        -2.0694e+00, -2.0029e+00, -2.1342e+00, -1.0697e+00, -2.0631e+00,\n",
      "        -2.1331e+00, -3.0599e+00, -1.7465e+00, -3.0973e+00, -2.2696e+00,\n",
      "        -2.1392e+00, -2.4144e+00, -3.1454e+00, -2.0458e+00, -2.1967e+00,\n",
      "        -2.4323e+00, -2.6313e+00, -2.2775e+00, -3.1617e+00, -3.4805e+00,\n",
      "        -1.5333e+00, -3.0479e+00, -1.3855e+00, -2.9014e+00, -1.6595e+00,\n",
      "        -1.9258e+00, -2.0365e+00, -2.6004e+00, -2.8405e+00, -1.8378e+00,\n",
      "        -3.3804e+00, -2.8771e+00, -2.0465e+00, -1.5623e+00, -2.3221e+00,\n",
      "        -1.5962e+00, -2.2232e+00, -2.3562e+00, -2.4561e+00, -2.4880e+00,\n",
      "        -1.9798e+00, -1.8528e+00, -2.1696e+00, -2.1077e+00, -2.3456e+00,\n",
      "        -1.2873e+00, -1.5555e+00, -1.5090e+00, -2.2006e+00, -1.3252e+00,\n",
      "        -1.7962e+00, -2.0299e+00, -3.4017e-01, -3.2798e-01, -5.8589e-01,\n",
      "        -5.2169e-01, -1.1636e+00, -4.8705e-01, -6.1472e-01, -1.5788e+00,\n",
      "        -9.5694e-01,  9.8652e-02, -1.1243e+00, -2.5952e-01, -1.2985e+00,\n",
      "        -2.0703e-01, -8.7553e-01,  3.2492e-01, -2.6124e-01, -5.1284e-01,\n",
      "        -1.4437e+00, -8.1817e-01, -8.6922e-01, -4.7188e-01, -1.0320e+00,\n",
      "         2.1248e-01, -1.0923e+00,  1.6729e-01,  1.9009e-01,  5.9543e-02,\n",
      "        -6.6426e-02,  3.2166e-01, -2.4339e-01, -1.0154e+00, -2.5303e-01,\n",
      "        -1.0004e-01, -2.0339e-01, -5.4491e-01,  3.9665e-01, -1.5272e-01,\n",
      "         1.0459e-01, -7.7006e-02, -2.1317e-01,  1.8260e-01, -6.5111e-01,\n",
      "        -8.4913e-01, -8.7275e-02, -8.2819e-01, -6.2097e-01, -1.1413e-01,\n",
      "        -1.1270e+00, -5.3303e-01, -4.2044e-01, -3.3456e-01, -7.2707e-01,\n",
      "        -3.5367e-01, -4.6782e-01, -8.3904e-01, -7.3900e-01, -4.2068e-01],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) # Small sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0797, -0.9949, -0.9238, -0.8581, -0.7988, -0.7402, -0.6864, -0.6413,\n",
      "        -0.6007, -0.5637, -0.5303, -0.5034, -0.4781, -0.4516, -0.4438, -0.4478,\n",
      "        -0.4525, -0.4622, -0.4775, -0.4831, -0.4824, -0.4824, -0.4840, -0.4829,\n",
      "        -0.4796, -0.4784, -0.4745, -0.4723, -0.4696, -0.4686, -0.4738, -0.4907,\n",
      "        -0.5179, -0.5654, -0.6264, -0.6943, -0.7702, -0.8488, -0.9312, -1.0187,\n",
      "        -1.1152, -1.2284, -1.3486, -1.4846, -1.6458, -1.8335, -2.0371, -2.2583,\n",
      "        -2.4803, -2.6908, -2.8747, -3.0401, -3.1835, -3.2991, -3.3773, -3.4140,\n",
      "        -3.4004, -3.3335, -3.2181, -3.0745, -2.9110, -2.7374, -2.5585, -2.3805,\n",
      "        -2.1968, -2.0178, -1.8431, -1.6839, -1.5378, -1.3989, -1.2665, -1.1307,\n",
      "        -0.9906, -0.8688, -0.7793, -0.7269, -0.7026, -0.7065, -0.7217, -0.7390,\n",
      "        -0.7422, -0.7402, -0.7296, -0.7031, -0.6639, -0.6237, -0.5844, -0.5494,\n",
      "        -0.5165, -0.4928, -0.4744, -0.4658, -0.4598, -0.4682, -0.4804, -0.4992,\n",
      "        -0.5257, -0.5651, -0.6205, -0.6941, -0.7820, -0.8762, -0.9792, -1.0830,\n",
      "        -1.1860, -1.2925, -1.3998, -1.5069, -1.6152, -1.7351, -1.8683, -2.0259,\n",
      "        -2.2131, -2.4256, -2.6556, -2.8895, -3.1145, -3.3130, -3.4665, -3.5678,\n",
      "        -3.6013, -3.5683, -3.4693, -3.3291, -3.1547, -2.9744, -2.7906, -2.6148,\n",
      "        -2.4369, -2.2563, -2.0721, -1.8849, -1.7012, -1.5338, -1.3869, -1.2611,\n",
      "        -1.1644, -1.0881, -1.0243, -0.9720, -0.9171, -0.8640, -0.8028, -0.7433,\n",
      "        -0.6825, -0.6341, -0.6034, -0.6033, -0.6143, -0.6381, -0.6638, -0.6799,\n",
      "        -0.6787, -0.6769, -0.6661, -0.6596, -0.6539, -0.6556, -0.6553, -0.6656,\n",
      "        -0.6793, -0.7042, -0.7418, -0.7945, -0.8589, -0.9346, -1.0123, -1.0908,\n",
      "        -1.1647, -1.2411, -1.3176, -1.4149, -1.5416, -1.7095, -1.9003, -2.1114,\n",
      "        -2.3280, -2.5345, -2.7175, -2.8897, -3.0498, -3.1876, -3.2975, -3.3698,\n",
      "        -3.3850, -3.3393, -3.2283, -3.0734, -2.8929, -2.7096, -2.5290, -2.3726,\n",
      "        -2.2205, -2.0771, -1.9308, -1.7917, -1.6456, -1.5045, -1.3665, -1.2382,\n",
      "        -1.1116, -1.0024, -0.9111, -0.8395, -0.7838, -0.7525, -0.7340, -0.7211,\n",
      "        -0.7133, -0.7147, -0.7136, -0.7130, -0.7133, -0.7065, -0.6910, -0.6715,\n",
      "        -0.6441, -0.6176, -0.5947, -0.5745, -0.5677, -0.5734, -0.5914, -0.6191,\n",
      "        -0.6562, -0.6960, -0.7349, -0.7681, -0.7970, -0.8236, -0.8433, -0.8648,\n",
      "        -0.8818, -0.9050, -0.9295, -0.9649, -1.0187, -1.1147, -1.2539, -1.4445,\n",
      "        -1.6810, -1.9478, -2.2324, -2.5187, -2.7749, -2.9926, -3.1669, -3.2939,\n",
      "        -3.3613, -3.3867, -3.3694, -3.3181, -3.2216, -3.1101, -2.9802, -2.8434,\n",
      "        -2.6935, -2.5383, -2.3763, -2.2236, -2.0704, -1.9252, -1.7893, -1.6546,\n",
      "        -1.5215, -1.3909, -1.2638, -1.1432, -1.0276, -0.9230, -0.8348, -0.7657,\n",
      "        -0.7085, -0.6715, -0.6397, -0.6155, -0.5917, -0.5774, -0.5623, -0.5571,\n",
      "        -0.5541, -0.5537, -0.5525, -0.5506, -0.5400, -0.5329, -0.5240, -0.5265,\n",
      "        -0.5309, -0.5408, -0.5493, -0.5667, -0.5783, -0.6042, -0.6412, -0.6991,\n",
      "        -0.7725, -0.8657, -0.9748, -1.1060, -1.2478, -1.4170, -1.6060, -1.8013,\n",
      "        -2.0041, -2.2108, -2.4030, -2.5873, -2.7657, -2.9232, -3.0525, -3.1439,\n",
      "        -3.1812, -3.1558, -3.0807, -2.9688, -2.8356, -2.6961, -2.5628, -2.4344,\n",
      "        -2.3059, -2.1768, -2.0468, -1.8984, -1.7287, -1.5682, -1.4171, -1.2679,\n",
      "        -1.1438, -1.0552, -0.9667, -0.8872, -0.8263, -0.7788, -0.7338, -0.7042,\n",
      "        -0.6748, -0.6494, -0.6202, -0.5968, -0.5709, -0.5468, -0.5222, -0.5032,\n",
      "        -0.4861, -0.4792, -0.4831, -0.4847, -0.4801, -0.4838, -0.4795, -0.4695,\n",
      "        -0.4577, -0.4593, -0.4465, -0.4412, -0.4448, -0.4661, -0.4885, -0.5210,\n",
      "        -0.5578, -0.5983, -0.6380, -0.7073, -0.8097, -0.9396, -1.0979, -1.2939,\n",
      "        -1.5008, -1.7206, -1.9423, -2.1616, -2.3708, -2.5623, -2.7179, -2.8489,\n",
      "        -2.9510, -3.0242, -3.0765, -3.1207, -3.1452, -3.1465, -3.1118, -3.0394,\n",
      "        -2.9214, -2.7769, -2.6068, -2.4276, -2.2343, -2.0452, -1.8542, -1.6732,\n",
      "        -1.4998, -1.3461, -1.2031, -1.0734, -0.9532, -0.8518, -0.7621, -0.6884,\n",
      "        -0.6280, -0.5819, -0.5368, -0.5053, -0.4781, -0.4612, -0.4546, -0.4563,\n",
      "        -0.4518, -0.4577, -0.4681, -0.4731, -0.4739, -0.4824, -0.4864, -0.4836,\n",
      "        -0.4826, -0.4855, -0.4862, -0.4853, -0.4914, -0.5074, -0.5273, -0.5593,\n",
      "        -0.5932, -0.6382, -0.7148, -0.8072, -0.9102, -1.0490, -1.2074, -1.3741,\n",
      "        -1.5799, -1.8253, -2.0765, -2.3253, -2.5468, -2.7286, -2.8550, -2.9493,\n",
      "        -3.0117, -3.0501, -3.0596, -3.0502, -3.0049, -2.9375, -2.8491, -2.7457,\n",
      "        -2.6359, -2.5251, -2.4159, -2.2942, -2.1545, -1.9922, -1.8248, -1.6577,\n",
      "        -1.5015, -1.3710, -1.2643, -1.1738, -1.0765, -0.9886, -0.9018, -0.8206,\n",
      "        -0.7427, -0.6827, -0.6381, -0.6044, -0.5763, -0.5544, -0.5360, -0.5163,\n",
      "        -0.5010, -0.4921, -0.4828, -0.4796, -0.4773, -0.4737, -0.4725, -0.4779,\n",
      "        -0.4796, -0.4833, -0.4889, -0.4964, -0.5063, -0.5192, -0.5337, -0.5501,\n",
      "        -0.5675, -0.5911, -0.6238, -0.6711, -0.7433, -0.8307, -0.9233, -1.0302,\n",
      "        -1.1394, -1.2457, -1.3570, -1.4870, -1.6171, -1.7593, -1.9088, -2.0682,\n",
      "        -2.2142, -2.3499, -2.4622, -2.5452, -2.5990, -2.6452, -2.6904, -2.7559,\n",
      "        -2.8504, -2.9750, -3.1111, -3.2609, -3.4012, -3.5267, -3.6280, -3.7084,\n",
      "        -3.7348, -3.7219, -3.6740, -3.5761, -3.4307, -3.2712, -3.0921, -2.8892,\n",
      "        -2.6808, -2.4697, -2.2520, -2.0380, -1.8317, -1.6474, -1.4899, -1.3601,\n",
      "        -1.2295, -1.1218, -1.0319, -0.9567, -0.8910, -0.8587, -0.8319, -0.8033,\n",
      "        -0.7660, -0.7373, -0.7042, -0.6735, -0.6461, -0.6349, -0.6148, -0.5984,\n",
      "        -0.5808, -0.5644, -0.5438, -0.5314, -0.5170, -0.5029, -0.4875, -0.4744,\n",
      "        -0.4700, -0.4703, -0.4691, -0.4714, -0.4752, -0.4666, -0.4598, -0.4624,\n",
      "        -0.4609, -0.4575, -0.4565, -0.4555, -0.4496, -0.4517, -0.4520, -0.4539,\n",
      "        -0.4585, -0.4663, -0.4735, -0.4913, -0.5101, -0.5289, -0.5503, -0.5745,\n",
      "        -0.5933, -0.6146, -0.6417, -0.7120, -0.8152, -0.9495, -1.1176, -1.3219,\n",
      "        -1.5269, -1.7501, -1.9895, -2.2339, -2.4507, -2.6322, -2.7601, -2.8289,\n",
      "        -2.8360, -2.8096, -2.7489, -2.6599, -2.5532, -2.4386, -2.3101, -2.1800,\n",
      "        -2.0514, -1.9234, -1.7977, -1.6771, -1.5579, -1.4421, -1.3242, -1.2106,\n",
      "        -1.0998, -0.9936, -0.8876, -0.7970, -0.7131, -0.6433, -0.5836, -0.5366,\n",
      "        -0.4972, -0.4721, -0.4523, -0.4402, -0.4308, -0.4326, -0.4298, -0.4297,\n",
      "        -0.4276, -0.4341, -0.4329, -0.4472, -0.4647, -0.4971, -0.5333, -0.5821,\n",
      "        -0.6374, -0.7066, -0.7751, -0.8523, -0.9373, -1.0420, -1.1624, -1.3157,\n",
      "        -1.4915, -1.6990, -1.9085, -2.1276, -2.3406, -2.5464, -2.7233, -2.8816,\n",
      "        -3.0023, -3.0875, -3.1368, -3.1630, -3.1513, -3.1118, -3.0371, -2.9328,\n",
      "        -2.7909, -2.6360, -2.4692, -2.3029, -2.1351, -1.9783, -1.8201, -1.6628,\n",
      "        -1.5014, -1.3578, -1.2180, -1.0926, -0.9863, -0.9090, -0.8404, -0.7861,\n",
      "        -0.7390, -0.6997, -0.6598, -0.6272, -0.5984, -0.5724, -0.5445, -0.5191,\n",
      "        -0.4880, -0.4606, -0.4381, -0.4268, -0.4158, -0.4147, -0.4228, -0.4436,\n",
      "        -0.4735, -0.5244, -0.5852, -0.6547, -0.7317, -0.8032, -0.8702, -0.9543,\n",
      "        -1.0574, -1.1781, -1.3417, -1.5422, -1.7678, -2.0123, -2.2693, -2.5161,\n",
      "        -2.7524, -2.9676, -3.1483, -3.2924, -3.3852, -3.4185, -3.3792, -3.2817,\n",
      "        -3.1328, -2.9521, -2.7454, -2.5399, -2.3371, -2.1475, -1.9793, -1.8398,\n",
      "        -1.7234, -1.6245, -1.5328, -1.4434, -1.3488, -1.2492, -1.1487, -1.0531,\n",
      "        -0.9642, -0.8915, -0.8301, -0.7808, -0.7365, -0.6968, -0.6527, -0.6161,\n",
      "        -0.5792, -0.5525, -0.5289, -0.5200, -0.5027, -0.4896, -0.4742, -0.4635,\n",
      "        -0.4477, -0.4460, -0.4504, -0.4617, -0.4751, -0.4929, -0.5073, -0.5212,\n",
      "        -0.5351, -0.5524, -0.5722, -0.6014, -0.6456, -0.6988, -0.7617, -0.8450,\n",
      "        -0.9553, -1.0998, -1.2831, -1.5168, -1.7941, -2.1110, -2.4486, -2.8074,\n",
      "        -3.1460, -3.4454, -3.6746, -3.8203, -3.8681, -3.8324, -3.7302, -3.5838,\n",
      "        -3.4108, -3.2261, -3.0425, -2.8690, -2.7049, -2.5481, -2.3970, -2.2551,\n",
      "        -2.1055, -1.9532, -1.7976, -1.6423, -1.4824, -1.3360, -1.1965, -1.0738,\n",
      "        -0.9588, -0.8660, -0.7814, -0.7115, -0.6566, -0.6216, -0.5949, -0.5757,\n",
      "        -0.5609, -0.5440, -0.5266, -0.5137, -0.4984, -0.4881, -0.4841, -0.4825,\n",
      "        -0.4820, -0.4949, -0.5113, -0.5411, -0.5865, -0.6468, -0.7258, -0.8322,\n",
      "        -0.9649, -1.1396, -1.3659, -1.6431, -1.9573, -2.2880, -2.5986, -2.8725,\n",
      "        -3.0817, -3.2273, -3.3033, -3.3248, -3.2905, -3.2235, -3.1266, -3.0227,\n",
      "        -2.9121, -2.8111, -2.7043, -2.5995, -2.4813, -2.3539, -2.2103, -2.0718,\n",
      "        -1.9267, -1.7962, -1.6674, -1.5426, -1.4104, -1.2835, -1.1550, -1.0463,\n",
      "        -0.9525, -0.8839, -0.8325, -0.7998, -0.7711, -0.7478, -0.7221, -0.6980,\n",
      "        -0.6712, -0.6601, -0.6577, -0.6756, -0.7150, -0.7785, -0.8431, -0.9244,\n",
      "        -1.0262, -1.1561, -1.3145, -1.5219, -1.7822, -2.0763, -2.3806, -2.6867,\n",
      "        -2.9663, -3.1816, -3.3204, -3.3875, -3.3812, -3.3179, -3.2147, -3.0858,\n",
      "        -2.9392, -2.7912, -2.6484, -2.5212, -2.4108, -2.3228, -2.2409, -2.1528,\n",
      "        -2.0390, -1.9094, -1.7548, -1.5906, -1.4208, -1.2712, -1.1401, -1.0338,\n",
      "        -0.9433, -0.8721, -0.8090, -0.7549, -0.7109, -0.6793, -0.6647, -0.6722,\n",
      "        -0.6969, -0.7316, -0.7818, -0.8461, -0.9158, -1.0143, -1.1508, -1.3142,\n",
      "        -1.5064, -1.7410, -2.0027, -2.2849, -2.5764, -2.8717, -3.1556, -3.4071,\n",
      "        -3.6075, -3.7559, -3.8407, -3.8533, -3.7934, -3.6755, -3.5125, -3.3245,\n",
      "        -3.1273, -2.9421, -2.7701, -2.6051, -2.4408, -2.2784, -2.1063, -1.9308,\n",
      "        -1.7572, -1.5961, -1.4427, -1.3063, -1.1842, -1.0813, -0.9990, -0.9365,\n",
      "        -0.8839, -0.8450, -0.8202, -0.8072, -0.8151, -0.8479, -0.9070, -0.9924,\n",
      "        -1.0971, -1.2188, -1.3647, -1.5358, -1.7322, -1.9559, -2.2003, -2.4591,\n",
      "        -2.7193, -2.9683, -3.1741, -3.3155, -3.3714, -3.3465, -3.2477, -3.0938,\n",
      "        -2.9204, -2.7492, -2.5781, -2.4217, -2.2841, -2.1466, -2.0104, -1.8793,\n",
      "        -1.7383, -1.5977, -1.4585, -1.3294, -1.2158, -1.1181, -1.0417, -1.0054,\n",
      "        -1.0037, -1.0384, -1.1167, -1.2438, -1.4030, -1.5846, -1.7942, -2.0435])\n"
     ]
    }
   ],
   "source": [
    "print(labels.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update (5 epochs): Semi-automated data loading procedure and additional convolutional-pooling layer to the CNN, reducing the output nodes that are fed to RNN by a factor of 10. Learning rate reduced tenfold (1e-4). Run over 5 epochs shows significant improvement. Root of average MSE in last epoch is 2.07, at that instance it seems it could be further lowered by learning.\n",
    "Note: Assuming values are equally distributed between -3 and 0, a random force predictor would have the metric above equal to 1.5 . Below 1.5 is needed in a test setting means so that the force predictor is relevant. Still, significant improvement must be noted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update (10 epochs): Average RMSE of 1.43 after 10 epochs, plateau reached but (in theory) better than a random guesser. Improvements to be considered: sequence length (e.g. 100 frames), reducing input resolution, adding 1 more conv&pool layer, increasing hidden state size of RNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
