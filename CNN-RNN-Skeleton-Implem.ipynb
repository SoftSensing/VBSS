{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "\n",
    "# relative path to npz files\n",
    "path = 'Measurements'\n",
    "file_name = 'output_batch_%d.npz'\n",
    "\n",
    "# Training\n",
    "# how many frames to load\n",
    "frames_num = 10\n",
    "data_frames = []\n",
    "data_forces = []\n",
    "for i in range(frames_num):\n",
    "    file_path = os.path.join(path,file_name %i)\n",
    "    data = np.load(file_path)\n",
    "    data_frames.append(data['frames'])\n",
    "    data_forces.append(data['forces'])\n",
    "\n",
    "combined_frames = np.concatenate(data_frames, axis=0)\n",
    "combined_forces = np.concatenate(data_forces, axis=0)\n",
    "\n",
    "# Test\n",
    "# file_path = os.path.join(path, file_name %11)\n",
    "# test_data = np.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split. Size of train data is 10/11 and size of test data is 1/11 \n",
    "# features_train, features_test, targets_train, targets_test = train_test_split(combined_frames,\n",
    "#                                                                              combined_forces[:,2],\n",
    "#                                                                              test_size = (1/11),\n",
    "#                                                                              random_state = 42) \n",
    "# train_test_split NOT RECOMMENDED, because it shuffles temporally dependent data\n",
    "features_train = combined_frames\n",
    "targets_train = combined_forces[:,2]\n",
    "# features_test = test_data['frames']\n",
    "# targets_test = test_data['forces']\n",
    "# targets_test = targets_test[:,2]\n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train)\n",
    "\n",
    "# create feature and targets tensor for test set. \n",
    "# featuresTest = torch.from_numpy(features_test)\n",
    "# targetsTest = torch.from_numpy(targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 9, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 4, stride = 4)\n",
    "        self.conv2 = nn.Conv2d(9, 18, kernel_size = 5, stride = 1, padding = 2) # If needed\n",
    "        \n",
    "        # RNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(18 * 16 * 30, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(nn.ELU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ELU()(self.conv2(x))) # If needed\n",
    "        \n",
    "        # Reshape for RNN\n",
    "        x = torch.reshape(x, (-1, 1, 18 * 16 * 30))  # Reshape to (batch_size, seq_len, input_size)\n",
    "        print(\"Size of x:\", x.size())  # Print size of x\n",
    "        \n",
    "        # RNN\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "        #print(\"Size of h0:\", h0.size())  # Print size of h0 - Debugging - Obsolete\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1000\n",
    "num_epochs = 2\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = TensorDataset(featuresTrain,targetsTrain)\n",
    "# test = TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "# test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# Create RNN\n",
    "input_channels = 3  # RGB channels\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "\n",
    "model = RNNModel(input_channels, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Define your loss function\n",
    "error = nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 1  Loss: 23510.298828125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 2  Loss: 33474.90234375\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 3  Loss: 1232.0384521484375\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 4  Loss: 10396.943359375\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 5  Loss: 1861.7091064453125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 6  Loss: 1363.58251953125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 7  Loss: 2874.8125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 8  Loss: 1053.53759765625\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 9  Loss: 2.760005235671997\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 10  Loss: 855.9058227539062\n",
      "~~~BEGINNING OF DATASET~~~\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 11  Loss: 1306.25439453125\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 12  Loss: 660.7703857421875\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 13  Loss: 101.91355895996094\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 14  Loss: 54.33291244506836\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 15  Loss: 286.3187561035156\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 16  Loss: 24.49819564819336\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 17  Loss: 276.5011291503906\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 18  Loss: 17.31190299987793\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 19  Loss: 104.37779235839844\n",
      "Size of x: torch.Size([1000, 1, 8640])\n",
      "Iteration: 20  Loss: 135.13844299316406\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "seq_dim = 50 # Consider the whole batch to be temporally correlated\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('~~~BEGINNING OF DATASET~~~')\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.float()\n",
    "        # print(images.shape)  # Add this line to check the shape of images - Debugging purposes\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = error(outputs, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "            \n",
    "        # Store loss and iteration\n",
    "        loss_list.append(loss.data)\n",
    "        # Print Loss\n",
    "        if count % 1 == 0: # for now print for every iteration\n",
    "            print('Iteration: {}  Loss: {}'.format(count, loss.data.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(23510.2988), tensor(33474.9023), tensor(1232.0385), tensor(10396.9434), tensor(1861.7091), tensor(1363.5825), tensor(2874.8125), tensor(1053.5376), tensor(2.7600), tensor(855.9058), tensor(1306.2544), tensor(660.7704), tensor(101.9136), tensor(54.3329), tensor(286.3188), tensor(24.4982), tensor(276.5011), tensor(17.3119), tensor(104.3778), tensor(135.1384)]\n",
      "3979.6953\n"
     ]
    }
   ],
   "source": [
    "print(loss_list)\n",
    "print(np.mean(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.0389, -12.2216, -12.4135, -12.9569, -14.0069, -13.2281, -13.5223,\n",
      "        -12.4763, -12.2662, -12.5269, -12.4992, -13.2178, -13.4738, -14.2197,\n",
      "        -13.4768, -13.3302, -13.6710, -13.1108, -13.9073, -13.9472, -14.5618,\n",
      "        -13.1335, -13.0651, -12.7524, -12.3736, -12.1626, -13.2127, -12.8865,\n",
      "        -12.6148, -12.9973, -13.1842, -13.6202, -13.7111, -12.5904, -12.9495,\n",
      "        -12.4834, -12.6316, -12.0620, -12.9607, -13.8921, -13.6567, -13.6668,\n",
      "        -13.8833, -14.1457, -13.5324, -13.4590, -13.9893, -13.6739, -13.4093,\n",
      "        -13.5735, -12.3026, -13.0420, -13.6567, -12.9436, -13.1029, -12.6699,\n",
      "        -12.9527, -12.8513, -13.2913, -12.6116, -12.6981, -13.2309, -13.1655,\n",
      "        -13.4798, -13.0522, -13.6539, -13.9016, -13.5796, -13.2757, -13.5511,\n",
      "        -14.3168, -13.7013, -14.0783, -12.7724, -13.0944, -12.1325, -12.4959,\n",
      "        -13.0983, -12.8399, -13.4726, -13.1921, -13.1825, -13.6478, -12.9765,\n",
      "        -13.0410, -12.4911, -12.8078, -13.9186, -13.9956, -13.2014, -13.0608,\n",
      "        -13.1098, -14.5274, -13.1560, -13.9255, -13.4970, -13.6711, -14.0800,\n",
      "        -14.2714, -14.2576, -13.1703, -12.6384, -13.0044, -12.7198, -12.4226,\n",
      "        -12.6058, -12.8568, -12.8318, -12.2418, -12.2510, -12.8637, -13.4485,\n",
      "        -13.8918, -13.5308, -14.0542, -13.9269, -13.4936, -14.1489, -13.4701,\n",
      "        -13.6352, -13.4638, -14.1349, -14.1313, -13.7313, -13.2677, -12.4809,\n",
      "        -11.7492, -12.8121, -12.6599, -13.1178, -13.0013, -12.3939, -12.6722,\n",
      "        -12.5008, -12.6840, -12.8833, -13.3237, -13.6951, -13.7986, -15.1230,\n",
      "        -13.9446, -14.1488, -14.2927, -13.9127, -13.9316, -14.0101, -14.0344,\n",
      "        -13.6266, -13.7649, -12.7863, -12.7938, -12.0939, -12.1500, -13.2180,\n",
      "        -13.2998, -12.4750, -12.5693, -12.7570, -11.8975, -12.3532, -12.6310,\n",
      "        -13.0643, -13.6381, -14.4730, -13.5168, -14.1084, -13.4934, -13.0941,\n",
      "        -13.9194, -14.1020, -13.7421, -13.7697, -13.9447, -13.9536, -14.3939,\n",
      "        -13.2104, -12.7904, -12.9729, -13.0327, -12.6498, -12.6066, -13.0108,\n",
      "        -13.1913, -12.9567, -13.4784, -12.8071, -12.9796, -12.1019, -12.8239,\n",
      "        -13.1642, -13.4632, -13.9478, -13.8035, -13.7055, -13.4579, -13.1147,\n",
      "        -13.7110, -13.1428, -14.1970, -13.9040, -13.9546, -13.1475, -13.3896,\n",
      "        -13.9752, -13.6941, -13.4662, -12.6598, -12.1704, -12.2952, -13.0007,\n",
      "        -12.4408, -13.1355, -13.5440, -13.2284, -13.6956, -13.1479, -13.0715,\n",
      "        -12.7542, -12.9252, -13.2560, -12.6479, -12.9727, -12.1925, -12.1763,\n",
      "        -12.5345, -12.9631, -12.5716, -12.4872, -12.5922, -12.8504, -12.6011,\n",
      "        -13.0652, -12.3867, -12.8383, -12.1252, -12.7711, -12.6796, -13.0380,\n",
      "        -13.4125, -13.0909, -13.0285, -13.1663, -13.2721, -12.5677, -13.7488,\n",
      "        -12.1824, -12.5766, -13.0062, -12.6627, -13.2668, -13.4647, -11.9436,\n",
      "        -12.9564, -13.2791, -12.9751, -12.9436, -13.9780, -13.3928, -13.1418,\n",
      "        -13.0041, -13.2327, -13.4029, -12.6488, -13.3168, -13.1207, -13.0465,\n",
      "        -13.3197, -12.6904, -12.4866, -13.3936, -12.7366, -12.6640, -13.2968,\n",
      "        -12.8130, -13.6709, -13.3550, -13.0874, -12.9099, -12.7627, -12.5269,\n",
      "        -12.5097, -13.0008, -13.2068, -13.1690, -12.7612, -13.1908, -12.8108,\n",
      "        -12.5609, -13.4975, -13.1484, -13.5165, -13.4877, -13.0736, -13.3607,\n",
      "        -13.7394, -13.1498, -12.9693, -12.7602, -13.1097, -13.3955, -13.2955,\n",
      "        -12.8350, -13.2124, -13.1433, -12.8432, -12.6297, -12.8098, -12.2672,\n",
      "        -12.8031, -12.6271, -13.3166, -12.5450, -12.8065, -13.1129, -12.3818,\n",
      "        -12.9168, -12.5697, -12.9057, -12.6340, -13.0649, -13.9351, -13.4800,\n",
      "        -12.8797, -13.4712, -12.6473, -12.7371, -13.4791, -13.9002, -12.9466,\n",
      "        -13.0323, -13.1846, -12.3306, -12.9921, -12.5758, -13.4632, -12.8095,\n",
      "        -13.1614, -12.5014, -12.9824, -12.7103, -13.1496, -12.5707, -12.4898,\n",
      "        -13.0301, -12.8074, -12.7077, -12.6000, -12.8753, -13.0371, -13.3363,\n",
      "        -12.7568, -12.8189, -12.7024, -12.7578, -13.4142, -13.4265, -12.9745,\n",
      "        -12.8253, -13.2600, -12.6794, -12.6753, -12.5025, -12.7870, -12.9727,\n",
      "        -13.4760, -12.6599, -12.8197, -13.2274, -12.6735, -12.9807, -14.0430,\n",
      "        -13.0772, -12.8591, -13.3394, -12.9713, -13.6938, -12.5219, -13.9628,\n",
      "        -12.1768, -12.7318, -13.0198, -12.9672, -13.2785, -12.8512, -13.1671,\n",
      "        -13.1524, -13.4335, -13.1844, -13.4143, -13.0644, -13.2540, -13.1240,\n",
      "        -13.8362, -13.6074, -13.5791, -13.2414, -12.5288, -13.1067, -12.6007,\n",
      "        -13.4714, -13.7191, -13.5800, -13.4549, -13.6564, -13.2020, -13.2630,\n",
      "        -13.3999, -13.4358, -13.1989, -12.6638, -12.6712, -13.3294, -12.8984,\n",
      "        -12.9355, -12.6184, -12.6666, -13.0659, -12.9362, -12.6721, -12.7401,\n",
      "        -12.6547, -13.0336, -13.2278, -13.2615, -13.3269, -12.6985, -12.8348,\n",
      "        -13.3197, -12.5035, -12.5991, -12.4935, -12.5430, -13.4543, -12.7767,\n",
      "        -12.9160, -13.2241, -12.5642, -13.2062, -13.2613, -13.4867, -13.1991,\n",
      "        -13.6656, -13.0814, -12.5566, -12.8259, -12.6985, -12.2752, -12.9757,\n",
      "        -12.9721, -11.7538, -12.8800, -12.4989, -12.5113, -12.8665, -13.5784,\n",
      "        -13.0017, -13.3365, -13.3694, -13.7813, -13.2598, -13.8795, -13.3377,\n",
      "        -12.8315, -14.0355, -13.0446, -13.4915, -13.5251, -13.2233, -13.6122,\n",
      "        -13.5271, -13.9747, -13.5673, -13.5421, -13.1427, -12.9787, -14.1020,\n",
      "        -14.0092, -13.3948, -13.2798, -13.1114, -12.7001, -12.5152, -12.4524,\n",
      "        -12.7347, -12.5548, -13.0296, -13.0234, -13.1634, -13.1733, -13.8855,\n",
      "        -13.0633, -13.7579, -13.0569, -12.8890, -12.4358, -13.1057, -13.4563,\n",
      "        -12.6535, -12.7924, -13.0057, -12.8880, -12.7461, -12.7504, -13.1070,\n",
      "        -13.4392, -12.3750, -13.4084, -13.5679, -12.4473, -13.0832, -12.8358,\n",
      "        -12.5746, -13.0117, -13.2901, -12.2798, -12.3589, -12.7533, -13.3289,\n",
      "        -13.1831, -12.9248, -13.5624, -13.2662, -13.2137, -13.0064, -12.9165,\n",
      "        -12.7616, -13.0345, -13.0235, -13.5262, -13.2757, -13.1638, -13.3203,\n",
      "        -13.7072, -14.0345, -13.1794, -13.0590, -13.6039, -12.4778, -13.6919,\n",
      "        -13.9674, -13.0800, -13.0418, -13.5437, -13.2071, -13.1327, -13.8352,\n",
      "        -13.0721, -13.2303, -13.4616, -13.0154, -13.3581, -13.9166, -12.7827,\n",
      "        -13.2951, -12.8295, -12.5320, -12.0835, -13.1054, -12.7675, -12.7856,\n",
      "        -13.0294, -13.5051, -12.6461, -12.8445, -12.3119, -12.5273, -12.8973,\n",
      "        -12.8721, -13.4535, -12.7959, -13.2477, -12.7208, -13.2872, -13.7081,\n",
      "        -12.0349, -13.0332, -12.5134, -12.4779, -12.2980, -12.7309, -12.6377,\n",
      "        -12.7815, -13.8899, -13.5199, -13.6270, -13.5690, -13.2628, -13.4508,\n",
      "        -12.9629, -13.7340, -12.8495, -13.5340, -13.2729, -13.6391, -13.1974,\n",
      "        -13.4215, -14.3891, -13.9106, -13.3322, -13.1150, -12.5902, -12.9180,\n",
      "        -12.5954, -12.7437, -12.2256, -11.9162, -12.3955, -11.4816, -12.1428,\n",
      "        -10.5862, -11.4531, -11.3298, -11.1517, -11.9158, -11.4961, -11.2887,\n",
      "        -10.7171, -11.7116, -12.4213, -13.0685, -12.6965, -13.6992, -13.6051,\n",
      "        -13.6937, -14.2122, -13.9929, -13.8284, -14.2298, -13.5930, -13.7987,\n",
      "        -13.3050, -13.1362, -12.8180, -12.9157, -13.2286, -12.8542, -13.3190,\n",
      "        -12.8197, -13.2353, -12.8640, -13.8341, -13.2845, -12.8723, -13.4064,\n",
      "        -13.0550, -13.0302, -12.0675, -12.7476, -12.5713, -12.5215, -12.7221,\n",
      "        -12.6387, -12.6011, -12.7404, -12.3670, -12.6220, -13.1722, -13.3373,\n",
      "        -12.7075, -13.4258, -12.8655, -12.8182, -12.8092, -13.4406, -12.4178,\n",
      "        -12.7421, -12.8837, -13.1946, -12.6036, -13.1800, -13.0435, -12.4888,\n",
      "        -12.9629, -13.1086, -12.8298, -13.5770, -13.3808, -12.6432, -12.8067,\n",
      "        -13.3429, -13.5385, -13.7895, -13.3698, -13.2180, -12.9277, -12.8172,\n",
      "        -12.8387, -13.0239, -13.2666, -13.0960, -13.1543, -13.2582, -12.8999,\n",
      "        -12.5911, -13.1293, -12.9730, -13.5440, -13.5378, -12.9538, -12.9504,\n",
      "        -12.5300, -13.3571, -12.8822, -12.7719, -12.9791, -13.3808, -12.9278,\n",
      "        -13.0869, -12.7234, -13.0578, -13.2008, -12.3840, -12.9486, -13.5521,\n",
      "        -12.8426, -12.7027, -12.7335, -13.1340, -13.1540, -12.5932, -12.4718,\n",
      "        -12.9202, -13.0658, -13.3036, -13.6405, -13.3149, -12.7286, -13.7479,\n",
      "        -12.9572, -13.9566, -13.0071, -13.5607, -13.0636, -13.1922, -13.2938,\n",
      "        -13.1634, -12.3259, -11.9380, -12.3452, -11.6213, -12.0171, -11.9778,\n",
      "        -11.3078, -11.6875, -11.9264, -11.6660, -11.6415, -11.2612, -11.0627,\n",
      "        -11.4148, -11.5160, -11.3894, -11.4841, -11.2988, -10.8590, -11.5066,\n",
      "        -11.1506, -11.2804, -11.0608, -12.0873, -11.6831, -11.3140, -11.4649,\n",
      "        -11.8574, -12.1860, -12.5642, -12.9368, -12.2327, -13.2812, -14.3640,\n",
      "        -13.9477, -13.9562, -13.9880, -14.5545, -14.3247, -14.1483, -14.4574,\n",
      "        -13.2982, -13.2562, -12.6364, -12.6802, -12.6181, -12.4066, -13.6815,\n",
      "        -13.2541, -14.3653, -13.1241, -13.3469, -13.5309, -13.3666, -12.8037,\n",
      "        -13.5095, -13.0623, -13.1472, -12.5095, -12.8321, -12.3573, -13.3277,\n",
      "        -13.1379, -13.2712, -12.7045, -12.9442, -13.8578, -12.9971, -13.5548,\n",
      "        -13.0053, -13.3350, -13.3535, -13.1800, -13.5065, -13.2989, -13.5278,\n",
      "        -13.0896, -12.8926, -13.2808, -13.2626, -13.0904, -13.3566, -13.2335,\n",
      "        -12.9526, -13.2438, -13.3191, -13.5808, -13.5411, -13.3646, -13.1150,\n",
      "        -13.8002, -13.2402, -13.2679, -13.4700, -13.4149, -13.0728, -13.4629,\n",
      "        -13.0263, -13.9269, -13.2874, -13.2750, -13.2858, -13.2116, -13.1617,\n",
      "        -13.3347, -12.9913, -14.0352, -13.4405, -13.2813, -13.3729, -13.5915,\n",
      "        -12.9444, -12.7749, -13.8053, -13.4718, -13.1446, -13.5792, -12.5258,\n",
      "        -13.2026, -13.2699, -13.3306, -13.6107, -13.3964, -13.2740, -13.5467,\n",
      "        -13.5206, -12.3840, -12.8700, -13.6399, -12.7226, -13.1885, -13.6063,\n",
      "        -13.8132, -12.5401, -13.0447, -12.9294, -12.6760, -13.1315, -12.9446,\n",
      "        -12.5897, -13.1388, -13.4093, -12.9884, -13.9119, -13.8199, -14.1555,\n",
      "        -13.2621, -14.2403, -13.1848, -12.5999, -12.5130, -12.5090, -11.8299,\n",
      "        -12.1678, -11.4369, -11.4522, -11.5138, -11.7552, -11.9461, -11.3573,\n",
      "        -11.5622, -11.8106, -11.5111, -11.9418, -11.6828, -11.4841, -11.5726,\n",
      "        -11.6755, -11.4331, -11.5853, -12.2283, -11.6470, -11.6490, -11.8939,\n",
      "        -12.7580, -13.6395, -14.3385, -14.5966, -14.1466, -14.4858, -14.8989,\n",
      "        -14.2243, -13.9933, -13.5326, -13.2272, -13.1788, -13.6701, -14.2112,\n",
      "        -12.8775, -13.3064, -12.7008, -12.6587, -13.2158, -13.8224, -13.6828,\n",
      "        -13.2650, -13.6355, -13.8000, -13.5843, -13.6618, -12.6712, -13.5211,\n",
      "        -12.4436, -13.1902, -12.8359, -12.8108, -13.1519, -13.1818, -12.7760,\n",
      "        -13.0786, -13.0017, -13.4061, -12.9730, -13.0308, -13.3354, -13.3429,\n",
      "        -13.4264, -13.4142, -13.4905, -13.5576, -13.0928, -13.7556, -13.7919,\n",
      "        -12.9648, -13.0345, -13.3171, -13.6507, -13.7820, -13.2605, -13.4299,\n",
      "        -12.8154, -13.4735, -13.1068, -12.9853, -13.7872, -13.3247, -13.3966,\n",
      "        -13.5377, -13.3478, -13.6111, -13.6410, -13.2584, -13.3488, -12.9598,\n",
      "        -13.7372, -12.8146, -13.2972, -13.5210, -13.6231, -13.5076, -13.7648,\n",
      "        -13.4008, -12.8720, -13.4995, -13.6986, -13.6783, -13.0788, -13.7482,\n",
      "        -12.9581, -13.6484, -13.3632, -13.2337, -14.1677, -13.5532],\n",
      "       grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs) # Small sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0797, -0.9949, -0.9238, -0.8581, -0.7988, -0.7402, -0.6864, -0.6413,\n",
      "        -0.6007, -0.5637, -0.5303, -0.5034, -0.4781, -0.4516, -0.4438, -0.4478,\n",
      "        -0.4525, -0.4622, -0.4775, -0.4831, -0.4824, -0.4824, -0.4840, -0.4829,\n",
      "        -0.4796, -0.4784, -0.4745, -0.4723, -0.4696, -0.4686, -0.4738, -0.4907,\n",
      "        -0.5179, -0.5654, -0.6264, -0.6943, -0.7702, -0.8488, -0.9312, -1.0187,\n",
      "        -1.1152, -1.2284, -1.3486, -1.4846, -1.6458, -1.8335, -2.0371, -2.2583,\n",
      "        -2.4803, -2.6908, -2.8747, -3.0401, -3.1835, -3.2991, -3.3773, -3.4140,\n",
      "        -3.4004, -3.3335, -3.2181, -3.0745, -2.9110, -2.7374, -2.5585, -2.3805,\n",
      "        -2.1968, -2.0178, -1.8431, -1.6839, -1.5378, -1.3989, -1.2665, -1.1307,\n",
      "        -0.9906, -0.8688, -0.7793, -0.7269, -0.7026, -0.7065, -0.7217, -0.7390,\n",
      "        -0.7422, -0.7402, -0.7296, -0.7031, -0.6639, -0.6237, -0.5844, -0.5494,\n",
      "        -0.5165, -0.4928, -0.4744, -0.4658, -0.4598, -0.4682, -0.4804, -0.4992,\n",
      "        -0.5257, -0.5651, -0.6205, -0.6941, -0.7820, -0.8762, -0.9792, -1.0830,\n",
      "        -1.1860, -1.2925, -1.3998, -1.5069, -1.6152, -1.7351, -1.8683, -2.0259,\n",
      "        -2.2131, -2.4256, -2.6556, -2.8895, -3.1145, -3.3130, -3.4665, -3.5678,\n",
      "        -3.6013, -3.5683, -3.4693, -3.3291, -3.1547, -2.9744, -2.7906, -2.6148,\n",
      "        -2.4369, -2.2563, -2.0721, -1.8849, -1.7012, -1.5338, -1.3869, -1.2611,\n",
      "        -1.1644, -1.0881, -1.0243, -0.9720, -0.9171, -0.8640, -0.8028, -0.7433,\n",
      "        -0.6825, -0.6341, -0.6034, -0.6033, -0.6143, -0.6381, -0.6638, -0.6799,\n",
      "        -0.6787, -0.6769, -0.6661, -0.6596, -0.6539, -0.6556, -0.6553, -0.6656,\n",
      "        -0.6793, -0.7042, -0.7418, -0.7945, -0.8589, -0.9346, -1.0123, -1.0908,\n",
      "        -1.1647, -1.2411, -1.3176, -1.4149, -1.5416, -1.7095, -1.9003, -2.1114,\n",
      "        -2.3280, -2.5345, -2.7175, -2.8897, -3.0498, -3.1876, -3.2975, -3.3698,\n",
      "        -3.3850, -3.3393, -3.2283, -3.0734, -2.8929, -2.7096, -2.5290, -2.3726,\n",
      "        -2.2205, -2.0771, -1.9308, -1.7917, -1.6456, -1.5045, -1.3665, -1.2382,\n",
      "        -1.1116, -1.0024, -0.9111, -0.8395, -0.7838, -0.7525, -0.7340, -0.7211,\n",
      "        -0.7133, -0.7147, -0.7136, -0.7130, -0.7133, -0.7065, -0.6910, -0.6715,\n",
      "        -0.6441, -0.6176, -0.5947, -0.5745, -0.5677, -0.5734, -0.5914, -0.6191,\n",
      "        -0.6562, -0.6960, -0.7349, -0.7681, -0.7970, -0.8236, -0.8433, -0.8648,\n",
      "        -0.8818, -0.9050, -0.9295, -0.9649, -1.0187, -1.1147, -1.2539, -1.4445,\n",
      "        -1.6810, -1.9478, -2.2324, -2.5187, -2.7749, -2.9926, -3.1669, -3.2939,\n",
      "        -3.3613, -3.3867, -3.3694, -3.3181, -3.2216, -3.1101, -2.9802, -2.8434,\n",
      "        -2.6935, -2.5383, -2.3763, -2.2236, -2.0704, -1.9252, -1.7893, -1.6546,\n",
      "        -1.5215, -1.3909, -1.2638, -1.1432, -1.0276, -0.9230, -0.8348, -0.7657,\n",
      "        -0.7085, -0.6715, -0.6397, -0.6155, -0.5917, -0.5774, -0.5623, -0.5571,\n",
      "        -0.5541, -0.5537, -0.5525, -0.5506, -0.5400, -0.5329, -0.5240, -0.5265,\n",
      "        -0.5309, -0.5408, -0.5493, -0.5667, -0.5783, -0.6042, -0.6412, -0.6991,\n",
      "        -0.7725, -0.8657, -0.9748, -1.1060, -1.2478, -1.4170, -1.6060, -1.8013,\n",
      "        -2.0041, -2.2108, -2.4030, -2.5873, -2.7657, -2.9232, -3.0525, -3.1439,\n",
      "        -3.1812, -3.1558, -3.0807, -2.9688, -2.8356, -2.6961, -2.5628, -2.4344,\n",
      "        -2.3059, -2.1768, -2.0468, -1.8984, -1.7287, -1.5682, -1.4171, -1.2679,\n",
      "        -1.1438, -1.0552, -0.9667, -0.8872, -0.8263, -0.7788, -0.7338, -0.7042,\n",
      "        -0.6748, -0.6494, -0.6202, -0.5968, -0.5709, -0.5468, -0.5222, -0.5032,\n",
      "        -0.4861, -0.4792, -0.4831, -0.4847, -0.4801, -0.4838, -0.4795, -0.4695,\n",
      "        -0.4577, -0.4593, -0.4465, -0.4412, -0.4448, -0.4661, -0.4885, -0.5210,\n",
      "        -0.5578, -0.5983, -0.6380, -0.7073, -0.8097, -0.9396, -1.0979, -1.2939,\n",
      "        -1.5008, -1.7206, -1.9423, -2.1616, -2.3708, -2.5623, -2.7179, -2.8489,\n",
      "        -2.9510, -3.0242, -3.0765, -3.1207, -3.1452, -3.1465, -3.1118, -3.0394,\n",
      "        -2.9214, -2.7769, -2.6068, -2.4276, -2.2343, -2.0452, -1.8542, -1.6732,\n",
      "        -1.4998, -1.3461, -1.2031, -1.0734, -0.9532, -0.8518, -0.7621, -0.6884,\n",
      "        -0.6280, -0.5819, -0.5368, -0.5053, -0.4781, -0.4612, -0.4546, -0.4563,\n",
      "        -0.4518, -0.4577, -0.4681, -0.4731, -0.4739, -0.4824, -0.4864, -0.4836,\n",
      "        -0.4826, -0.4855, -0.4862, -0.4853, -0.4914, -0.5074, -0.5273, -0.5593,\n",
      "        -0.5932, -0.6382, -0.7148, -0.8072, -0.9102, -1.0490, -1.2074, -1.3741,\n",
      "        -1.5799, -1.8253, -2.0765, -2.3253, -2.5468, -2.7286, -2.8550, -2.9493,\n",
      "        -3.0117, -3.0501, -3.0596, -3.0502, -3.0049, -2.9375, -2.8491, -2.7457,\n",
      "        -2.6359, -2.5251, -2.4159, -2.2942, -2.1545, -1.9922, -1.8248, -1.6577,\n",
      "        -1.5015, -1.3710, -1.2643, -1.1738, -1.0765, -0.9886, -0.9018, -0.8206,\n",
      "        -0.7427, -0.6827, -0.6381, -0.6044, -0.5763, -0.5544, -0.5360, -0.5163,\n",
      "        -0.5010, -0.4921, -0.4828, -0.4796, -0.4773, -0.4737, -0.4725, -0.4779,\n",
      "        -0.4796, -0.4833, -0.4889, -0.4964, -0.5063, -0.5192, -0.5337, -0.5501,\n",
      "        -0.5675, -0.5911, -0.6238, -0.6711, -0.7433, -0.8307, -0.9233, -1.0302,\n",
      "        -1.1394, -1.2457, -1.3570, -1.4870, -1.6171, -1.7593, -1.9088, -2.0682,\n",
      "        -2.2142, -2.3499, -2.4622, -2.5452, -2.5990, -2.6452, -2.6904, -2.7559,\n",
      "        -2.8504, -2.9750, -3.1111, -3.2609, -3.4012, -3.5267, -3.6280, -3.7084,\n",
      "        -3.7348, -3.7219, -3.6740, -3.5761, -3.4307, -3.2712, -3.0921, -2.8892,\n",
      "        -2.6808, -2.4697, -2.2520, -2.0380, -1.8317, -1.6474, -1.4899, -1.3601,\n",
      "        -1.2295, -1.1218, -1.0319, -0.9567, -0.8910, -0.8587, -0.8319, -0.8033,\n",
      "        -0.7660, -0.7373, -0.7042, -0.6735, -0.6461, -0.6349, -0.6148, -0.5984,\n",
      "        -0.5808, -0.5644, -0.5438, -0.5314, -0.5170, -0.5029, -0.4875, -0.4744,\n",
      "        -0.4700, -0.4703, -0.4691, -0.4714, -0.4752, -0.4666, -0.4598, -0.4624,\n",
      "        -0.4609, -0.4575, -0.4565, -0.4555, -0.4496, -0.4517, -0.4520, -0.4539,\n",
      "        -0.4585, -0.4663, -0.4735, -0.4913, -0.5101, -0.5289, -0.5503, -0.5745,\n",
      "        -0.5933, -0.6146, -0.6417, -0.7120, -0.8152, -0.9495, -1.1176, -1.3219,\n",
      "        -1.5269, -1.7501, -1.9895, -2.2339, -2.4507, -2.6322, -2.7601, -2.8289,\n",
      "        -2.8360, -2.8096, -2.7489, -2.6599, -2.5532, -2.4386, -2.3101, -2.1800,\n",
      "        -2.0514, -1.9234, -1.7977, -1.6771, -1.5579, -1.4421, -1.3242, -1.2106,\n",
      "        -1.0998, -0.9936, -0.8876, -0.7970, -0.7131, -0.6433, -0.5836, -0.5366,\n",
      "        -0.4972, -0.4721, -0.4523, -0.4402, -0.4308, -0.4326, -0.4298, -0.4297,\n",
      "        -0.4276, -0.4341, -0.4329, -0.4472, -0.4647, -0.4971, -0.5333, -0.5821,\n",
      "        -0.6374, -0.7066, -0.7751, -0.8523, -0.9373, -1.0420, -1.1624, -1.3157,\n",
      "        -1.4915, -1.6990, -1.9085, -2.1276, -2.3406, -2.5464, -2.7233, -2.8816,\n",
      "        -3.0023, -3.0875, -3.1368, -3.1630, -3.1513, -3.1118, -3.0371, -2.9328,\n",
      "        -2.7909, -2.6360, -2.4692, -2.3029, -2.1351, -1.9783, -1.8201, -1.6628,\n",
      "        -1.5014, -1.3578, -1.2180, -1.0926, -0.9863, -0.9090, -0.8404, -0.7861,\n",
      "        -0.7390, -0.6997, -0.6598, -0.6272, -0.5984, -0.5724, -0.5445, -0.5191,\n",
      "        -0.4880, -0.4606, -0.4381, -0.4268, -0.4158, -0.4147, -0.4228, -0.4436,\n",
      "        -0.4735, -0.5244, -0.5852, -0.6547, -0.7317, -0.8032, -0.8702, -0.9543,\n",
      "        -1.0574, -1.1781, -1.3417, -1.5422, -1.7678, -2.0123, -2.2693, -2.5161,\n",
      "        -2.7524, -2.9676, -3.1483, -3.2924, -3.3852, -3.4185, -3.3792, -3.2817,\n",
      "        -3.1328, -2.9521, -2.7454, -2.5399, -2.3371, -2.1475, -1.9793, -1.8398,\n",
      "        -1.7234, -1.6245, -1.5328, -1.4434, -1.3488, -1.2492, -1.1487, -1.0531,\n",
      "        -0.9642, -0.8915, -0.8301, -0.7808, -0.7365, -0.6968, -0.6527, -0.6161,\n",
      "        -0.5792, -0.5525, -0.5289, -0.5200, -0.5027, -0.4896, -0.4742, -0.4635,\n",
      "        -0.4477, -0.4460, -0.4504, -0.4617, -0.4751, -0.4929, -0.5073, -0.5212,\n",
      "        -0.5351, -0.5524, -0.5722, -0.6014, -0.6456, -0.6988, -0.7617, -0.8450,\n",
      "        -0.9553, -1.0998, -1.2831, -1.5168, -1.7941, -2.1110, -2.4486, -2.8074,\n",
      "        -3.1460, -3.4454, -3.6746, -3.8203, -3.8681, -3.8324, -3.7302, -3.5838,\n",
      "        -3.4108, -3.2261, -3.0425, -2.8690, -2.7049, -2.5481, -2.3970, -2.2551,\n",
      "        -2.1055, -1.9532, -1.7976, -1.6423, -1.4824, -1.3360, -1.1965, -1.0738,\n",
      "        -0.9588, -0.8660, -0.7814, -0.7115, -0.6566, -0.6216, -0.5949, -0.5757,\n",
      "        -0.5609, -0.5440, -0.5266, -0.5137, -0.4984, -0.4881, -0.4841, -0.4825,\n",
      "        -0.4820, -0.4949, -0.5113, -0.5411, -0.5865, -0.6468, -0.7258, -0.8322,\n",
      "        -0.9649, -1.1396, -1.3659, -1.6431, -1.9573, -2.2880, -2.5986, -2.8725,\n",
      "        -3.0817, -3.2273, -3.3033, -3.3248, -3.2905, -3.2235, -3.1266, -3.0227,\n",
      "        -2.9121, -2.8111, -2.7043, -2.5995, -2.4813, -2.3539, -2.2103, -2.0718,\n",
      "        -1.9267, -1.7962, -1.6674, -1.5426, -1.4104, -1.2835, -1.1550, -1.0463,\n",
      "        -0.9525, -0.8839, -0.8325, -0.7998, -0.7711, -0.7478, -0.7221, -0.6980,\n",
      "        -0.6712, -0.6601, -0.6577, -0.6756, -0.7150, -0.7785, -0.8431, -0.9244,\n",
      "        -1.0262, -1.1561, -1.3145, -1.5219, -1.7822, -2.0763, -2.3806, -2.6867,\n",
      "        -2.9663, -3.1816, -3.3204, -3.3875, -3.3812, -3.3179, -3.2147, -3.0858,\n",
      "        -2.9392, -2.7912, -2.6484, -2.5212, -2.4108, -2.3228, -2.2409, -2.1528,\n",
      "        -2.0390, -1.9094, -1.7548, -1.5906, -1.4208, -1.2712, -1.1401, -1.0338,\n",
      "        -0.9433, -0.8721, -0.8090, -0.7549, -0.7109, -0.6793, -0.6647, -0.6722,\n",
      "        -0.6969, -0.7316, -0.7818, -0.8461, -0.9158, -1.0143, -1.1508, -1.3142,\n",
      "        -1.5064, -1.7410, -2.0027, -2.2849, -2.5764, -2.8717, -3.1556, -3.4071,\n",
      "        -3.6075, -3.7559, -3.8407, -3.8533, -3.7934, -3.6755, -3.5125, -3.3245,\n",
      "        -3.1273, -2.9421, -2.7701, -2.6051, -2.4408, -2.2784, -2.1063, -1.9308,\n",
      "        -1.7572, -1.5961, -1.4427, -1.3063, -1.1842, -1.0813, -0.9990, -0.9365,\n",
      "        -0.8839, -0.8450, -0.8202, -0.8072, -0.8151, -0.8479, -0.9070, -0.9924,\n",
      "        -1.0971, -1.2188, -1.3647, -1.5358, -1.7322, -1.9559, -2.2003, -2.4591,\n",
      "        -2.7193, -2.9683, -3.1741, -3.3155, -3.3714, -3.3465, -3.2477, -3.0938,\n",
      "        -2.9204, -2.7492, -2.5781, -2.4217, -2.2841, -2.1466, -2.0104, -1.8793,\n",
      "        -1.7383, -1.5977, -1.4585, -1.3294, -1.2158, -1.1181, -1.0417, -1.0054,\n",
      "        -1.0037, -1.0384, -1.1167, -1.2438, -1.4030, -1.5846, -1.7942, -2.0435])\n"
     ]
    }
   ],
   "source": [
    "print(labels.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Summary: Semi-automated data loading procedure and additional convolutional-pooling layer to the CNN, reducing the output nodes that are fed to RNN by a factor of 10. Significant improvement in terms of results is observed after only 2 epochs, but it is as computationally expensive as before.\n",
    "A not-saved run over 8 epochs with this configuration yielded an average RMSE of 1.5 over the training data set. This is still not close enough to the target, and overfitting may occur, but it goes to show that the new architecture is much more promising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
